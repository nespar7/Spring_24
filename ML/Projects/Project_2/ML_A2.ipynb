{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nespar7/Spring_24/blob/main/ML/Projects/Project_2/ML_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laIuj0GkJSp7",
        "outputId": "99e47a07-012f-46d7-a384-f72b9f38e20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install numpy\n",
        "%pip install pandas\n",
        "%pip install matplotlib\n",
        "%pip install itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kSEonbYxJSp9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r2r8MDX8JSp9"
      },
      "outputs": [],
      "source": [
        "# import filterwarnings\n",
        "from warnings import filterwarnings\n",
        "\n",
        "filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuXAgn6TJSp-"
      },
      "source": [
        "# Train Test Split Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u1KcEiItJSp_"
      },
      "outputs": [],
      "source": [
        "# take an np array x and return values at indices\n",
        "def get_val_at_indices(x, indices):\n",
        "    return x[indices]\n",
        "\n",
        "def train_test_split(*arrays, test_size=0.25, shufffle=True, random_state=1):\n",
        "    # get length of first array\n",
        "    length = len(arrays[0])\n",
        "\n",
        "    # split lengths\n",
        "    len_test = int(np.ceil(length*test_size))\n",
        "    len_train = length - len_test\n",
        "\n",
        "    if shufffle:\n",
        "        perm = np.random.RandomState(random_state).permutation(length)\n",
        "        test_indices = perm[:len_test]\n",
        "        train_indices = perm[len_test:]\n",
        "    else:\n",
        "        train_indices = np.arange(len_train)\n",
        "        test_indices = np.arange(len_train, length)\n",
        "\n",
        "    return list(chain.from_iterable((get_val_at_indices(x, train_indices), get_val_at_indices(x, test_indices)) for x in arrays))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UesVXR3CJSp_"
      },
      "source": [
        "# Custom Class Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P3C6llwNJSqA"
      },
      "outputs": [],
      "source": [
        "class CNN:\n",
        "    def __init__(self, hidden_layer_sizes=(100,), learning_rate=0.1, epochs=200, batch_size=32, random_state=1, tol=1e-4, hidden_layer_activation='logistic', output_layer_activation='linear', optim_algo='SGD', loss_function='MSE', early_stopping=False):\n",
        "        # learning rate should be casted to np.float64\n",
        "        self.learning_rate = np.float64(learning_rate)\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.random_state = random_state\n",
        "        self.tol = tol\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.is_first_pass = True\n",
        "        if hidden_layer_activation == 'logistic':\n",
        "            self.hidden_layer_activation = self.sigmoid\n",
        "            self.hidden_layer_activation_derivative = self.sigmoid_derivative\n",
        "        elif hidden_layer_activation == 'ReLU':\n",
        "            self.hidden_layer_activation = self.ReLU\n",
        "            self.hidden_layer_activation_derivative = self.ReLU_derivative\n",
        "        elif hidden_layer_activation == 'linear':\n",
        "            self.hidden_layer_activation = self.linear\n",
        "            self.hidden_layer_activation_derivative = self.linear_derivative\n",
        "        else:\n",
        "            # default to sigmoid\n",
        "            self.hidden_layer_activation = self.sigmoid\n",
        "            self.hidden_layer_activation_derivative = self.sigmoid_derivative\n",
        "\n",
        "        if output_layer_activation == 'logistic':\n",
        "            self.output_layer_activation = self.sigmoid\n",
        "            self.output_layer_activation_derivative = self.sigmoid_derivative\n",
        "        elif output_layer_activation == 'ReLU':\n",
        "            self.output_layer_activation = self.ReLU\n",
        "            self.output_layer_activation_derivative = self.ReLU_derivative\n",
        "        elif output_layer_activation == 'linear':\n",
        "            self.output_layer_activation = self.linear\n",
        "            self.output_layer_activation_derivative = self.linear_derivative\n",
        "        else:\n",
        "            # default to sigmoid\n",
        "            self.output_layer_activation = self.sigmoid\n",
        "            self.output_layer_activation_derivative = self.sigmoid_derivative\n",
        "\n",
        "        if optim_algo == 'SGD':\n",
        "            self.optim_algo = self.msgd\n",
        "\n",
        "        if loss_function == 'MSE':\n",
        "            self.loss_function = self.MSE\n",
        "\n",
        "        self.early_stopping = early_stopping\n",
        "\n",
        "    # train test split to store the train and test data\n",
        "    def train_test_split(self, *arrays, test_size=0.25, shufffle=True, random_state=1):\n",
        "        # get length of first array\n",
        "        length = len(arrays[0])\n",
        "\n",
        "        # split lengths\n",
        "        len_test = int(np.ceil(length*test_size))\n",
        "        len_train = length - len_test\n",
        "\n",
        "        if shufffle:\n",
        "            perm = np.random.RandomState(random_state).permutation(length)\n",
        "            test_indices = perm[:len_test]\n",
        "            train_indices = perm[len_test:]\n",
        "        else:\n",
        "            train_indices = np.arange(len_train)\n",
        "            test_indices = np.arange(len_train, length)\n",
        "\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = list(chain.from_iterable((get_val_at_indices(x, train_indices), get_val_at_indices(x, test_indices)) for x in arrays))\n",
        "\n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X.astype(np.float64)))\n",
        "\n",
        "    def sigmoid_derivative(self, X):\n",
        "        return X * (1-X)\n",
        "\n",
        "    def ReLU(self, X):\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def ReLU_derivative(self, X):\n",
        "        return 1 * (X > 0)\n",
        "\n",
        "    def linear(self, X):\n",
        "        return X\n",
        "\n",
        "    def linear_derivative(self, X):\n",
        "        return 1\n",
        "\n",
        "    def load_data(self, X, y):\n",
        "        mini_batches = []\n",
        "\n",
        "        num_batches = int(np.ceil(X.shape[0] / self.batch_size))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start = i * self.batch_size\n",
        "            end = start + self.batch_size\n",
        "\n",
        "            if end > X.shape[0]:\n",
        "                end = X.shape[0]\n",
        "\n",
        "            mini_batches.append(((start, end)))\n",
        "\n",
        "        return mini_batches\n",
        "\n",
        "    # MSE loss\n",
        "    def MSE(self, y, y_pred):\n",
        "        return np.mean((y - y_pred)**2)\n",
        "\n",
        "    def msgd(self, X, y):\n",
        "        activations = [None]*(self.num_layers)\n",
        "        deltas = [None] * (self.num_layers-1)\n",
        "\n",
        "        mini_batches = self.load_data(X, y)\n",
        "\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            epoch_loss = 0\n",
        "            for mini_batch in mini_batches:\n",
        "                start, end = mini_batch\n",
        "\n",
        "                activations[0] = self.X_train[start:end]\n",
        "\n",
        "                self.forward(activations)\n",
        "\n",
        "                epoch_loss += self.backward(activations, deltas, self.y_train[start:end], (end-start+1)) * (end-start+1)\n",
        "\n",
        "            if i%10 == 9 and self.plot_losses:\n",
        "                train_losses.append(epoch_loss / X.shape[0])\n",
        "                y_pred = self.predict(self.X_test)\n",
        "                test_losses.append(self.loss_function(self.y_test, y_pred) / self.X_test.shape[0])\n",
        "\n",
        "            # If early stopping is enabled, stop if loss is less than tolerance\n",
        "            if self.early_stopping:\n",
        "                if epoch_loss < self.tol:\n",
        "                    break\n",
        "\n",
        "        self.train_losses = train_losses\n",
        "        self.test_losses = test_losses\n",
        "\n",
        "    def weight_initialization(self, layer_sizes):\n",
        "        np.random.seed(self.random_state)\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            weight, bias = self.add_layer(layer_sizes[i], layer_sizes[i+1])\n",
        "\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(bias)\n",
        "\n",
        "    def add_layer(self, input_size, output_size):\n",
        "        # random weights and biases between -1 and 1\n",
        "        weight = np.array(((np.random.rand(input_size, output_size) - 0.5) * 2), dtype=np.float64)\n",
        "        bias = np.array(((np.random.rand(1, output_size) - 0.5) * 2), dtype=np.float64)\n",
        "\n",
        "        return weight, bias\n",
        "\n",
        "    def first_pass(self, layer_sizes):\n",
        "        self.num_layers = len(layer_sizes)\n",
        "\n",
        "        self.weight_initialization(layer_sizes)\n",
        "\n",
        "    def forward(self, activations):\n",
        "        activations[0] = self.hidden_layer_activation(activations[0])\n",
        "\n",
        "        for i in range(self.num_layers-1):\n",
        "            # if not output layer, apply hidden layer activation function\n",
        "            if i != self.num_layers-2:\n",
        "                activations[i+1] = self.hidden_layer_activation(np.dot(activations[i], self.weights[i]) + self.biases[i])\n",
        "            else:\n",
        "                activations[i+1] = self.output_layer_activation(np.dot(activations[i], self.weights[i]) + self.biases[i])\n",
        "\n",
        "        return activations\n",
        "\n",
        "    def backward(self, activations, deltas, y, current_batch_size):\n",
        "        loss = self.loss_function(y, activations[-1])\n",
        "\n",
        "        deltas[self.num_layers-2] = (activations[-1] - y) * self.output_layer_activation_derivative(activations[-1])\n",
        "\n",
        "        self.weights[self.num_layers-2] -= (np.dot(activations[self.num_layers-2].T, deltas[self.num_layers-2]).astype(np.float64) / current_batch_size) * self.learning_rate\n",
        "        self.biases[self.num_layers-2] -= (np.sum(deltas[self.num_layers-2], axis=0).astype(np.float64) / current_batch_size) * self.learning_rate\n",
        "\n",
        "        for i in range(self.num_layers-2, 0, -1):\n",
        "            deltas[i-1] = np.dot(deltas[i], self.weights[i].T) * self.hidden_layer_activation_derivative(activations[i])\n",
        "\n",
        "            self.weights[i-1] -= (np.dot(activations[i-1].T, deltas[i-1]).astype(np.float64) / current_batch_size) * self.learning_rate\n",
        "            self.biases[i-1] -= (np.sum(deltas[i-1], axis=0).astype(np.float64) / current_batch_size) * self.learning_rate\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self, plot_losses=False):\n",
        "        if not hasattr(self, 'X_train') or not hasattr(self, 'y_train'):\n",
        "            raise AttributeError('X_train and y_train not found. Please run train test split first.')\n",
        "\n",
        "        self.plot_losses = plot_losses\n",
        "\n",
        "        # if y is 1D, convert to 2D\n",
        "        if len(self.y_train.shape) == 1:\n",
        "            self.y_train = self.y_train.reshape(-1, 1)\n",
        "\n",
        "        # Initialize weights and biases if it's the first pass\n",
        "        if self.is_first_pass:\n",
        "            self.first_pass([self.X_train.shape[1]] + list(self.hidden_layer_sizes) + [self.y_train.shape[1]])\n",
        "            self.is_first_pass = False\n",
        "\n",
        "        # Train using the optimization algorithm set while initializing the model\n",
        "        self.optim_algo(self.X_train, self.y_train)\n",
        "\n",
        "        if plot_losses:\n",
        "            plt.plot(self.train_losses, label='Train Loss')\n",
        "            plt.plot(self.test_losses, label='Test Loss')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "    def predict(self, X):\n",
        "        activation = X\n",
        "\n",
        "        activation = self.hidden_layer_activation(activation)\n",
        "\n",
        "        for i in range(self.num_layers-1):\n",
        "            activation = np.dot(activation, self.weights[i]) + self.biases[i]\n",
        "\n",
        "            if i != self.num_layers-2:\n",
        "                activation = self.hidden_layer_activation(activation)\n",
        "            else:\n",
        "                activation = self.output_layer_activation(activation)\n",
        "\n",
        "        return activation\n",
        "\n",
        "    def score(self, X, y):\n",
        "        # Get the prediction of the model on the test data\n",
        "        y_pred = self.predict(X)\n",
        "\n",
        "        # return accuracy\n",
        "        return self.loss_function(y, y_pred)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nxEqZGYJSqB"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_NMjRDJSqB"
      },
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs5Hz-dlJSqB",
        "outputId": "b651b4ac-4aac-4052-d24a-639d3adaa43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'key', 'fare_amount', 'pickup_datetime',\n",
            "       'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
            "       'dropoff_latitude', 'passenger_count'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/taxi.csv')\n",
        "\n",
        "# print column names\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta-OWrQMJSqC"
      },
      "source": [
        "## Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISep1rIgJSqC",
        "outputId": "5f2946f5-75bb-4ba7-c373-d9687f8ef618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 7)\n",
            "[7.5 '2015-05-07 19:52:06 UTC' -73.99981689453125 40.73835372924805\n",
            " -73.99951171875 40.72321701049805 1]\n",
            "[7.5 71526.0 -73.99981689453125 40.73835372924805 -73.99951171875\n",
            " 40.72321701049805 1]\n"
          ]
        }
      ],
      "source": [
        "# from the df remove first 2 columns\n",
        "data = data.iloc[:, 2:]\n",
        "\n",
        "# convert to numpy array\n",
        "data = data.values\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "print(data[0])\n",
        "\n",
        "# for each data point, the second column is a date time object of the format 'YYYY-MM-DD HH:MM:SS UTC', convert this to a float denoting hours*3600 + mins*60 + seconds\n",
        "data[:, 1] = [x.split(\" \") for x in data[:, 1]]\n",
        "data[:, 1] = [np.float64(x[1].split(\":\")[0])*3600 + np.float64(x[1].split(\":\")[1])*60 + np.float64(x[1].split(\":\")[2]) for x in data[:, 1]]\n",
        "\n",
        "\n",
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcO58Z2yJSqD",
        "outputId": "425dee61-dbac-450a-a8e2-ad18955e6846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: (200000, 6), y: (200000,)\n",
            "X_train: (160000, 6), y_train: (160000, 1)\n"
          ]
        }
      ],
      "source": [
        "# fares is the first column values\n",
        "y = data[:, 0]\n",
        "\n",
        "# features are the rest of the columns\n",
        "X = data[:, 1:]\n",
        "\n",
        "print(f\"X: {X.shape}, y: {y.shape}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape y_train and y_test to be 2D arrays so that np computations can be done without issues\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "\n",
        "del data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAvE-APyJSqD"
      },
      "source": [
        "\n",
        "## Case 1\n",
        "\n",
        "* No of hidden layers: 1\n",
        "* No. of neurons in hidden layer: 32\n",
        "* Activation function in the hidden layer: Sigmoid\n",
        "* 1 neuron in the output layer.\n",
        "* Activation function in the output layer: Linear\n",
        "* Optimisation algorithm: Mini Batch Stochastic Gradient Descent (SGD)\n",
        "* Loss function: Mean Squared Error (MSE)\n",
        "* Learning rate: 0.01\n",
        "* No. of epochs = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNxZa_KgJSqD"
      },
      "source": [
        "### Custom Class Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NQjo12nBJSqD"
      },
      "outputs": [],
      "source": [
        "CustomNN = CNN(\n",
        "        hidden_layer_sizes=(32,),\n",
        "        hidden_layer_activation='logistic',\n",
        "        output_layer_activation='linear',\n",
        "        optim_algo='SGD',\n",
        "        loss_function='MSE',\n",
        "        learning_rate=0.01,\n",
        "        epochs=200,\n",
        "        random_state=42,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9-Q5_k2JSqE"
      },
      "source": [
        "Training the model using X_train and y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "McW_Q_jmJSqE"
      },
      "outputs": [],
      "source": [
        "CustomNN.train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMV8NtNJJSqE"
      },
      "source": [
        "Testing the model using X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tLz-NqpZJSqE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "6146128f-e0df-46a3-b9d0-aebf2965274a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1013e16d79fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCustomNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train score for the custom model: {CustomNN.predict(X_train)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-40b431cbfba5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, plot_losses)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Train using the optimization algorithm set while initializing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-40b431cbfba5>\u001b[0m in \u001b[0;36mmsgd\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-40b431cbfba5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# if not output layer, apply hidden layer activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-40b431cbfba5>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "CustomNN.train(plot_losses=False)\n",
        "\n",
        "print(f\"train score for the custom model: {CustomNN.predict(X_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_score = CustomNN.score(X_test, y_test)\n",
        "\n",
        "print(f\"test score for the custom model: {test_score}\")"
      ],
      "metadata": {
        "id": "5rxdC3ZNaiN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5TU6kKRJSqE"
      },
      "source": [
        "### Sklearn Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQp9LrtYJSqF"
      },
      "outputs": [],
      "source": [
        "MLP = MLPRegressor(\n",
        "    hidden_layer_sizes=(32,),\n",
        "    activation='logistic',\n",
        "    solver='sgd',\n",
        "    learning_rate_init=0.01,\n",
        "    max_iter=200,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUu_OZ8cJSqF"
      },
      "source": [
        "Training the model using X_train and y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgO8KlxLJSqF"
      },
      "outputs": [],
      "source": [
        "MLP.fit(X_train, y_train)\n",
        "\n",
        "print(f\"train score for the MLP model: {MLP.score(X_train, y_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"train score for the MLP model: {MLP.score(X_train, y_train)}\")"
      ],
      "metadata": {
        "id": "XzGjmf53NKG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtecmA9TJSqG"
      },
      "source": [
        "Testing the model using X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDjK3mmEJSqG"
      },
      "outputs": [],
      "source": [
        "MLP_test_score = MLP.score(X_test, y_test)\n",
        "\n",
        "print(f\"test score for the MLP model: {MLP_test_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKQcny_uJSqG"
      },
      "source": [
        "## Case 2\n",
        "* No of hidden layers: 2\n",
        "* No. of neurons in the 1st hidden layer: 64\n",
        "* No. of neurons in the 2nd hidden layer: 32\n",
        "* Activation function in both the hidden layers: ReLU\n",
        "* 1 neuron in the output layer.\n",
        "* Activation function in the output layer: Linear\n",
        "* Optimisation algorithm: Mini Batch Stochastic Gradient Descent (SGD)\n",
        "* Loss function: Mean Squared Error (MSE)\n",
        "* Learning rate: 0.01\n",
        "* No. of epochs = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Class Implementation"
      ],
      "metadata": {
        "id": "IbrPxxHkMxNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CustomNN = CNN(\n",
        "        hidden_layer_sizes=(64, 32,),\n",
        "        hidden_layer_activation='ReLU',\n",
        "        output_layer_activation='linear',\n",
        "        optim_algo='SGD',\n",
        "        loss_function='MSE',\n",
        "        learning_rate=0.01,\n",
        "        epochs=200,\n",
        "        random_state=42,\n",
        "    )"
      ],
      "metadata": {
        "id": "HfDH2pZiM1dv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}