{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install itertools\n",
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import filterwarnings\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an np array x and return values at indices\n",
    "def get_val_at_indices(x, indices):\n",
    "    return x[indices]\n",
    "\n",
    "def train_test_split(*arrays, test_size=0.25, shufffle=True, random_state=1):\n",
    "    # get length of first array\n",
    "    length = len(arrays[0])\n",
    "\n",
    "    # split lengths\n",
    "    len_test = int(np.ceil(length*test_size))\n",
    "    len_train = length - len_test\n",
    "\n",
    "    if shufffle:\n",
    "        perm = np.random.RandomState(random_state).permutation(length)\n",
    "        test_indices = perm[:len_test]\n",
    "        train_indices = perm[len_test:]\n",
    "    else:\n",
    "        train_indices = np.arange(len_train)\n",
    "        test_indices = np.arange(len_train, length)\n",
    "\n",
    "    return list(chain.from_iterable((get_val_at_indices(x, train_indices), get_val_at_indices(x, test_indices)) for x in arrays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, hidden_layer_sizes=(100,), learning_rate=0.1, epochs=200, batch_size=32, random_state=1, tol=1e-4, hidden_layer_activation='logistic', output_layer_activation='linear', optim_algo='SGD', loss_function='MSE', early_stopping=False):\n",
    "        # learning rate should be casted to np.float64\n",
    "        self.learning_rate = np.float64(learning_rate)\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.tol = tol\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.is_first_pass = True\n",
    "        if hidden_layer_activation == 'logistic':\n",
    "            self.hidden_layer_activation = self.sigmoid\n",
    "            self.hidden_layer_activation_derivative = self.sigmoid_derivative\n",
    "        elif hidden_layer_activation == 'ReLU':\n",
    "            self.hidden_layer_activation = self.ReLU\n",
    "            self.hidden_layer_activation_derivative = self.ReLU_derivative\n",
    "        elif hidden_layer_activation == 'linear':\n",
    "            self.hidden_layer_activation = self.linear\n",
    "            self.hidden_layer_activation_derivative = self.linear_derivative\n",
    "        else:\n",
    "            # default to sigmoid\n",
    "            self.hidden_layer_activation = self.sigmoid\n",
    "            self.hidden_layer_activation_derivative = self.sigmoid_derivative\n",
    "\n",
    "        if output_layer_activation == 'logistic':\n",
    "            self.output_layer_activation = self.sigmoid\n",
    "            self.output_layer_activation_derivative = self.sigmoid_derivative\n",
    "        elif output_layer_activation == 'ReLU':\n",
    "            self.output_layer_activation = self.ReLU\n",
    "            self.output_layer_activation_derivative = self.ReLU_derivative\n",
    "        elif output_layer_activation == 'linear':\n",
    "            self.output_layer_activation = self.linear\n",
    "            self.output_layer_activation_derivative = self.linear_derivative\n",
    "        else:\n",
    "            # default to sigmoid\n",
    "            self.output_layer_activation = self.sigmoid\n",
    "            self.output_layer_activation_derivative = self.sigmoid_derivative\n",
    "\n",
    "        if optim_algo == 'SGD':\n",
    "            self.optim_algo = self.msgd\n",
    "    \n",
    "        if loss_function == 'MSE':\n",
    "            self.loss_function = self.MSE\n",
    "\n",
    "        self.early_stopping = early_stopping\n",
    "\n",
    "    # train test split to store the train and test data\n",
    "    def train_test_split(self, *arrays, test_size=0.25, shufffle=True, random_state=1):\n",
    "        # get length of first array\n",
    "        length = len(arrays[0])\n",
    "\n",
    "        # split lengths\n",
    "        len_test = int(np.ceil(length*test_size))\n",
    "        len_train = length - len_test\n",
    "\n",
    "        if shufffle:\n",
    "            perm = np.random.RandomState(random_state).permutation(length)\n",
    "            test_indices = perm[:len_test]\n",
    "            train_indices = perm[len_test:]\n",
    "        else:\n",
    "            train_indices = np.arange(len_train)\n",
    "            test_indices = np.arange(len_train, length)\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = list(chain.from_iterable((get_val_at_indices(x, train_indices), get_val_at_indices(x, test_indices)) for x in arrays))\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def sigmoid_derivative(self, X):\n",
    "        return X * (1-X)\n",
    "    \n",
    "    def ReLU(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def ReLU_derivative(self, X):\n",
    "        return 1 * (X > 0)\n",
    "    \n",
    "    def linear(self, X):\n",
    "        return X\n",
    "    \n",
    "    def linear_derivative(self, X):\n",
    "        return 1\n",
    "\n",
    "    def load_data(self, X, y):\n",
    "        mini_batches = []\n",
    "\n",
    "        num_batches = int(np.ceil(X.shape[0] / self.batch_size))\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * self.batch_size\n",
    "            end = start + self.batch_size\n",
    "\n",
    "            if end > X.shape[0]:\n",
    "                end = X.shape[0]\n",
    "\n",
    "            mini_batches.append((X[start:end], y[start:end]))\n",
    "\n",
    "        return mini_batches\n",
    "\n",
    "    # MSE loss\n",
    "    def MSE(self, y, y_pred):\n",
    "        return np.mean((y - y_pred)**2)\n",
    "\n",
    "    def msgd(self, X, y):\n",
    "        activations = [X] + [None]*(self.num_layers-1)\n",
    "        deltas = [None] * (self.num_layers-1)\n",
    "\n",
    "        mini_batches = self.load_data(X, y)\n",
    "\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            epoch_loss_train = 0\n",
    "            for mini_batch in mini_batches:\n",
    "                X_mini, y_mini = mini_batch\n",
    "\n",
    "                activations[0] = X_mini\n",
    "\n",
    "                self.forward(activations)\n",
    "\n",
    "                epoch_loss += self.backward(activations, deltas, y_mini, len(X_mini)) * len(X_mini)\n",
    "\n",
    "            if i%10 == 9:\n",
    "                train_losses.append(epoch_loss / X.shape[0])\n",
    "                y_pred = self.predict(self.X_test)\n",
    "                test_losses.append(self.loss_function(self.y_test, y_pred) / self.X_test.shape[0])\n",
    "\n",
    "            # If early stopping is enabled, stop if loss is less than tolerance\n",
    "            if self.early_stopping:\n",
    "                if epoch_loss < self.tol:\n",
    "                    break\n",
    "\n",
    "        self.train_losses = train_losses\n",
    "        self.test_losses = test_losses\n",
    "\n",
    "    def weight_initialization(self, layer_sizes):\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            weight, bias = self.add_layer(layer_sizes[i], layer_sizes[i+1])\n",
    "\n",
    "        self.weights.append(weight)\n",
    "        self.biases.append(bias)\n",
    "    \n",
    "    def add_layer(self, input_size, output_size):\n",
    "        weight = np.random.RandomState.uniform(-1, 1, (input_size, output_size))\n",
    "        bias = np.random.RandomState.uniform(-1, 1, (1, output_size))\n",
    "\n",
    "        weight = np.array(weight, dtype=np.float64)\n",
    "        bias = np.array(bias, dtype=np.float64)\n",
    "\n",
    "        return weight, bias\n",
    "\n",
    "    def first_pass(self, y, layer_sizes):\n",
    "        self.num_layers = len(layer_sizes)\n",
    "\n",
    "        self.weight_initialization(layer_sizes)\n",
    "\n",
    "    def forward(self, activations):\n",
    "        activations[0] = self.hidden_layer_activation(activations[0])\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            activations[i+1] = np.dot(activations[i], self.weights[i]) + self.biases[i]\n",
    "\n",
    "            # if not output layer, apply activation function\n",
    "            if i != self.num_layers-2:\n",
    "                activations[i+1] = self.hidden_layer_activation(activations[i+1])\n",
    "            else:\n",
    "                activations[i+1] = self.output_layer_activation(activations[i+1])\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def backward(self, activations, deltas, y, current_batch_size):\n",
    "        loss = self.loss_function(y, activations[-1])\n",
    "\n",
    "        deltas[self.num_layers-2] = (activations[-1] - y) * self.output_layer_activation_derivative(self.activations[-1])\n",
    "\n",
    "        weights_gradient = np.dot(activations[self.num_layers-2].T, deltas[self.num_layers-2]) / current_batch_size\n",
    "        biases_gradient = np.sum(deltas[self.num_layers-2], axis=0) / current_batch_size\n",
    "\n",
    "        self.weights[self.num_layers-2] += self.learning_rate * weights_gradient\n",
    "        self.biases[self.num_layers-2] += self.learning_rate * biases_gradient\n",
    "\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            deltas[i-1] = np.dot(deltas[i], self.weights[i].T) * self.hidden_layer_activation_derivative(activations[i])\n",
    "\n",
    "            weights_gradient = np.dot(activations[i-1].T, deltas[i-1]) / current_batch_size\n",
    "            biases_gradient = np.sum(deltas[i-1], axis=0) / current_batch_size\n",
    "\n",
    "            self.weights[i-1] += self.learning_rate * weights_gradient\n",
    "            self.biases[i-1] += self.learning_rate * biases_gradient\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, plot_test_losses=False):\n",
    "        self.plot_test_losses = plot_test_losses\n",
    "\n",
    "        X = self.X_train\n",
    "        y = self.y_train\n",
    "        # if y is 1D, convert to 2D\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        if len(y_test.shape) == 1:\n",
    "            y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "        # Initialize weights and biases if it's the first pass\n",
    "        if self.is_first_pass:\n",
    "            self.first_pass(y, [X.shape[1]] + list(self.hidden_layer_sizes) + [y.shape[1]])\n",
    "            self.is_first_pass = False\n",
    "\n",
    "        # Train using the optimization algorithm set while initializing the model\n",
    "        self.optim_algo(X, y)\n",
    "\n",
    "        if plot_test_losses:\n",
    "            plt.plot(self.train_losses, label='Train Loss')\n",
    "            plt.plot(self.test_losses, label='Test Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        activation = X\n",
    "\n",
    "        activation = self.hidden_layer_activation(activation) \n",
    "        \n",
    "        for i in range(self.num_layers-1):\n",
    "            activation = np.dot(activation, self.weights[i]) + self.biases[i]\n",
    "\n",
    "            if i != self.num_layers-2:\n",
    "                activation = self.hidden_layer_activation(activation)\n",
    "            else:\n",
    "                activation = self.output_layer_activation(activation)\n",
    "\n",
    "        return activation\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # Get the prediction of the model on the test data\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # The score is the loss function value between the true and predicted values\n",
    "        return self.loss_function(y, y_pred) / X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./taxi.csv')\n",
    "\n",
    "# print column names\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the df remove first 2 columns\n",
    "data = data.iloc[:, 2:]\n",
    "\n",
    "# convert to numpy array\n",
    "data = data.values\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "print(data[0])\n",
    "\n",
    "# for each data point, the second column is a date time object of the format 'YYYY-MM-DD HH:MM:SS UTC', convert this to a unix timestamp \n",
    "data[:, 1] = pd.to_datetime(data[:, 1]).astype('int64') / 10**9\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fares is the first column values\n",
    "y = data[:, 0]\n",
    "\n",
    "# features are the rest of the columns\n",
    "X = data[:, 1:]\n",
    "\n",
    "print(f\"X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape y_train and y_test to be 2D arrays so that np computations can be done without issues\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_train: (3, 2), output_train: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# smaller example of size 64\n",
    "inputs = [[x for x in range(64)]]\n",
    "outputs = [[x for x in range(64)]]\n",
    "\n",
    "input_train, input_test, output_train, output_test = train_test_split(inputs, outputs, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"input_train: {input_train.shape}, output_train: {output_train.shape}\")\n",
    "\n",
    "# Create a CNN model with one hidden layer of 3 neurons\n",
    "model = CNN(hidden_layer_sizes=(3,), learning_rate=0.1, epochs=200, batch_size=32, random_state=1, tol=1e-4, hidden_layer_activation='logistic', output_layer_activation='linear', optim_algo='SGD', loss_function='MSE', early_stopping=False)\n",
    "\n",
    "# Train the model on the smaller example\n",
    "model.train(input_train, output_train)\n",
    "\n",
    "# Get the score of the model on the test data\n",
    "score = model.score(input_test, output_test)\n",
    "\n",
    "print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1\n",
    "\n",
    "* No of hidden layers: 1\n",
    "* No. of neurons in hidden layer: 32\n",
    "* Activation function in the hidden layer: Sigmoid\n",
    "* 1 neuron in the output layer.\n",
    "* Activation function in the output layer: Linear\n",
    "* Optimisation algorithm: Mini Batch Stochastic Gradient Descent (SGD)\n",
    "* Loss function: Mean Squared Error (MSE)\n",
    "* Learning rate: 0.01\n",
    "* No. of epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN = CNN(\n",
    "        hidden_layer_sizes=(32,), \n",
    "        hidden_layer_activation='logistic', \n",
    "        output_layer_activation='linear',\n",
    "        optim_algo='SGD',\n",
    "        loss_function='MSE',\n",
    "        learning_rate=0.01,\n",
    "        epochs=200,\n",
    "        random_state=42,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN.train(X_train, y_train)\n",
    "\n",
    "print(f\"train score for the custom model: {CustomNN.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model using X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = CustomNN.score(X_test, y_test)\n",
    "\n",
    "print(f\"test score for the custom model: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPRegressor(\n",
    "    hidden_layer_sizes=(32,),\n",
    "    activation='logistic',\n",
    "    solver='sgd',\n",
    "    learning_rate_init=0.01,\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.fit(X_train, y_train)\n",
    "\n",
    "print(f\"train score for the MLP model: {MLP.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model using X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_test_score = MLP.score(X_test, y_test)\n",
    "\n",
    "print(f\"test score for the MLP model: {MLP_test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2\n",
    "* No of hidden layers: 2\n",
    "* No. of neurons in the 1st hidden layer: 64\n",
    "* No. of neurons in the 2nd hidden layer: 32\n",
    "* Activation function in both the hidden layers: ReLU\n",
    "* 1 neuron in the output layer.\n",
    "* Activation function in the output layer: Linear\n",
    "* Optimisation algorithm: Mini Batch Stochastic Gradient Descent (SGD)\n",
    "* Loss function: Mean Squared Error (MSE)\n",
    "* Learning rate: 0.01\n",
    "* No. of epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN = CNN(\n",
    "                hidden_layer_sizes=(64, 32,),\n",
    "                hidden_layer_activation='ReLU',\n",
    "                output_layer_activation='linear',\n",
    "                optim_algo='SGD',\n",
    "                loss_function='MSE',\n",
    "                learning_rate=0.01,\n",
    "                epochs=200,\n",
    "                random_state=42,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN.train(X_train, y_train)\n",
    "\n",
    "print(f\"train score for the custom model: {CustomNN.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model using X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN_test_score = CustomNN.score(X_test, y_test)\n",
    "\n",
    "print(f\"test score for the custom model: {CustomNN_test_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
