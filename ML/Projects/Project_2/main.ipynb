{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install itertools\n",
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import filterwarnings\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an np array x and return values at indices\n",
    "def get_val_at_indices(x, indices):\n",
    "    return x[indices]\n",
    "\n",
    "def train_test_split(*arrays, test_size=0.25, shufffle=True, random_state=1):\n",
    "    # get length of first array\n",
    "    length = len(arrays[0])\n",
    "\n",
    "    # split lengths\n",
    "    len_test = int(np.ceil(length*test_size))\n",
    "    len_train = length - len_test\n",
    "\n",
    "    if shufffle:\n",
    "        perm = np.random.RandomState(random_state).permutation(length)\n",
    "        test_indices = perm[:len_test]\n",
    "        train_indices = perm[len_test:]\n",
    "    else:\n",
    "        train_indices = np.arange(len_train)\n",
    "        test_indices = np.arange(len_train, length)\n",
    "\n",
    "    return list(chain.from_iterable((get_val_at_indices(x, train_indices), get_val_at_indices(x, test_indices)) for x in arrays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, hidden_layer_sizes=(100,), learning_rate=0.1, epochs=200, batch_size=32, random_state=1, tol=1e-4, hidden_layer_activation='logistic', output_layer_activation='linear', optim_algo='SGD', loss_function='MSE', early_stopping=False):\n",
    "        # learning rate should be casted to np.float64\n",
    "        self.learning_rate = np.float64(learning_rate)\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.tol = tol\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.is_first_pass = True\n",
    "        if hidden_layer_activation == 'logistic':\n",
    "            self.hidden_layer_activation = self.sigmoid\n",
    "            self.hidden_layer_activation_derivative = self.sigmoid_derivative\n",
    "        elif hidden_layer_activation == 'ReLU':\n",
    "            self.hidden_layer_activation = self.ReLU\n",
    "            self.hidden_layer_activation_derivative = self.ReLU_derivative\n",
    "        elif hidden_layer_activation == 'linear':\n",
    "            self.hidden_layer_activation = self.linear\n",
    "            self.hidden_layer_activation_derivative = self.linear_derivative\n",
    "        else:\n",
    "            # default to sigmoid\n",
    "            self.hidden_layer_activation = self.sigmoid\n",
    "            self.hidden_layer_activation_derivative = self.sigmoid_derivative\n",
    "\n",
    "        if output_layer_activation == 'logistic':\n",
    "            self.output_layer_activation = self.sigmoid\n",
    "            self.output_layer_activation_derivative = self.sigmoid_derivative\n",
    "        elif output_layer_activation == 'ReLU':\n",
    "            self.output_layer_activation = self.ReLU\n",
    "            self.output_layer_activation_derivative = self.ReLU_derivative\n",
    "        elif output_layer_activation == 'linear':\n",
    "            self.output_layer_activation = self.linear\n",
    "            self.output_layer_activation_derivative = self.linear_derivative\n",
    "        else:\n",
    "            # default to sigmoid\n",
    "            self.output_layer_activation = self.sigmoid\n",
    "            self.output_layer_activation_derivative = self.sigmoid_derivative\n",
    "\n",
    "        if optim_algo == 'SGD':\n",
    "            self.optim_algo = self.msgd\n",
    "    \n",
    "        if loss_function == 'MSE':\n",
    "            self.loss_function = self.MSE\n",
    "\n",
    "        self.early_stopping = early_stopping\n",
    "\n",
    "    # train test split to store the train and test data\n",
    "    def train_test_split(self, *arrays, test_size=0.25, shufffle=True, random_state=1):\n",
    "        # get length of first array\n",
    "        length = len(arrays[0])\n",
    "\n",
    "        # split lengths\n",
    "        len_test = int(np.ceil(length*test_size))\n",
    "        len_train = length - len_test\n",
    "\n",
    "        if shufffle:\n",
    "            perm = np.random.RandomState(random_state).permutation(length)\n",
    "            test_indices = perm[:len_test]\n",
    "            train_indices = perm[len_test:]\n",
    "        else:\n",
    "            train_indices = np.arange(len_train)\n",
    "            test_indices = np.arange(len_train, length)\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = list(chain.from_iterable((get_val_at_indices(x, train_indices), get_val_at_indices(x, test_indices)) for x in arrays))\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X.astype(np.float64)))\n",
    "    \n",
    "    def sigmoid_derivative(self, X):\n",
    "        return X * (1-X)\n",
    "    \n",
    "    def ReLU(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def ReLU_derivative(self, X):\n",
    "        return 1 * (X > 0)\n",
    "    \n",
    "    def linear(self, X):\n",
    "        return X\n",
    "    \n",
    "    def linear_derivative(self, X):\n",
    "        return 1\n",
    "\n",
    "    def load_data(self, X, y):\n",
    "        mini_batches = []\n",
    "\n",
    "        num_batches = int(np.ceil(X.shape[0] / self.batch_size))\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * self.batch_size\n",
    "            end = start + self.batch_size\n",
    "\n",
    "            if end > X.shape[0]:\n",
    "                end = X.shape[0]\n",
    "\n",
    "            mini_batches.append(((start, end)))\n",
    "\n",
    "        return mini_batches\n",
    "\n",
    "    # MSE loss\n",
    "    def MSE(self, y, y_pred):\n",
    "        return np.mean((y - y_pred)**2)\n",
    "\n",
    "    def msgd(self, X, y):\n",
    "        activations = [None]*(self.num_layers)\n",
    "        deltas = [None] * (self.num_layers-1)\n",
    "\n",
    "        mini_batches = self.load_data(X, y)\n",
    "\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "            for mini_batch in mini_batches:\n",
    "                start, end = mini_batch\n",
    "\n",
    "                activations[0] = self.X_train[start:end]\n",
    "\n",
    "                self.forward(activations)\n",
    "\n",
    "                epoch_loss += self.backward(activations, deltas, self.y_train[start:end], (end-start+1)) * (end-start+1)\n",
    "\n",
    "            if i%10 == 9 and self.plot_losses:\n",
    "                train_losses.append(epoch_loss / X.shape[0])\n",
    "                y_pred = self.predict(self.X_test)\n",
    "                test_losses.append(self.loss_function(self.y_test, y_pred) / self.X_test.shape[0])\n",
    "\n",
    "            # If early stopping is enabled, stop if loss is less than tolerance\n",
    "            if self.early_stopping:\n",
    "                if epoch_loss < self.tol:\n",
    "                    break\n",
    "\n",
    "        self.train_losses = train_losses\n",
    "        self.test_losses = test_losses\n",
    "\n",
    "    def weight_initialization(self, layer_sizes):\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            weight, bias = self.add_layer(layer_sizes[i], layer_sizes[i+1])\n",
    "\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "    \n",
    "    def add_layer(self, input_size, output_size):\n",
    "        # random weights and biases between -1 and 1\n",
    "        weight = np.array(((np.random.rand(input_size, output_size) - 0.5) * 2), dtype=np.float64)\n",
    "        bias = np.array(((np.random.rand(1, output_size) - 0.5) * 2), dtype=np.float64)\n",
    "\n",
    "        return weight, bias\n",
    "\n",
    "    def first_pass(self, layer_sizes):\n",
    "        self.num_layers = len(layer_sizes)\n",
    "\n",
    "        self.weight_initialization(layer_sizes)\n",
    "\n",
    "    def forward(self, activations):\n",
    "        activations[0] = self.hidden_layer_activation(activations[0])\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            # if not output layer, apply hidden layer activation function\n",
    "            if i != self.num_layers-2:\n",
    "                activations[i+1] = self.hidden_layer_activation(np.dot(activations[i], self.weights[i]) + self.biases[i])\n",
    "            else:\n",
    "                activations[i+1] = self.output_layer_activation(np.dot(activations[i], self.weights[i]) + self.biases[i])\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def backward(self, activations, deltas, y, current_batch_size):\n",
    "        loss = self.loss_function(y, activations[-1])\n",
    "\n",
    "        deltas[self.num_layers-2] = (activations[-1] - y) * self.output_layer_activation_derivative(activations[-1])\n",
    "\n",
    "        self.weights[self.num_layers-2] -= (np.dot(activations[self.num_layers-2].T, deltas[self.num_layers-2]).astype(np.float64) / current_batch_size) * self.learning_rate\n",
    "        self.biases[self.num_layers-2] -= (np.sum(deltas[self.num_layers-2], axis=0).astype(np.float64) / current_batch_size) * self.learning_rate\n",
    "\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            deltas[i-1] = np.dot(deltas[i], self.weights[i].T) * self.hidden_layer_activation_derivative(activations[i])\n",
    "\n",
    "            self.weights[i-1] -= (np.dot(activations[i-1].T, deltas[i-1]).astype(np.float64) / current_batch_size) * self.learning_rate\n",
    "            self.biases[i-1] -= (np.sum(deltas[i-1], axis=0).astype(np.float64) / current_batch_size) * self.learning_rate\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, plot_losses=False):\n",
    "        if not hasattr(self, 'X_train') or not hasattr(self, 'y_train'):\n",
    "            raise AttributeError('X_train and y_train not found. Please run train test split first.')\n",
    "\n",
    "        self.plot_losses = plot_losses\n",
    "\n",
    "        # if y is 1D, convert to 2D\n",
    "        if len(self.y_train.shape) == 1:\n",
    "            self.y_train = self.y_train.reshape(-1, 1)\n",
    "\n",
    "        # Initialize weights and biases if it's the first pass\n",
    "        if self.is_first_pass:\n",
    "            self.first_pass([self.X_train.shape[1]] + list(self.hidden_layer_sizes) + [self.y_train.shape[1]])\n",
    "            self.is_first_pass = False\n",
    "\n",
    "        # Train using the optimization algorithm set while initializing the model\n",
    "        self.optim_algo(self.X_train, self.y_train)\n",
    "\n",
    "        if plot_losses:\n",
    "            plt.plot(self.train_losses, label='Train Loss')\n",
    "            plt.plot(self.test_losses, label='Test Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        activation = X\n",
    "\n",
    "        activation = self.hidden_layer_activation(activation) \n",
    "        \n",
    "        for i in range(self.num_layers-1):\n",
    "            activation = np.dot(activation, self.weights[i]) + self.biases[i]\n",
    "\n",
    "            if i != self.num_layers-2:\n",
    "                activation = self.hidden_layer_activation(activation)\n",
    "            else:\n",
    "                activation = self.output_layer_activation(activation)\n",
    "\n",
    "        return activation\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # Get the prediction of the model on the test data\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # return accuracy\n",
    "        return self.loss_function(y, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'key', 'fare_amount', 'pickup_datetime',\n",
      "       'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
      "       'dropoff_latitude', 'passenger_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./taxi.csv')\n",
    "\n",
    "# print column names\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from the df remove first 2 columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# convert to numpy array\u001b[39;00m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# from the df remove first 2 columns\n",
    "data = data.iloc[:, 2:]\n",
    "\n",
    "# convert to numpy array\n",
    "data = data.values\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "print(data[0])\n",
    "\n",
    "# for each data point, the second column is a date time object of the format 'YYYY-MM-DD HH:MM:SS UTC', convert this to a float denoting hours*3600 + mins*60 + seconds \n",
    "data[:, 1] = [x.split(\" \") for x in data[:, 1]]\n",
    "data[:, 1] = [np.float64(x[1].split(\":\")[0])*3600 + np.float64(x[1].split(\":\")[1])*60 + np.float64(x[1].split(\":\")[2]) for x in data[:, 1]]\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (200000, 6), y: (200000,)\n",
      "X_train: (160000, 6), y_train: (160000, 1)\n"
     ]
    }
   ],
   "source": [
    "# fares is the first column values\n",
    "y = data[:, 0]\n",
    "\n",
    "# features are the rest of the columns\n",
    "X = data[:, 1:]\n",
    "\n",
    "print(f\"X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape y_train and y_test to be 2D arrays so that np computations can be done without issues\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1\n",
    "\n",
    "* No of hidden layers: 1\n",
    "* No. of neurons in hidden layer: 32\n",
    "* Activation function in the hidden layer: Sigmoid\n",
    "* 1 neuron in the output layer.\n",
    "* Activation function in the output layer: Linear\n",
    "* Optimisation algorithm: Mini Batch Stochastic Gradient Descent (SGD)\n",
    "* Loss function: Mean Squared Error (MSE)\n",
    "* Learning rate: 0.01\n",
    "* No. of epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN = CNN(\n",
    "        hidden_layer_sizes=(32,), \n",
    "        hidden_layer_activation='logistic', \n",
    "        output_layer_activation='linear',\n",
    "        optim_algo='SGD',\n",
    "        loss_function='MSE',\n",
    "        learning_rate=0.01,\n",
    "        epochs=200,\n",
    "        random_state=42,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN.train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN.train(plot_losses=True)\n",
    "\n",
    "print(f\"train score for the custom model: {CustomNN.predict(X_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model using X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = CustomNN.score(X_test, y_test)\n",
    "\n",
    "print(f\"test score for the custom model: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPRegressor(\n",
    "    hidden_layer_sizes=(32,),\n",
    "    activation='logistic',\n",
    "    solver='sgd',\n",
    "    learning_rate_init=0.01,\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.fit(X_train, y_train)\n",
    "\n",
    "print(f\"train score for the MLP model: {MLP.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model using X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_test_score = MLP.score(X_test, y_test)\n",
    "\n",
    "print(f\"test score for the MLP model: {MLP_test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2\n",
    "* No of hidden layers: 2\n",
    "* No. of neurons in the 1st hidden layer: 64\n",
    "* No. of neurons in the 2nd hidden layer: 32\n",
    "* Activation function in both the hidden layers: ReLU\n",
    "* 1 neuron in the output layer.\n",
    "* Activation function in the output layer: Linear\n",
    "* Optimisation algorithm: Mini Batch Stochastic Gradient Descent (SGD)\n",
    "* Loss function: Mean Squared Error (MSE)\n",
    "* Learning rate: 0.01\n",
    "* No. of epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN = CNN(\n",
    "                hidden_layer_sizes=(64, 32,),\n",
    "                hidden_layer_activation='ReLU',\n",
    "                output_layer_activation='linear',\n",
    "                optim_algo='SGD',\n",
    "                loss_function='MSE',\n",
    "                learning_rate=0.01,\n",
    "                epochs=200,\n",
    "                random_state=42,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN.train(X_train, y_train)\n",
    "\n",
    "print(f\"train score for the custom model: {CustomNN.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model using X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomNN_test_score = CustomNN.score(X_test, y_test)\n",
    "\n",
    "print(f\"test score for the custom model: {CustomNN_test_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
