{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXXZ8ETcPfuow9zlC011Ds",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nespar7/Spring_24/blob/main/NLP/Assignments/Assignment_1/NLP_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "0r-rTccDuFr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64yWLxtXoOrG"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install contextualSpellCheck\n",
        "!pip install scikit-learn\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "YO4VRNdHoxSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import contextualSpellCheck\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from nltk import download as nltk_dw\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk_dw('punkt')"
      ],
      "metadata": {
        "id": "Nt8Cj2NMpmTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "opEEAC1UqVO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "contextualSpellCheck.add_to_pipe(nlp)"
      ],
      "metadata": {
        "id": "h34NMkubqXbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Definitions"
      ],
      "metadata": {
        "id": "Dk6ujq08qMEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_check(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    return doc._.performed_spellCheck, doc._.outcome_spellCheck"
      ],
      "metadata": {
        "id": "K9L5_aGpprWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spellings(csv_file, out_csv_file, column_name, should_log=False):\n",
        "    # read a csv file, in each line, see if spellCheck was performed(if text was correctly spelled) and print original and corrected texts\n",
        "    lines = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        # Index of the column to be spell checked\n",
        "        column_index = lines[0].index(column_name)\n",
        "\n",
        "        # For each line, perform spell check\n",
        "        for i in range(1, len(lines)):\n",
        "            line = lines[i][column_index]\n",
        "            performed, outcome = spell_check(line)\n",
        "\n",
        "\n",
        "            # If spell check was performed, update the line and print the original and corrected texts\n",
        "            if performed:\n",
        "                # Update the line\n",
        "                lines[i][column_index] = outcome\n",
        "\n",
        "                if should_log:\n",
        "                    print(line)\n",
        "                    print(outcome)\n",
        "                    print()\n",
        "\n",
        "            if i%100 == 0:\n",
        "                print(i)\n",
        "\n",
        "    with open(out_csv_file, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(lines)\n",
        "\n",
        "    print(\"Spell check completed\")"
      ],
      "metadata": {
        "id": "hJlxZytPw3oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process(csv_file, out_csv_file, column_name):\n",
        "    # read a csv file, in each line, remove characters apart from alphanumerics and whitespaces in the given column\n",
        "    lines = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        # Index of the column to be pre processed\n",
        "        column_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            lines[i][column_index] = ''.join([c for c in lines[i][column_index] if c.isalnum() or c.isspace()])\n",
        "\n",
        "    # write the pre-processed data to the resulting csv file\n",
        "    with open(out_csv_file, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(lines)\n",
        "\n",
        "    print(\"Pre procesing completed\")"
      ],
      "metadata": {
        "id": "iXrFFBtPsQIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary(model, csv_file, column_name):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    docs = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        doc_col_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            docs.append(lines[i][doc_col_index])\n",
        "\n",
        "        num_docs = len(docs)\n",
        "        lower_lim = 5\n",
        "        upper_lim = int(num_docs * 0.85)\n",
        "\n",
        "        # tokenize the documents using spacy and store the tokens in a vocabulary,\n",
        "        # remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "\n",
        "        doc_tokenized = []\n",
        "\n",
        "        # use the model to tokenize the documents\n",
        "        for doc in docs:\n",
        "            # process doc using the model passed\n",
        "            doc = model(doc)\n",
        "\n",
        "            # Select alphabetical tokens and remove stop words\n",
        "            tokens = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "            doc_tokenized.append(tokens)\n",
        "\n",
        "        # Now that we have the tokenized documents, we can create a vocabulary\n",
        "        vocabulary = {}\n",
        "\n",
        "        for doc in doc_tokenized:\n",
        "            for token in doc:\n",
        "                if token in vocabulary:\n",
        "                    vocabulary[token] += 1\n",
        "                else:\n",
        "                    vocabulary[token] = 1\n",
        "\n",
        "        # Remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "        vocabulary = [k for k, v in vocabulary.items() if v >= lower_lim and v <= upper_lim]\n",
        "\n",
        "        return vocabulary"
      ],
      "metadata": {
        "id": "AMSjXyLexJ9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_processed_text(model, csv_file, col_name):\n",
        "    # Read the csv file and return the processed text using the passed model\n",
        "    lines = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        col_index = lines[0].index(col_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            lines[i][col_index] = model(lines[i][col_index])\n",
        "\n",
        "    return lines[:][col_index]"
      ],
      "metadata": {
        "id": "nNLDLx_7f5tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stemmed_vocabulary(csv_file, col_name):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    docs = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        doc_col_index = lines[0].index(col_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            docs.append(lines[i][doc_col_index])\n",
        "\n",
        "    # tokenize the documents using nltk and store the tokens in a vocabulary,\n",
        "    # remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "\n",
        "    lower_lim = 5\n",
        "    upper_lim = int(len(docs) * 0.85)\n",
        "\n",
        "    doc_tokenized = []\n",
        "    stemmed_docs = []\n",
        "\n",
        "    # use nltk to tokenize the documents\n",
        "    for doc in docs:\n",
        "        doc = word_tokenize(doc)\n",
        "\n",
        "        tokens = [token.lower() for token in doc if token.isalpha()]\n",
        "\n",
        "        # stem the tokens\n",
        "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
        "\n",
        "        stemmed_docs.append(' '.join(stemmed_tokens))\n",
        "\n",
        "        doc_tokenized.append(stemmed_tokens)\n",
        "\n",
        "    vocabulary = {}\n",
        "\n",
        "    for doc in doc_tokenized:\n",
        "        for token in doc:\n",
        "            if token in vocabulary:\n",
        "                vocabulary[token] += 1\n",
        "            else:\n",
        "                vocabulary[token] = 1\n",
        "\n",
        "    # Remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "    stemmed_vocabulary = [k for k, v in vocabulary.items() if v >= lower_lim and v <= upper_lim]\n",
        "\n",
        "    return stemmed_vocabulary, stemmed_docs"
      ],
      "metadata": {
        "id": "pqW6THucFcwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_queries(csv_file, porter_stemmer):\n",
        "    # Read the spell checked docs csv file and store the queries in a list\n",
        "    queries = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        query_col_index = lines[0].index('query_text')\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            queries.append(lines[i][query_col_index])\n",
        "\n",
        "    stemmed_queries = []\n",
        "\n",
        "    # use nltk to tokenize the documents\n",
        "    for query in queries:\n",
        "        query = word_tokenize(query)\n",
        "\n",
        "        tokens = [token.lower() for token in query if token.isalpha()]\n",
        "\n",
        "        # stem the tokens\n",
        "        stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        stemmed_queries.append(' '.join(stemmed_tokens))\n",
        "\n",
        "    return stemmed_queries"
      ],
      "metadata": {
        "id": "IK51o0M6OvSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_texts(csv_file, id_col, text_col):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    ids = []\n",
        "    texts = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        id_col_index = lines[0].index(id_col)\n",
        "        text_col_index = lines[0].index(text_col)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            ids.append(lines[i][id_col_index])\n",
        "            texts.append(lines[i][text_col_index])\n",
        "\n",
        "    return ids, texts"
      ],
      "metadata": {
        "id": "J9UTSpYTrXO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_docs(qdrels):\n",
        "    # Get relevant docs list for each query from qdrels.csv file\n",
        "    relevant_docs = {}\n",
        "\n",
        "    with open(qdrels, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            line = lines[i]\n",
        "\n",
        "            query_id = line[1]\n",
        "            doc_id = line[2]\n",
        "\n",
        "            if query_id in relevant_docs:\n",
        "                relevant_docs[query_id].append(doc_id)\n",
        "            else:\n",
        "                relevant_docs[query_id] = [doc_id]\n",
        "\n",
        "    return relevant_docs"
      ],
      "metadata": {
        "id": "ciOjy6BSv6S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_k_score(query_id_idx_map, relevant_docs, top_docs, k):\n",
        "    # For each query in relevant_docs, calculate true positives in top_docs and return average precision_k score\n",
        "    precision_k = 0\n",
        "\n",
        "    for query_id, relevant_docs_list in relevant_docs.items():\n",
        "        true_positives = 0\n",
        "\n",
        "        query_idx = query_id_idx_map[query_id]\n",
        "\n",
        "        for doc in top_docs[query_idx]:\n",
        "            if doc in relevant_docs_list:\n",
        "                true_positives += 1\n",
        "\n",
        "        precision_k += true_positives / k\n",
        "\n",
        "    return precision_k / len(relevant_docs)"
      ],
      "metadata": {
        "id": "bb6tV5nK1R8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "jDvR6WfCtVT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CSV file locations need to be changed as applicable if being run locally."
      ],
      "metadata": {
        "id": "K9vjusFbu--4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_file = \"/content/Query_Doc/docs.csv\"\n",
        "queries_file = \"/content/Query_Doc/queries.csv\"\n",
        "pre_processed_docs_file = \"/content/Query_Doc/pre_processed_docs.csv\"\n",
        "pre_processed_queries_file = \"/content/Query_Doc/pre_processed_queries.csv\"\n",
        "spell_checked_docs_file = \"/content/Query_Doc/spell_checked_docs.csv\"\n",
        "spell_checked_queries_file = \"/content/Query_Doc/spell_checked_queries.csv\"\n",
        "qdrels_file = \"/content/Query_Doc/qdrel.csv\""
      ],
      "metadata": {
        "id": "JLx7wJi-uCAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre processing"
      ],
      "metadata": {
        "id": "K9r6lSZ36o1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_process(docs_file, pre_processed_docs_file, \"doc_text\")\n",
        "pre_process(queries_file, pre_processed_queries_file, \"query_text\")"
      ],
      "metadata": {
        "id": "Skt7Avvj6sr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spell correction on the pre processed docs and queries"
      ],
      "metadata": {
        "id": "U9UG2gEAzrTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print logs(original and corrected for query doc only)\n",
        "correct_spellings(pre_processed_queries_file, spell_checked_queries_file, \"query_text\", True)\n",
        "correct_spellings(pre_processed_docs_file, spell_checked_docs_file, \"doc_text\")"
      ],
      "metadata": {
        "id": "VVCp3evivOyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the documents and get the tokens forming the vocabulary"
      ],
      "metadata": {
        "id": "07oTyhn7wLoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove contextualSpellCheck, lemmatizer, ner and tagger from the nlp pipe since we are checking performance after only spell check\n",
        "nlp.remove_pipe('contextual spellchecker')\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('lemmatizer')\n",
        "nlp.remove_pipe('ner')"
      ],
      "metadata": {
        "id": "MvyVyY85wOiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = get_vocabulary(nlp, spell_checked_docs_file, \"doc_text\")"
      ],
      "metadata": {
        "id": "ewjngCJy3Aha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabulary)\n",
        "print(len(vocabulary))"
      ],
      "metadata": {
        "id": "KNv8dpHa4wK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ids, docs = get_texts(spell_checked_docs_file, 'doc_id', 'doc_text')\n",
        "query_ids, queries = get_texts(spell_checked_queries_file, 'query_id', 'query_text')\n",
        "\n",
        "query_id_idx_map = {query_ids[i]: i for i in range(len(query_ids))}\n",
        "doc_id_idx_map = {doc_ids[i]: i for i in range(len(doc_ids))}"
      ],
      "metadata": {
        "id": "22C0oTTPpFwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs = get_relevant_docs(qdrels_file)"
      ],
      "metadata": {
        "id": "sHni6hNqwuUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_id_idx_map)"
      ],
      "metadata": {
        "id": "2W8_C62OyAgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize documents and queries, and find cosine similarities for each query with the documents"
      ],
      "metadata": {
        "id": "3F6cfubesRxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
        "\n",
        "# Create tf idf vectors for the documents and queries\n",
        "doc_vectors = vectorizer.fit_transform(docs)\n",
        "query_vectors = vectorizer.transform(queries)"
      ],
      "metadata": {
        "id": "CMXPyt65sbnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = cosine_similarity(query_vectors, doc_vectors)"
      ],
      "metadata": {
        "id": "zZVlHYxRsfBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find top 5 and top 10 similar document indices for each query\n",
        "top_5_indices = np.argsort(similarities, axis=1)[:, -5:]\n",
        "top_10_indices = np.argsort(similarities, axis=1)[:, -10:]\n",
        "\n",
        "# for each query, store the top 5 and top 10 similar document ids\n",
        "top_5_ids = [[doc_ids[i] for i in indices] for indices in top_5_indices]\n",
        "top_10_ids = [[doc_ids[i] for i in indices] for indices in top_10_indices]\n",
        "top_ids = [[i[-1]] for i in top_5_ids]"
      ],
      "metadata": {
        "id": "hqaIYiMxsn0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prec_1 = precision_k_score(query_id_idx_map, relevant_docs, top_ids, 1)\n",
        "prec_5 = precision_k_score(query_id_idx_map, relevant_docs, top_5_ids, 5)\n",
        "prec_10 = precision_k_score(query_id_idx_map, relevant_docs, top_10_ids, 10)\n",
        "\n",
        "print(f\"Average precision at 1: {round(prec_1 * 100, 2)} %\")\n",
        "print(f\"Average precision at 5: {round(prec_5 * 100, 2)} %\")\n",
        "print(f\"Average precision at 10: {round(prec_10 * 100, 2)} %\")"
      ],
      "metadata": {
        "id": "M-AMHjh4y_Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "V-fkgWra68EA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "Using nltk's porter stemmer here since spacy does not provide a stemmer"
      ],
      "metadata": {
        "id": "EuREmYXY7kHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "mvlGOo2869b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_vocabulary, stemmed_docs = get_stemmed_vocabulary(spell_checked_docs_file, \"doc_text\")\n",
        "\n",
        "print(stemmed_vocabulary)\n",
        "print(len(stemmed_vocabulary))\n",
        "\n",
        "print('\\n'.join(stemmed_docs[0:10]))"
      ],
      "metadata": {
        "id": "t5lTj9GoF1L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_queries = stem_queries(spell_checked_queries_file, ps)\n",
        "\n",
        "print('\\n'.join(stemmed_queries[0:10]))"
      ],
      "metadata": {
        "id": "mbZKvADwO-sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the documents and queries, and find cosine similarities for each query with documents"
      ],
      "metadata": {
        "id": "r650FjYsPrqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_stemmed = TfidfVectorizer(vocabulary=stemmed_vocabulary)\n",
        "\n",
        "# Create tf idf vectors for the documents and queries\n",
        "stemmed_doc_vectors = vectorizer_stemmed.fit_transform(stemmed_docs)\n",
        "stemmed_query_vectors = vectorizer_stemmed.transform(stemmed_queries)"
      ],
      "metadata": {
        "id": "1GQ4bILwPqF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_similarities = cosine_similarity(stemmed_query_vectors, stemmed_doc_vectors)\n",
        "\n",
        "# find top 5 and top 10 similar document indices for each query\n",
        "stemmed_top_5_indices = np.argsort(stemmed_similarities, axis=1)[:, -5:]\n",
        "stemmed_top_10_indices = np.argsort(stemmed_similarities, axis=1)[:, -10:]\n",
        "\n",
        "# for each query, store the top 5 and top 10 similar document ids\n",
        "stemmed_top_5_ids = [[doc_ids[i] for i in indices] for indices in stemmed_top_5_indices]\n",
        "stemmed_top_10_ids = [[doc_ids[i] for i in indices] for indices in stemmed_top_10_indices]\n",
        "stemmed_top_ids = [[ids[-1]] for ids in stemmed_top_5_ids]"
      ],
      "metadata": {
        "id": "TLbal6o7RCkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision Scores"
      ],
      "metadata": {
        "id": "j20Lh7gXRF-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec_1_stemmed = precision_k_score(query_id_idx_map, relevant_docs, stemmed_top_ids, 1)\n",
        "prec_5_stemmed = precision_k_score(query_id_idx_map, relevant_docs, stemmed_top_5_ids, 5)\n",
        "prec_10_stemmed = precision_k_score(query_id_idx_map, relevant_docs, stemmed_top_10_ids, 10)\n",
        "\n",
        "print(f\"Average precision at 1 for stemmed: {round(prec_1_stemmed * 100, 2)} %\")\n",
        "print(f\"Average precision at 5 for stemmed: {round(prec_5_stemmed * 100, 2)} %\")\n",
        "print(f\"Average precision at 10 for stemmed: {round(prec_10_stemmed * 100, 2)} %\")"
      ],
      "metadata": {
        "id": "_MRJ7DFAREdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "Add lemmatizer back to the nlp pipe"
      ],
      "metadata": {
        "id": "J-R7czJ5RPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = nlp.add_pipe('lemmatizer')\n",
        "lemmatizer.initialize()"
      ],
      "metadata": {
        "id": "zdIsBYvlRT9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_vocabulary = get_vocabulary(lemmatizer, spell_checked_docs_file, \"doc_text\")\n",
        "lemmatized_docs = get_processed_text(lemmatizer, spell_checked_docs_file, \"doc_text\")\n",
        "lemmatized_queries = get_processed_text(lemmatizer, spell_checked_queries_file, \"query_text\")\n",
        "\n",
        "print(lemmatized_docs[0:10])\n",
        "print(lemmatized_queries[0:10])\n"
      ],
      "metadata": {
        "id": "wJNY6i8hfrvO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}