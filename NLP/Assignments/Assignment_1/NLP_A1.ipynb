{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJCz/sFFZ+ea7XE5bmLJLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nespar7/Spring_24/blob/main/NLP/Assignments/Assignment_1/NLP_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "0r-rTccDuFr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "64yWLxtXoOrG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "270212c5-2fc5-42f1-c7b2-339d04fa9bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy\n",
            "  Using cached spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.9/156.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
            "  Using cached thinc-8.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Using cached srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer<0.10.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.13.0 (from spacy)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2 (from spacy)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools (from spacy)\n",
            "  Downloading setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0 (from spacy)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.16.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.6.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.9/120.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
            "  Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
            "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
            "Collecting click<9.0.0,>=7.1.1 (from typer<0.10.0,>=0.3.0->spacy)\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: cymem, wasabi, urllib3, typing-extensions, tqdm, spacy-loggers, spacy-legacy, smart-open, setuptools, packaging, numpy, murmurhash, MarkupSafe, langcodes, idna, click, charset-normalizer, certifi, catalogue, annotated-types, typer, srsly, requests, pydantic-core, preshed, jinja2, cloudpathlib, blis, pydantic, confection, weasel, thinc, spacy\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.8\n",
            "    Uninstalling cymem-2.0.8:\n",
            "      Successfully uninstalled cymem-2.0.8\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 0.10.1\n",
            "    Uninstalling wasabi-0.10.1:\n",
            "      Successfully uninstalled wasabi-0.10.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: spacy-loggers\n",
            "    Found existing installation: spacy-loggers 1.0.5\n",
            "    Uninstalling spacy-loggers-1.0.5:\n",
            "      Successfully uninstalled spacy-loggers-1.0.5\n",
            "  Attempting uninstall: spacy-legacy\n",
            "    Found existing installation: spacy-legacy 3.0.12\n",
            "    Uninstalling spacy-legacy-3.0.12:\n",
            "      Successfully uninstalled spacy-legacy-3.0.12\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.4.0\n",
            "    Uninstalling smart-open-6.4.0:\n",
            "      Successfully uninstalled smart-open-6.4.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.10\n",
            "    Uninstalling murmurhash-1.0.10:\n",
            "      Successfully uninstalled murmurhash-1.0.10\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.4\n",
            "    Uninstalling MarkupSafe-2.1.4:\n",
            "      Successfully uninstalled MarkupSafe-2.1.4\n",
            "  Attempting uninstall: langcodes\n",
            "    Found existing installation: langcodes 3.3.0\n",
            "    Uninstalling langcodes-3.3.0:\n",
            "      Successfully uninstalled langcodes-3.3.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.11.17\n",
            "    Uninstalling certifi-2023.11.17:\n",
            "      Successfully uninstalled certifi-2023.11.17\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.10\n",
            "    Uninstalling catalogue-2.0.10:\n",
            "      Successfully uninstalled catalogue-2.0.10\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.8\n",
            "    Uninstalling srsly-2.4.8:\n",
            "      Successfully uninstalled srsly-2.4.8\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.9\n",
            "    Uninstalling preshed-3.0.9:\n",
            "      Successfully uninstalled preshed-3.0.9\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.3\n",
            "    Uninstalling Jinja2-3.1.3:\n",
            "      Successfully uninstalled Jinja2-3.1.3\n",
            "  Attempting uninstall: cloudpathlib\n",
            "    Found existing installation: cloudpathlib 0.16.0\n",
            "    Uninstalling cloudpathlib-0.16.0:\n",
            "      Successfully uninstalled cloudpathlib-0.16.0\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.14\n",
            "    Uninstalling pydantic-1.10.14:\n",
            "      Successfully uninstalled pydantic-1.10.14\n",
            "  Attempting uninstall: confection\n",
            "    Found existing installation: confection 0.1.4\n",
            "    Uninstalling confection-0.1.4:\n",
            "      Successfully uninstalled confection-0.1.4\n",
            "  Attempting uninstall: weasel\n",
            "    Found existing installation: weasel 0.3.4\n",
            "    Uninstalling weasel-0.3.4:\n",
            "      Successfully uninstalled weasel-0.3.4\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.2\n",
            "    Uninstalling thinc-8.2.2:\n",
            "      Successfully uninstalled thinc-8.2.2\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.2\n",
            "    Uninstalling spacy-3.7.2:\n",
            "      Successfully uninstalled spacy-3.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "en-core-web-sm 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.7.2 which is incompatible.\n",
            "spacy-lookup 0.1.0 requires spacy<3.0.0,>=2.0.16, but you have spacy 3.7.2 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 idna-3.6 jinja2-3.1.3 langcodes-3.3.0 murmurhash-1.0.10 numpy-1.26.3 packaging-23.2 preshed-3.0.9 pydantic-2.6.0 pydantic-core-2.16.1 requests-2.31.0 setuptools-69.0.3 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.2 tqdm-4.66.1 typer-0.9.0 typing-extensions-4.9.0 urllib3-2.2.0 wasabi-1.1.2 weasel-0.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "blis",
                  "catalogue",
                  "certifi",
                  "charset_normalizer",
                  "click",
                  "confection",
                  "cymem",
                  "jinja2",
                  "markupsafe",
                  "murmurhash",
                  "pkg_resources",
                  "preshed",
                  "requests",
                  "setuptools",
                  "spacy",
                  "srsly",
                  "thinc",
                  "tqdm",
                  "wasabi",
                  "weasel"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.3.1\n",
            "    Uninstalling en-core-web-sm-2.3.1:\n",
            "      Successfully uninstalled en-core-web-sm-2.3.1\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: contextualSpellCheck in /usr/local/lib/python3.10/dist-packages (0.4.4)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from contextualSpellCheck) (2.1.0+cu121)\n",
            "Requirement already satisfied: editdistance==0.6.2 in /usr/local/lib/python3.10/dist-packages (from contextualSpellCheck) (0.6.2)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from contextualSpellCheck) (4.35.2)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from contextualSpellCheck) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (8.2.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (1.26.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->contextualSpellCheck) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->contextualSpellCheck) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->contextualSpellCheck) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->contextualSpellCheck) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->contextualSpellCheck) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->contextualSpellCheck) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (0.4.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->contextualSpellCheck) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->contextualSpellCheck) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->contextualSpellCheck) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->contextualSpellCheck) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->contextualSpellCheck) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->contextualSpellCheck) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy>=3.0.0->contextualSpellCheck) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy>=3.0.0->contextualSpellCheck) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.0.0->contextualSpellCheck) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.0.0->contextualSpellCheck) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.0.0->contextualSpellCheck) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->contextualSpellCheck) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy --force-reinstall\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install contextualSpellCheck\n",
        "!pip install scikit-learn\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "YO4VRNdHoxSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import contextualSpellCheck\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from nltk import download as nltk_dw\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk_dw('punkt')"
      ],
      "metadata": {
        "id": "Nt8Cj2NMpmTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01fa81c-d76f-431f-88c5-eeff453160f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "opEEAC1UqVO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "contextualSpellCheck.add_to_pipe(nlp)"
      ],
      "metadata": {
        "id": "h34NMkubqXbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247f16ec-818c-438b-d82e-175ea5188eb6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Definitions"
      ],
      "metadata": {
        "id": "Dk6ujq08qMEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_check(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    return doc._.performed_spellCheck, doc._.outcome_spellCheck"
      ],
      "metadata": {
        "id": "K9L5_aGpprWg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spellings(csv_file, out_csv_file, column_name, should_log=False):\n",
        "    # read a csv file, in each line, see if spellCheck was performed(if text was correctly spelled) and print original and corrected texts\n",
        "    lines = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        # Index of the column to be spell checked\n",
        "        column_index = lines[0].index(column_name)\n",
        "\n",
        "        # For each line, perform spell check\n",
        "        for i in range(1, len(lines)):\n",
        "            line = lines[i][column_index]\n",
        "            performed, outcome = spell_check(line)\n",
        "\n",
        "\n",
        "            # If spell check was performed, update the line and print the original and corrected texts\n",
        "            if performed:\n",
        "                # Update the line\n",
        "                lines[i][column_index] = outcome\n",
        "\n",
        "                if should_log:\n",
        "                    print(line)\n",
        "                    print(outcome)\n",
        "                    print()\n",
        "\n",
        "            if i%100 == 0:\n",
        "                print(i)\n",
        "\n",
        "    with open(out_csv_file, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(lines)\n",
        "\n",
        "    print(\"Spell check completed\")"
      ],
      "metadata": {
        "id": "hJlxZytPw3oK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process(csv_file, out_csv_file, column_name):\n",
        "    # read a csv file, in each line, remove characters apart from alphanumerics and whitespaces in the given column\n",
        "    lines = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        # Index of the column to be pre processed\n",
        "        column_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            lines[i][column_index] = ''.join([c for c in lines[i][column_index] if c.isalnum() or c.isspace()])\n",
        "\n",
        "    # write the pre-processed data to the resulting csv file\n",
        "    with open(out_csv_file, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(lines)\n",
        "\n",
        "    print(\"Pre procesing completed\")"
      ],
      "metadata": {
        "id": "iXrFFBtPsQIV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary(model, csv_file, column_name):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    docs = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        doc_col_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            docs.append(lines[i][doc_col_index])\n",
        "\n",
        "        num_docs = len(docs)\n",
        "        lower_lim = 5\n",
        "        upper_lim = int(num_docs * 0.85)\n",
        "\n",
        "        # tokenize the documents using spacy and store the tokens in a vocabulary,\n",
        "        # remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "\n",
        "        doc_tokenized = []\n",
        "\n",
        "        # use the model to tokenize the documents\n",
        "        for doc in docs:\n",
        "            # process doc using the model passed\n",
        "            doc = model(doc)\n",
        "\n",
        "            # Select alphabetical tokens and remove stop words\n",
        "            tokens = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "            doc_tokenized.append(tokens)\n",
        "\n",
        "        # Now that we have the tokenized documents, we can create a vocabulary\n",
        "        vocabulary = {}\n",
        "\n",
        "        for doc in doc_tokenized:\n",
        "            for token in doc:\n",
        "                if token in vocabulary:\n",
        "                    vocabulary[token] += 1\n",
        "                else:\n",
        "                    vocabulary[token] = 1\n",
        "\n",
        "        # Remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "        vocabulary = [k for k, v in vocabulary.items() if v >= lower_lim and v <= upper_lim]\n",
        "\n",
        "        return vocabulary"
      ],
      "metadata": {
        "id": "AMSjXyLexJ9l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_processed_text(model, csv_file, col_name):\n",
        "    # Read the csv file and return the processed text using the passed model\n",
        "    lines = []\n",
        "    docs = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        col_index = lines[0].index(col_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            doc = model(lines[i][col_index])\n",
        "            docs.append(' '.join([token.text.lower() for token in doc]))\n",
        "\n",
        "    return docs"
      ],
      "metadata": {
        "id": "nNLDLx_7f5tT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stemmed_vocabulary(csv_file, col_name):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    docs = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        doc_col_index = lines[0].index(col_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            docs.append(lines[i][doc_col_index])\n",
        "\n",
        "    # tokenize the documents using nltk and store the tokens in a vocabulary,\n",
        "    # remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "\n",
        "    lower_lim = 5\n",
        "    upper_lim = int(len(docs) * 0.85)\n",
        "\n",
        "    doc_tokenized = []\n",
        "\n",
        "    # use nltk to tokenize the documents\n",
        "    for doc in docs:\n",
        "        doc = word_tokenize(doc)\n",
        "\n",
        "        tokens = [token.lower() for token in doc if token.isalpha()]\n",
        "\n",
        "        # stem the tokens\n",
        "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
        "\n",
        "        doc_tokenized.append(stemmed_tokens)\n",
        "\n",
        "    vocabulary = {}\n",
        "\n",
        "    for doc in doc_tokenized:\n",
        "        for token in doc:\n",
        "            if token in vocabulary:\n",
        "                vocabulary[token] += 1\n",
        "            else:\n",
        "                vocabulary[token] = 1\n",
        "\n",
        "    # Remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "    stemmed_vocabulary = [k for k, v in vocabulary.items() if v >= lower_lim and v <= upper_lim]\n",
        "\n",
        "    return stemmed_vocabulary"
      ],
      "metadata": {
        "id": "pqW6THucFcwZ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stemmed_texts(csv_file, column_name, porter_stemmer):\n",
        "    # Read the spell checked docs csv file and store the queries in a list\n",
        "    texts = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        col_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            texts.append(lines[i][col_index])\n",
        "\n",
        "    stemmed_texts = []\n",
        "\n",
        "    # use nltk to tokenize the documents\n",
        "    for text in texts:\n",
        "        text = word_tokenize(text)\n",
        "\n",
        "        tokens = [token.lower() for token in text]\n",
        "\n",
        "        # stem the tokens\n",
        "        stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        stemmed_texts.append(' '.join(stemmed_tokens))\n",
        "\n",
        "    return stemmed_texts"
      ],
      "metadata": {
        "id": "IK51o0M6OvSc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_texts(csv_file, id_col, text_col):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    ids = []\n",
        "    texts = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        id_col_index = lines[0].index(id_col)\n",
        "        text_col_index = lines[0].index(text_col)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            ids.append(lines[i][id_col_index])\n",
        "            texts.append(lines[i][text_col_index])\n",
        "\n",
        "    return ids, texts"
      ],
      "metadata": {
        "id": "J9UTSpYTrXO2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_docs(qdrels):\n",
        "    # Get relevant docs list for each query from qdrels.csv file\n",
        "    relevant_docs = {}\n",
        "\n",
        "    with open(qdrels, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            line = lines[i]\n",
        "\n",
        "            query_id = line[1]\n",
        "            doc_id = line[2]\n",
        "\n",
        "            if query_id in relevant_docs:\n",
        "                relevant_docs[query_id].append(doc_id)\n",
        "            else:\n",
        "                relevant_docs[query_id] = [doc_id]\n",
        "\n",
        "    return relevant_docs"
      ],
      "metadata": {
        "id": "ciOjy6BSv6S1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_k_score(query_id_idx_map, relevant_docs, top_docs, k):\n",
        "    # For each query in relevant_docs, calculate true positives in top_docs and return average precision_k score\n",
        "    precision_k = 0\n",
        "\n",
        "    for query_id, relevant_docs_list in relevant_docs.items():\n",
        "        true_positives = 0\n",
        "\n",
        "        query_idx = query_id_idx_map[query_id]\n",
        "\n",
        "        for doc in top_docs[query_idx]:\n",
        "            if doc in relevant_docs_list:\n",
        "                true_positives += 1\n",
        "\n",
        "        precision_k += true_positives / k\n",
        "\n",
        "    return precision_k / len(relevant_docs)"
      ],
      "metadata": {
        "id": "bb6tV5nK1R8h"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "jDvR6WfCtVT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CSV file locations need to be changed as applicable if being run locally."
      ],
      "metadata": {
        "id": "K9vjusFbu--4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_file = \"/content/Query_Doc/docs.csv\"\n",
        "queries_file = \"/content/Query_Doc/queries.csv\"\n",
        "pre_processed_docs_file = \"/content/Query_Doc/pre_processed_docs.csv\"\n",
        "pre_processed_queries_file = \"/content/Query_Doc/pre_processed_queries.csv\"\n",
        "spell_checked_docs_file = \"/content/Query_Doc/spell_checked_docs.csv\"\n",
        "spell_checked_queries_file = \"/content/Query_Doc/spell_checked_queries.csv\"\n",
        "qdrels_file = \"/content/Query_Doc/qdrel.csv\""
      ],
      "metadata": {
        "id": "JLx7wJi-uCAn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre processing"
      ],
      "metadata": {
        "id": "K9r6lSZ36o1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_process(docs_file, pre_processed_docs_file, \"doc_text\")\n",
        "pre_process(queries_file, pre_processed_queries_file, \"query_text\")"
      ],
      "metadata": {
        "id": "Skt7Avvj6sr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spell correction on the pre processed docs and queries"
      ],
      "metadata": {
        "id": "U9UG2gEAzrTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print logs(original and corrected for query doc only)\n",
        "correct_spellings(pre_processed_queries_file, spell_checked_queries_file, \"query_text\", True)\n",
        "correct_spellings(pre_processed_docs_file, spell_checked_docs_file, \"doc_text\")"
      ],
      "metadata": {
        "id": "VVCp3evivOyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the documents and get the tokens forming the vocabulary"
      ],
      "metadata": {
        "id": "07oTyhn7wLoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove contextualSpellCheck, lemmatizer, ner and tagger from the nlp pipe since we are checking performance after only spell check\n",
        "nlp.remove_pipe('contextual spellchecker')\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('lemmatizer')\n",
        "nlp.remove_pipe('ner')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvyVyY85wOiG",
        "outputId": "0bc69f8c-8807-4ad6-d942-2022e0b80941"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7aa82ab77f80>)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = get_vocabulary(nlp, spell_checked_docs_file, \"doc_text\")"
      ],
      "metadata": {
        "id": "ewjngCJy3Aha"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabulary)\n",
        "print(len(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNv8dpHa4wK9",
        "outputId": "0cf4ca68-139e-4bfd-ed25-bf3f78722239"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['step', 'invest', 'share', 'market', 'india', 'story', 'happen', 'indian', 'government', 'increase', 'speed', 'internet', 'connection', 'increased', 'mentally', 'solve', 'find', 'water', 'sugar', 'salt', 'carbon', 'survive', 'moon', 'sun', 'm', 'buy', 'active', 'far', 'phone', 'video', 'games', 'good', 'great', 'use', 'instead', 'company', 'code', 'files', 'free', 'method', 'atoms', 'things', 'tell', 'components', 'read', 'youtube', 'comments', 'physics', 'easy', 'learn', 'sexual', 'experience', 'like', 'laws', 'change', 'status', 'student', 'visa', 'green', 'card', 'compare', 'immigration', 'canada', 'japan', 'trump', 'presidency', 'mean', 'current', 'international', 'masters', 'students', 'eu', 'affect', 'planning', 'study', 'means', 'girls', 'want', 'friends', 'guy', 'guys', 'feel', 'girl', 'quora', 'users', 'questions', 'answered', 'google', 'people', 'ask', 'easily', 'best', 'digital', 'marketing', 'bangladesh', 'institute', 'look', 'white', 'bombs', 's', 'causing', 'jealous', 'avoid', 'hp', 'chart', 'time', 'numbers', 'times', 'day', 'hands', 'tips', 'making', 'job', 'interview', 'process', 'medicine', 'web', 'application', 'framework', 'society', 'place', 'importance', 'sports', 'way', 'money', 'online', 'prepare', 'final', 'law', 'know', 'completely', 'exam', 'thing', 'better', 'despite', 'knowing', 'special', 'nose', 'gets', 'cut', 'night', 'getting', 'stuck', 'game', 'likely', 'united', 'states', 'accept', 'etc', 'citizens', 'political', 'views', 'average', 'gas', 'molecules', 'determined', 'travel', 'website', 'spain', 'think', 'obama', 'try', 'guns', 'away', 'gun', 'control', 'improve', 'skills', 'years', 'year', 'old', 'billionaire', 'girlfriend', 'asks', 'boyfriend', 'choose', 'makes', 'reply', 'said', 'end', 'feelings', 'wants', 'upsc', 'civil', 'service', 'eagle', 'stop', 'expect', 'mail', 'month', 'trading', 'kid', 'worth', 'long', 'run', 'bored', 'universities', 'recruit', 'new', 'kids', 'majors', 'looking', 'graduates', 'followers', 'number', 'darth', 'star', 'wars', 'legends', 'character', 'limit', 'profile', 'breaking', 'couple', 'happens', 'emotionally', 'male', 'female', 'affected', 'boy', 'examples', 'products', 'oil', 'career', 'grade', 'b', 'preparation', 'program', 'rbi', 'play', 'regular', 'player', 'sad', 've', 'dish', 'tax', 'officers', 'difficult', 'apply', 'programs', 'senior', 'friend', 'true', 'attracted', 'songs', 'dance', 'suddenly', 'blocked', 'gmail', 'remember', 'password', 'recovery', 'email', 'longer', 'alive', 'recover', 'ways', 'download', 'content', 'single', 'store', 'registration', 'normal', 'dark', 'ring', 'inside', 'eye', 'causes', 'treated', 'harry', 'potter', 'book', 'child', 'bad', 'family', 'office', 'database', 'java', 'programming', 'language', 'computer', 'important', 'energy', 'produced', 'lightning', 'possible', 'review', 'performance', 'testing', 'cost', 'germany', 'come', 'lost', 'gain', 'enjoy', 'types', 'different', 'body', 'personality', 'disorder', 'autism', 'speak', 'english', 'helpful', 'quickbooks', 'auto', 'data', 'support', 'usa', 'richest', 'reach', 'level', 'fire', 'bullet', 'going', 'faster', 'sound', 'shot', 'death', 'occurs', 'prevent', 'cancer', 'log', 'account', 'telling', 'ip', 'address', 'device', 'selling', 'purpose', 'life', 'actually', 'muslims', 'model', 'illegal', 'right', 'happy', 'birthday', 'person', 'wish', 'open', 'commercial', 'fm', 'radio', 'station', 'city', 'procedure', 'zealand', 'technical', 'employees', 'sales', 'high', 'salary', 'income', 'jobs', 'field', 'psychology', 'paying', 'height', 'major', 'effects', 'cambodia', 'earthquake', 'difference', 'trust', 'gaming', 'laptop', 'inr', 'proving', 'reference', 'class', 'chemistry', 'cbse', 'board', 'national', 'technology', 'social', 'lessons', 'father', 'leave', 'romantic', 'movies', 'movie', 'seen', 'real', 'painting', 'printing', 'work', 'product', 'manager', 'early', 'join', 'management', 'general', 'based', 'software', 'android', 'war', 'pakistan', 'air', 'attack', 'nuclear', 'line', 'speech', 'react', 'union', 'possibly', 'american', 'novel', 'novels', 'forgot', 'recent', 'results', 'higher', 'rest', 'short', 'heard', 'whatsapp', 'love', 'competitive', 'hiring', 'bank', 't', 'search', 'traffic', 'organic', 'vs', 'page', 'second', 'watch', 'powerful', 'country', 'world', 'pain', 'low', 'heat', 'ice', 'china', 'food', 'chinese', 'taking', 'advantage', 'cry', 'stick', 'pictures', 'teen', 'women', 'allow', 'age', 'differ', 'similar', 'basic', 'tools', 'exist', 'donot', 'intelligent', 'tooth', 'dry', 'applying', 'question', 'marked', 'needing', 'need', 'state', 'buffer', 'win', 'highest', 'electrical', 'charge', 'hold', 'greatest', 'mystery', 'universe', 'alternative', 'machine', 'learning', 'create', 'set', 'block', 'un', 'future', 'budget', 'meaning', 'vitamin', 'c', 'cause', 'disease', 'glass', 'pay', 'scale', 'letter', 'access', 'ancient', 'party', 'successful', 'course', 'register', 'domain', 'site', 'older', 'men', 'young', 'strongest', 'structure', 'matter', 'humanity', 'evil', 'transport', 'cells', 'hair', 'head', 'loose', 'mass', 'north', 'korea', 'websites', 'diesel', 'petrol', 'diet', 'growing', 'black', 'hole', 'correct', 'says', 'talking', 'quote', 'currently', 'offer', 'stock', 'options', 'uber', 'children', 'hire', 'hours', 'hour', 'presentation', 'days', 'late', 'dog', 'bite', 'britain', 'standard', 'given', 'afraid', 'working', 'red', 'season', 'lose', 'tall', 'forget', 'wife', 'killed', 'release', 'issue', 'switch', 'canon', 'price', 'members', 'answer', 'answering', 'joke', 'biggest', 'got', 'london', 'pm', 'advice', 'saving', 'leaving', 'sell', 'having', 'kingdom', 'business', 'source', 'transfer', 'paypal', 'send', 'song', 'deep', 'rs', 'forward', 'engineering', 'suit', 'ex', 'past', 'hurt', 'attract', 'gross', 'payment', 'oldest', 'kerala', 'cities', 'confidence', 'relation', 'gods', 'draw', 'hindu', 'mythology', 'apple', 'music', 'spotify', 'fixed', 'fund', 'live', 'robert', 'al', 'actor', 'prefer', 'small', 'families', 'animals', 'humans', 'deleted', 'view', 'private', 'eyes', 'follow', 'able', 'companies', 'funding', 'check', 'history', 'phones', 'wifi', 'mobile', 'network', 'significance', 'battle', 'contrast', 'creative', 'college', 'admissions', 'essay', 'admission', 'officer', 'public', 'reading', 'cover', 'pursue', 'author', 'christian', 'start', 'hyderabad', 'lord', 'combination', 'courses', 'taken', 'seat', 'mac', 'facebook', 'pro', 'director', 'tv', 'shows', 'die', 'fluent', 'impact', 'paper', 'notes', 'estate', 'fastest', 'lectures', 'colleges', 'weeks', 'behavior', 'daily', 'development', 'hate', 'hillary', 'clinton', 'reasons', 'periods', 'expected', 'woman', 'care', 'cooking', 'especially', 'culture', 'lack', 'term', 'god', 'series', 'color', 'documents', 'shops', 'creativity', 'continue', 'presidential', 'campaign', 'democratic', 'candidate', 'serve', 'channels', 'doesnot', 'install', 'growth', 'engineers', 'advantages', 'benefits', 'objects', 'position', 'respect', 'drawing', 'fun', 'gpa', 'harvard', 'school', 'profitable', 'trade', 'binary', 'depression', 'ones', 'package', 'security', 'started', 'information', 'earn', 'sentences', 'word', 'home', 'wireless', 'drive', 'road', 'car', 'shell', 'linux', 'religion', 'perception', 'cognitive', 'tatkal', 'waiting', 'list', 'reservation', 'train', 'league', 'drinking', 'drink', 'lakes', 'wildlife', 'stars', 'adult', 'supporters', 'walking', 'promises', 'president', 'sold', 'worried', 'opinion', 'months', 'little', 'jump', 'minutes', 'test', 'gate', 'cs', 'stream', 'science', 'material', 'understanding', 'analysis', 'math', 'complete', 'begin', 'understand', 'algorithms', 'books', 'suitable', 'mix', 'balls', 'weight', 'ball', 'formed', 'man', 'signs', 'smart', 'playing', 'intelligence', 'created', 'infinite', 'expanding', 'potential', 'write', 'publish', 'flash', 'fast', 'large', 'period', 'bill', 'lives', 'common', 'exactly', 'mental', 'illness', 'manage', 'giving', 'blood', 'qualities', 'sea', 'bay', 'narendra', 'modi', 'prime', 'minister', 'eat', 'egg', 'tickets', 'ticket', 'core', 'con', 'resolutions', 'resolution', 'dream', 'dreams', 'marks', 'score', 'rank', 'minimum', 'room', 'persons', 'equal', 'broken', 'doctors', 'surgery', 'operation', 'knowledge', 'developing', 'apps', 'scratch', 'app', 'rich', 'rights', 'visiting', 'idea', 'texts', 'till', 'films', 'feature', 'strong', 'characters', 'wait', 'health', 'clothes', 'cotton', 'meat', 'taste', 'branded', 'happened', 'brand', 'mark', 'dead', 'river', 'havenot', 'miss', 'loved', 'travelling', 'quality', 'chocolate', 'cars', 'gold', 'coast', 'plan', 'entire', 'deal', 'fear', 'dying', 'reduce', 'screen', 'iphone', 'interesting', 'innovations', 'coming', 'funds', 'raise', 'effect', 'banning', 'ban', 'eccentric', 'episode', 'suck', 'news', 'posted', 'media', 'post', 'ads', 'beer', 'construction', 'acquire', 'positive', 'hollywood', 'kolkata', 'creating', 'fill', 'solution', 'self', 'break', 'jio', 'rules', 'gap', 'explain', 'sweet', 'birth', 'saying', 'nice', 'countries', 'finance', 'design', 'system', 'request', 'meet', 'goes', 'banks', 'note', 'implications', 'currency', 'economy', 'dogs', 'considered', 'cheated', 'relationship', 'turn', 'user', 'd', 'text', 'concentration', 'sent', 'chance', 'german', 'pet', 'habit', 'rock', 'ok', 'husband', 'stay', 'cheating', 'partner', 'consent', 'require', 'cse', 'iit', 'firm', 'reason', 'emotional', 'facts', 'hard', 'fake', 'location', 'running', 'quit', 'smoking', 'actress', 'relationships', 'popular', 'date', 'table', 'crack', 'calls', 'j', 'shopping', 'classes', 'tamil', 'message', 'contacts', 'written', 'uk', 'interested', 'sector', 'required', 'medical', 'doctor', 'industries', 'racist', 'differences', 'roots', 'function', 'zero', 'devices', 'add', 'speaker', 'effective', 'election', 'donald', 'snow', 'jon', 'theories', 'house', 'called', 'nadu', 'feed', 'anxiety', 'sign', 'stomach', 'university', 'degree', 'mechanical', 'maintain', 'peace', 'mind', 'build', 'reliable', 'treat', 'cure', 'available', 'useful', 'motivated', 'assessment', 'firms', 'industry', 'discharge', 'pregnancy', 'scientists', 'candidates', 'style', 'wedding', 'western', 'condition', 'bed', 'offshore', 'khan', 'help', 'title', 'gift', 'mom', 'hit', 'freedom', 'middle', 'found', 'changing', 'pass', 'drug', 'projects', 'kind', 'cricket', 'players', 'tea', 'entry', 'mit', 'requirements', 'crush', 'dating', 'likes', 'according', 'robots', 'existed', 'type', 'kg', 'summer', 'nights', 'million', 'regret', 'divorce', 'dna', 'cool', 'order', 'secret', 'training', 'comedy', 'fighting', 'graduation', 'watching', 'introduced', 'services', 'il', 'compared', 'relative', 'cell', 'sodium', 'spouse', 'rid', 'feeling', 'non', 'ac', 'island', 'described', 'tourist', 'attractions', 'healthy', 'meal', 'sore', 'ugc', 'net', 'literature', 'latest', 'protein', 'exchange', 'breakfast', 'coffee', 'designers', 'designer', 'israel', 'figure', 'fall', 'lee', 'favourite', 'film', 'governments', 'decision', 'euro', 'saltwater', 'candy', 'imported', 'enter', 'learned', 'iran', 'went', 'piano', 'natural', 'plus', 'g', 'id', 'adopted', 'certificate', 'replace', 'light', 'factors', 'main', 'platform', 'power', 'big', 'point', 'anymore', 'amazon', 'daniel', 'ek', 'phrase', 'weed', 'hand', 'police', 'considering', 'print', 'chicken', 'pizza', 'recipes', 'places', 'institutes', 'tennis', 'damage', 'connect', 'solutions', 'particular', 'atm', 'wrong', 'gravity', 'theory', 'interstellar', 'pilot', 'force', 'wake', 'sleep', 'successfully', 'climate', 'global', 'believe', 'exists', 'congress', 'starting', 'purchase', 'safe', 'pounds', 'pokémon', 'blue', 'improvement', 'needs', 'owns', 'nature', 'file', 'value', 'gandhi', 'chances', 'gym', 'morning', 'swimming', 'milk', 'tricks', 'education', 'image', 'highways', 'california', 'texas', 'space', 'physical', 'chemical', 'grow', 'programmer', 'iii', 'risk', 'foreign', 'crop', 'circles', 'generated', 'virus', 'certain', 'prospects', 'display', 'sites', 'sex', 'younger', 'macbook', 'sleeping', 'brain', 'techniques', 'server', 'recently', 'exercise', 'happening', 'cameras', 'words', 'biting', 'severe', 'foot', 'symptoms', 'recommend', 'visit', 'nepal', 'lines', 'tired', 'quickly', 'solid', 'placement', 'tests', 'known', 'writer', 'pen', 'realize', 'languages', 'ssc', 'delhi', 'demonetization', 'fact', 'problem', 'fusion', 'developed', 'capable', 'problems', 'face', 'version', 'cheap', 'engineer', 'works', 'option', 'developer', 'negative', 'attractive', 'trumps', 'phd', 'eating', 'anime', 'favorite', 'transaction', 'graduate', 'equity', 'particles', 'mutual', 'putin', 'writing', 'bang', 'changes', 'preparing', 'systems', 'abroad', 'solar', 'gb', 'ram', 'intel', 'mhz', 'methods', 'studies', 'relations', 'efficiently', 'playstation', 'european', 'basis', 'portable', 'british', 'living', 'research', 'earth', 'thinking', 'steel', 'held', 'ratio', 'huge', 'ground', 'straight', 'feet', 'jokes', 'stories', 'perfect', 'constant', 'benefit', 'slowly', 'boring', 'qualified', 'bollywood', 'celebrity', 'statement', 'told', 'prove', 'modern', 'changed', 'produce', 'advanced', 'investment', 'sources', 'describe', 'lots', 'interests', 'camera', 'fly', 'barack', 'piece', 'unusual', 'aspects', 'politics', 'singapore', 'respond', 'boss', 'fired', 'okay', 'conditions', 'loves', 'somebody', 'steps', 'develop', 'presence', 'proof', 'mathematics', 'credit', 'baby', 'base', 'supreme', 'courts', 'anthem', 'cinema', 'elected', 'related', 'production', 'finally', 'agree', 'including', 'left', 'embedded', 'spoken', 'speaking', 'gay', 'w', 'percent', 'ideas', 'burn', 'yes', 'multiple', 'stress', 'decent', 'scientific', 'allowed', 'professional', 'sydney', 'instagram', 'anger', 'raw', 'electronic', 'role', 'present', 'cup', 'coding', 'stamps', 'example', 'insurance', 'applications', 'army', 'removal', 'parents', 'group', 'individual', 'pursuing', 'dishes', 'close', 'maximum', 'seats', 'protect', 'beautiful', 'uses', 'banned', 'photos', 'photo', 'truth', 'clean', 'following', 'turkey', 'asked', 'form', 'studying', 'worlds', 'legal', 'fbi', 'local', 'flow', 'typical', 'actual', 'banking', 'today', 'inventions', 'details', 'contact', 'horror', 'inches', 'trying', 'mexico', 'dangerous', 'amazing', 'reality', 'actors', 'size', 'building', 'teacher', 'teachers', 'west', 'paint', 'exams', 'letters', 'sentence', 'babies', 'opera', 'paid', 'america', 'return', 'fantasy', 'properties', 'calculate', 'percentage', 'wales', 'dubai', 'gifts', 'algorithm', 'answers', 'caste', 'prices', 'property', 'john', 'tried', 'spin', 'x', 'rate', 'distance', 'guitar', 'extremely', 'weak', 'clear', 'jee', 'drop', 'entrance', 'cases', 'neural', 'networks', 'practice', 'update', 'track', 'bike', 'fashion', 'evidence', 'historical', 'jesus', 'existence', 'suggest', 'action', 'cash', 'winning', 'shift', 'plant', 'records', 'evolution', 'provide', 'cat', 'necessary', 'anal', 'charges', 'case', 'winter', 'weather', 'oh', 'stocks', 'skill', 'miles', 'safety', 'regulations', 'handling', 'proposed', 'nra', 'arizona', 'max', 'martial', 'later', 'claims', 'peoples', 'court', 'truly', 'area', 'involved', 'schools', 'vote', 'blind', 'vice', 'airline', 'names', 'permanently', 'needed', 'gre', 'personal', 'experiences', 'pollution', 'land', 'celebrate', 'terms', 'upgrade', 'windows', 'bit', 'famous', 'philosophy', 'sing', 'coaching', 'certification', 'center', 'received', 'updates', 'points', 'troops', 'sister', 'muslim', 'picture', 'comment', 'principle', 'visual', 'puppy', 'wearing', 'significant', 'situation', 'teach', 'treatment', 'sperm', 'penis', 'rates', 'shares', 'corruption', 'let', 'talk', 'custom', 'shoes', 'o', 'adobe', 'script', 'hardest', 'economic', 'football', 'taller', 'hot', 'stopped', 'businesses', 'result', 'simple', 'mumbai', 'magic', 'contract', 'voltage', 'fight', 'shadows', 'blog', 'removed', 'tata', 'broker', 'angry', 'consequences', 'kill', 'computers', 'moment', 'completing', 'domestic', 'human', 'practical', 'vacuum', 'virtual', 'bihar', 'determine', 'scope', 'supply', 'dollars', 'expensive', 'hong', 'drama', 'brexit', 'lock', 'policy', 'medals', 'affordable', 'areas', 'silicon', 'valley', 'australia', 'generally', 'wins', 'elections', 'mother', 'comfortably', 'animal', 'secretary', 'band', 'accounts', 'venezuela', 'microsoft', 'project', 'thoughts', 'awkward', 'messages', 'cards', 'define', 'commit', 'suicide', 'accidentally', 'pressure', 'twice', 'paranormal', 'died', 'heart', 'cope', 'sale', 'planet', 'convert', 'mentor', 'messenger', 'identify', 'speakers', 'art', 'remove', 'son', 'topics', 'specific', 'capacity', 'elements', 'decide', 'act', 'processor', 'license', 'event', 'investigation', 'topic', 'ms', 'hotel', 'unmarried', 'couples', 'harassment', 'staff', 'moral', 'proven', 'worst', 'hindi', 'everyday', 'constitution', 'document', 'key', 'pan', 'traits', 'added', 'goals', 'empire', 'retrieve', 'link', 'scientist', 'bruce', 'hip', 'inner', 'circle', 'flat', 'temperature', 'root', 'count', 'promote', 'colors', 'super', 'pregnant', 'subject', 'angel', 'independent', 'domains', 'brazil', 'unique', 'lot', 'transgender', 'named', 'dropped', 'total', 'browser', 'select', 'keyboard', 'aliens', 'seven', 'wisdom', 'teeth', 'clintons', 'background', 'vocabulary', 'poor', 'mr', 'native', 'fan', 'official', 'metro', 'audio', 'fat', 'acid', 'analyst', 'consider', 'choice', 'happiness', 'lead', 'loss', 'muscle', 'wear', 'seeing', 'channel', 'stand', 'driving', 'alcohol', 'therapy', 'county', 'scared', 'flight', 'queen', 'wine', 'stores', 'bigger', 'genetic', 'save', 'trip', 'battery', 'samsung', 'secure', 'leaves', 'climb', 'academy', 'sharma', 'mistake', 'impress', 'vision', 'panel', 'installation', 'provider', 'near', 'south', 'pronunciation', 'shop', 'floor', 'wall', 'criteria', 'papers', 'mains', 'democracy', 'liberal', 'rule', 'overcome', 'secrets', 'experienced', 'existing', 'bands', 'classic', 'pick', 'wanted', 'shield', 'canadian', 'spanish', 'conductor', 'memory', 'cookies', 'omegle', 'situations', 'strategy', 'economics', 'features', 'married', 'teenager', 'premium', 'quotes', 'skin', 'holes', 'emails', 'comes', 'addiction', 'drugs', 'shipping', 'didnot', 'campus', 'mba', 'earning', 'buying', 'destination', 'paris', 'parts', 'shower', 'territories', 'tech', 'ready', 'xbox', 'e', 'king', 'marry', 'rape', 'scars', 'logic', 'import', 'php', 'losing', 'week', 'visitor', 'bring', 'invited', 'france', 'measure', 'spend', 'cuisine', 'electronics', 'race', 'greater', 'sony', 'grammar', 'invented', 'equation', 'parties', 'meeting', 'embarrassing', 'russia', 'sa', 'anybody', 'june', 'identity', 'nato', 'failed', 'military', 'honda', 'quantum', 'gear', 'box', 'york', 'factor', 'ivy', 'financial', 'spot', 'fit', 'masturbation', 'childhood', 'decided', 'twitter', 'investors', 'backup', 'icloud', 'valid', 'malaysia', 'plastic', 'electric', 'bernie', 'sanders', 'won', 'error', 'handle', 'introduction', 'permanent', 'pair', 'library', 'cold', 'population', 'lyrics', 'fever', 'associated', 'tcs', 'bush', 'fix', 'introduce', 'networking', 'ii', 'emotions', 'sending', 'lower', 'discover', 'tough', 'warming', 'permit', 'listen', 'maybe', 'repair', 'incredibly', 'navy', 'lesser', 'member', 'assistant', 'eggs', 'plants', 'essential', 'stronger', 'iron', 'metal', 'politicians', 'stupid', 'direct', 'load', 'critical', 'yahoo', 'formula', 'capital', 'thought', 'designing', 'weird', 'hearing', 'deaf', 'drivers', 'expert', 'meter', 'visitors', 'notice', 'gps', 'chip', 'pokemon', 'lie', 'rental', 'interior', 'species', 'team', 'arguments', 'overall', 'departments', 'cm', 'penalty', 'alabama', 'success', 'restaurant', 'admitted', 'videos', 'boys', 'abuse', 'comfortable', 'recommended', 'chennai', 'internship', 'soap', 'viewed', 'galaxy', 'relevant', 'guatemala', 'offers', 'approach', 'dell', 'factory', 'arabic', 'century', 'uniform', 'n', 'appropriate', 'disadvantage', 'marine', 'industrial', 'leading', 'path', 'goods', 'harmful', 'zone', 'korean', 'restaurants', 'matrix', 'nearest', 'mm', 'fighters', 'widely', 'accepted', 'interest', 'england', 'terrorists', 'bomb', 'resources', 'wordpress', 'soon', 'half', 'generation', 'driver', 'cpu', 'marriage', 'caught', 'completed', 'equivalent', 'curb', 'electricity', 'issues', 'range', 'michael', 'copy', 'origin', 'sweat', 'phase', 'l', 'k', 'complex', 'demand', 'typing', 'galaxies', 'boost', 'communication', 'reaction', 'philippines', 'object', 'vehicle', 'balance', 'remote', 'directly', 'engine', 'extra', 'metals', 'singer', 'underwear', 'belly', 'button', 'ias', 'caused', 'studio', 'born', 'rice', 'highlands', 'liked', 'advertising', 'cycle', 'translate', 'shouldnot', 'led', 'park', 'touch', 'explanation', 'jersey', 'cats', 'wheel', 'r', 'round', 'enfield', 'forms', 'obsessed', 'pure', 'master', 'positions', 'ipod', 'conversation', 'desert', 'crew', 'motion', 'gurgaon', 'diploma', 'falling', 'located', 'johnson', 'dual', 'checking', 'inflation', 'deposit', 'weighted', 'launched', 'commerce', 'teenage', 'ecosystem', 'destroying', 'variable', 'san', 'francisco', 'angle', 'sql', 'throat', 'temperatures', 'sahara', 'external', 'procrastination', 'sikh', 'jews', 'bottle', 'automobile', 'naruto', 'definition', 'premier', 'isis', 'switched', 'evolved', 'saudi', 'arabia', 'met', 'birds', 'article', 'vol', 'washing']\n",
            "2086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ids, docs = get_texts(spell_checked_docs_file, 'doc_id', 'doc_text')\n",
        "query_ids, queries = get_texts(spell_checked_queries_file, 'query_id', 'query_text')\n",
        "\n",
        "query_id_idx_map = {query_ids[i]: i for i in range(len(query_ids))}\n",
        "doc_id_idx_map = {doc_ids[i]: i for i in range(len(doc_ids))}"
      ],
      "metadata": {
        "id": "22C0oTTPpFwr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs = get_relevant_docs(qdrels_file)"
      ],
      "metadata": {
        "id": "sHni6hNqwuUI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_id_idx_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W8_C62OyAgi",
        "outputId": "9c9ffbe8-c90f-4a4d-d5ec-9b42d6189428"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'4584': 0, '6588': 1, '10113': 2, '7957': 3, '5498': 4, '7614': 5, '7301': 6, '4974': 7, '2615': 8, '8950': 9, '9658': 10, '2429': 11, '1803': 12, '318': 13, '6055': 14, '7841': 15, '9733': 16, '4874': 17, '2824': 18, '744': 19, '9889': 20, '7822': 21, '10024': 22, '4105': 23, '2791': 24, '7988': 25, '378': 26, '8808': 27, '11167': 28, '2757': 29, '7319': 30, '9294': 31, '5799': 32, '5647': 33, '8810': 34, '6483': 35, '3796': 36, '5040': 37, '6253': 38, '3076': 39, '9965': 40, '8456': 41, '9615': 42, '399': 43, '7372': 44, '1578': 45, '8831': 46, '3459': 47, '540': 48, '1956': 49, '3017': 50, '6209': 51, '10155': 52, '8055': 53, '3322': 54, '2658': 55, '6936': 56, '548': 57, '6276': 58, '609': 59, '1350': 60, '10188': 61, '2785': 62, '3521': 63, '9312': 64, '420': 65, '3959': 66, '2822': 67, '2145': 68, '2678': 69, '784': 70, '5162': 71, '7396': 72, '1248': 73, '7463': 74, '1453': 75, '8458': 76, '7222': 77, '5516': 78, '1992': 79, '9376': 80, '9214': 81, '11591': 82, '4356': 83, '5851': 84, '2661': 85, '6957': 86, '12584': 87, '8306': 88, '6244': 89, '8482': 90, '858': 91, '7105': 92, '1166': 93, '9340': 94, '975': 95, '379': 96, '1164': 97, '12508': 98, '1079': 99}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize documents and queries, and find cosine similarities for each query with the documents"
      ],
      "metadata": {
        "id": "3F6cfubesRxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
        "\n",
        "# Create tf idf vectors for the documents and queries\n",
        "doc_vectors = vectorizer.fit_transform(docs)\n",
        "query_vectors = vectorizer.transform(queries)"
      ],
      "metadata": {
        "id": "CMXPyt65sbnc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = cosine_similarity(query_vectors, doc_vectors)"
      ],
      "metadata": {
        "id": "zZVlHYxRsfBS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find top 5 and top 10 similar document indices for each query\n",
        "top_5_indices = np.argsort(similarities, axis=1)[:, -5:]\n",
        "top_10_indices = np.argsort(similarities, axis=1)[:, -10:]\n",
        "\n",
        "# for each query, store the top 5 and top 10 similar document ids\n",
        "top_5_ids = [[doc_ids[i] for i in indices] for indices in top_5_indices]\n",
        "top_10_ids = [[doc_ids[i] for i in indices] for indices in top_10_indices]\n",
        "top_ids = [[i[-1]] for i in top_5_ids]"
      ],
      "metadata": {
        "id": "hqaIYiMxsn0x"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prec_1 = precision_k_score(query_id_idx_map, relevant_docs, top_ids, 1)\n",
        "prec_5 = precision_k_score(query_id_idx_map, relevant_docs, top_5_ids, 5)\n",
        "prec_10 = precision_k_score(query_id_idx_map, relevant_docs, top_10_ids, 10)\n",
        "\n",
        "print(f\"Average precision at 1: {round(prec_1 * 100, 2)} %\")\n",
        "print(f\"Average precision at 5: {round(prec_5 * 100, 2)} %\")\n",
        "print(f\"Average precision at 10: {round(prec_10 * 100, 2)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-AMHjh4y_Ad",
        "outputId": "d3f778c1-01f8-4c88-d2fc-d3ce16ed2855"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average precision at 1: 51.0 %\n",
            "Average precision at 5: 16.2 %\n",
            "Average precision at 10: 8.7 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "V-fkgWra68EA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "Using nltk's porter stemmer here since spacy does not provide a stemmer"
      ],
      "metadata": {
        "id": "EuREmYXY7kHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "mvlGOo2869b6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_vocabulary = get_stemmed_vocabulary(spell_checked_docs_file, \"doc_text\")\n",
        "stemmed_docs = get_stemmed_texts(spell_checked_docs_file, \"doc_text\", ps)\n",
        "stemmed_queries = get_stemmed_texts(spell_checked_queries_file, \"query_text\", ps)\n",
        "\n",
        "print(stemmed_vocabulary)\n",
        "print(len(stemmed_vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5lTj9GoF1L1",
        "outputId": "de350244-b581-4fbc-b5d5-d5f9493a29b9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what', 'is', 'the', 'step', 'by', 'to', 'invest', 'in', 'share', 'market', 'india', 'stori', 'of', 'would', 'happen', 'if', 'indian', 'govern', 'back', 'how', 'can', 'i', 'increas', 'speed', 'my', 'internet', 'connect', 'while', 'use', 'a', 'be', 'pass', 'through', 'whi', 'am', 'mental', 'veri', 'solv', 'it', 'find', 'when', 'that', 'which', 'one', 'water', 'sugar', 'salt', 'and', 'carbon', 'fish', 'surviv', 'moon', 'sun', 'doe', 'say', 'about', 'me', 'im', 'not', 'thi', 'should', 'buy', 'keep', 'her', 'activ', 'far', 'from', 'phone', 'video', 'game', 'good', 'do', 'great', 'you', 'instead', 'compani', 'have', 'code', 'file', 'for', 'free', 'method', 'separ', 'atom', 'are', 'some', 'thing', 'tell', 'reliabl', 'laptop', 'compon', 'read', 'youtub', 'comment', 'see', 'all', 'make', 'physic', 'easi', 'learn', 'wa', 'your', 'first', 'sexual', 'experi', 'like', 'law', 'chang', 'statu', 'student', 'visa', 'green', 'card', 'us', 'they', 'compar', 'immigr', 'canada', 'japan', 'trump', 'presid', 'mean', 'current', 'intern', 'master', 'on', 'an', 'eu', 'will', 'affect', 'present', 'or', 'plan', 'studi', 'girl', 'want', 'friend', 'with', 'guy', 'reject', 'feel', 'after', 'so', 'mani', 'quora', 'user', 'post', 'question', 'answer', 'googl', 'peopl', 'ask', 'easili', 'best', 'digit', 'institut', 'bangladesh', 'look', 'white', 'bomb', 'paint', 'caus', 'someon', 'jealou', 'avoid', 'much', 'hp', 'where', 'convers', 'chart', 'everi', 'time', 'at', 'number', 'same', 'day', 'hand', 'tip', 'job', 'interview', 'process', 'medicin', 'foundat', 'web', 'applic', 'framework', 'societi', 'place', 'too', 'import', 'sport', 'contribut', 'way', 'money', 'onlin', 'prepar', 'ca', 'final', 'know', 'complet', 'exam', 'better', 'despit', 'special', 'care', 'nose', 'get', 'cut', 'dure', 'night', 'stuck', 'most', 'give', 'unit', 'state', 'still', 'accept', 'employ', 'etc', 'citizen', 'becaus', 'their', 'polit', 'view', 'averag', 'ga', 'molecul', 'determin', 'travel', 'websit', 'spain', 'think', 'obama', 'tri', 'take', 'gun', 'away', 'ha', 'there', 'been', 'control', 'initi', 'alreadi', 'own', 'improv', 'skill', 'becom', 'entrepreneur', 'next', 'few', 'year', 'old', 'billionair', 'girlfriend', 'boyfriend', 'did', 'choos', 'repli', 'said', 'we', 'end', 'she', 'wish', 'well', 'call', 'togeth', 'upsc', 'civil', 'servic', 'eagl', 'stop', 'those', 'leg', 'expect', 'confirm', 'mail', 'month', 'trade', 'kid', 'worth', 'long', 'run', 'bore', 'univers', 'recruit', 'new', 'major', 'food', 'graduat', 'follow', 'our', 'darth', 'star', 'war', 'legend', 'charact', 'limit', 'profil', 'stage', 'break', 'up', 'between', 'coupl', 'emot', 'whether', 'male', 'femal', 'who', 'more', 'boy', 'exampl', 'product', 'oil', 'made', 'career', 'grade', 'b', 'program', 'rbi', 'play', 'regular', 'player', 'alway', 'sad', 'youv', 'ever', 'dish', 'tax', 'offic', 'difficult', 'into', 'appli', 'rise', 'senior', 'hi', 'lie', 'true', 'he', 'attract', 'song', 'danc', 'suddenli', 'block', 'off', 'gmail', 'rememb', 'password', 'just', 'realiz', 'recoveri', 'email', 'no', 'longer', 'aliv', 'recov', 'download', 'content', 'singl', 'store', 'without', 'registr', 'normal', 'dark', 'ring', 'around', 'insid', 'eye', 'treat', 'harri', 'potter', 'book', 'curs', 'child', 'bad', 'depress', 'even', 'whole', 'famili', 'databas', 'java', 'languag', 'comput', 'energi', 'produc', 'lightn', 'possibl', 'review', 'perform', 'test', 'cost', 'as', 'germani', 'come', 'els', 'lost', 'gain', 'ani', 'enjoy', 'dress', 'type', 'differ', 'bodi', 'person', 'disord', 'autism', 'speak', 'english', 'help', 'quickbook', 'auto', 'data', 'support', 'collect', 'custom', 'usa', 'richest', 'reach', 'level', 'fire', 'bullet', 'go', 'faster', 'than', 'sound', 'shot', 'death', 'occur', 'prevent', 'cancer', 'log', 'out', 'account', 'ip', 'address', 'devic', 'name', 'sell', 'purpos', 'life', 'actual', 'muslim', 'christian', 'put', 'them', 'ship', 'model', 'illeg', 'right', 'happi', 'birthday', 'open', 'commerci', 'fm', 'radio', 'station', 'citi', 'procedur', 'zealand', 'technic', 'employe', 'sale', 'high', 'salari', 'incom', 'field', 'psycholog', 'pay', 'height', 'also', 'were', 'effect', 'cambodia', 'earthquak', 'these', 'trust', 'under', 'inr', 'prove', 'ground', 'part', 'refer', 'class', 'chemistri', 'cbse', 'board', 'nation', 'technolog', 'social', 'lesson', 'father', 'befor', 'leav', 'romant', 'movi', 'seen', 'seem', 'real', 'express', 'influenc', 'print', 'work', 'attend', 'associ', 'manag', 'earli', 'join', 'gener', 'requir', 'base', 'softwar', 'busi', 'could', 'android', 'realli', 'pakistan', 'over', 'air', 'attack', 'nuclear', 'line', 'speech', 'react', 'strategi', 'union', 'forc', 'american', 'fiction', 'novel', 'forgot', 'recent', 'result', 'higher', 'rest', 'both', 'short', 'heard', 'whatsapp', 'love', 'competit', 'hire', 'bank', 's', 't', 'rank', 'search', 'distribut', 'traffic', 'organ', 'vs', 'page', 'second', 'watch', 'power', 'countri', 'world', 'pain', 'low', 'heat', 'ice', 'china', 'chines', 'advantag', 'cri', 'stick', 'pictur', 'teen', 'women', 'allow', 'age', 'similar', 'basic', 'tool', 'exist', 'donot', 'intellig', 'tooth', 'dri', 'mark', 'need', 'buffer', 'declar', 'against', 'each', 'other', 'win', 'miner', 'hold', 'highest', 'electr', 'charg', 'greatest', 'mysteri', 'altern', 'machin', 'creat', 'set', 'un', 'futur', 'budget', 'amount', 'vitamin', 'c', 'diseas', 'glass', 'scale', 'two', 'letter', 'access', 'ancient', 'six', 'parti', 'talk', 'success', 'cours', 'regist', 'domain', 'site', 'older', 'men', 'young', 'strongest', 'structur', 'onli', 'matter', 'human', 'evil', 'transport', 'cell', 'hair', 'head', 'then', 'loos', 'retir', 'possess', 'mass', 'north', 'korea', 'diesel', 'petrol', 'diet', 'grow', 'black', 'hole', 'correct', 'quot', 'offer', 'stock', 'option', 'uber', 'ident', 'piec', 'least', 'appl', 'among', 'children', 'such', 'hour', 'late', 'dog', 'bite', 'britain', 'rule', 'standard', 'given', 'afraid', 'everyth', 'red', 'key', 'season', 'lose', 'tall', 'forget', 'wife', 'kill', 'releas', 'issu', 'switch', 'canon', 'price', 'member', 'joke', 'biggest', 'got', 'london', 'pm', 'advic', 'expens', 'save', 'down', 'apart', 'kingdom', 'sourc', 'transfer', 'paypal', 'send', 'withdraw', 'deep', 'rs', 'forward', 'engin', 'suit', 'abus', 'sever', 'ex', 'past', 'hurt', 'gross', 'payment', 'agre', 'oldest', 'kerala', 'confid', 'anyon', 'relat', 'god', 'draw', 'hindu', 'mytholog', 'music', 'spotifi', 'fix', 'fund', 'live', 'robert', 'al', 'regard', 'actor', 'prefer', 'small', 'anim', 'delet', 'privat', 'order', 'abl', 'investig', 'check', 'histori', 'wifi', 'mobil', 'network', 'signific', 'battl', 'contrast', 'creativ', 'colleg', 'admiss', 'essay', 'public', 'cover', 'enter', 'pursu', 'pleas', 'but', 'author', 'start', 'hyderabad', 'whom', 'lord', 'combin', 'along', 'enhanc', 'must', 'taken', 'seat', 'mac', 'facebook', 'pro', 'director', 'tv', 'show', 'die', 'fluent', 'impact', 'paper', 'note', 'estat', 'fastest', 'lectur', 'conduct', 'almost', 'week', 'event', 'behavior', 'daili', 'develop', 'hate', 'hillari', 'clinton', 'reason', 'period', 'woman', 'cook', 'especi', 'cultur', 'respons', 'alon', 'equal', 'lack', 'belief', 'term', 'claim', 'assassin', 'seri', 'color', 'document', 'shop', 'continu', 'presidenti', 'campaign', 'democrat', 'candid', 'serv', 'nomine', 'channel', 'doesnot', 'instal', 'growth', 'list', 'benefit', 'object', 'posit', 'respect', 'fun', 'gpa', 'enough', 'top', 'harvard', 'school', 'profit', 'binari', 'packag', 'secur', 'inform', 'earn', 'sentenc', 'word', 'here', 'home', 'wireless', 'anoth', 'dollar', 'drive', 'road', 'turn', 'car', 'shell', 'linux', 'religion', 'percept', 'cognit', 'tatkal', 'wait', 'reserv', 'train', 'cancel', 'leagu', 'drink', 'lake', 'wildlif', 'adult', 'walk', 'promis', 'now', 'sold', 'worri', 'opinion', 'littl', 'jump', 'five', 'minut', 'gate', 'cs', 'stream', 'scienc', 'materi', 'understand', 'analysi', 'math', 'begin', 'algorithm', 'suitabl', 'mix', 'ball', 'weigh', 'weight', 'form', 'man', 'ad', 'sign', 'smart', 'hide', 'infinit', 'conserv', 'expand', 'potenti', 'publish', 'write', 'flash', 'fast', 'mile', 'legal', 'larg', 'rel', 'implement', 'bill', 'common', 'exactli', 'accur', 'ill', 'blood', 'qualiti', 'leader', 'sea', 'bay', 'narendra', 'modi', 'prime', 'minist', 'eat', 'egg', 'ticket', 'done', 'core', 'con', 'journal', 'resolut', 'had', 'dream', 'score', 'minimum', 'oper', 'room', 'broken', 'doctor', 'surgeri', 'knowledg', 'app', 'scratch', 'rich', 'prison', 'suffer', 'visit', 'idea', 'text', 'till', 'film', 'featur', 'strong', 'move', 'health', 'cloth', 'cotton', 'bone', 'meat', 'tast', 'brand', 'liber', 'dead', 'river', 'havenot', 'miss', 'him', 'last', 'region', 'yourself', 'spend', 'chocol', 'gold', 'coast', 'entir', 'deal', 'fear', 'reduc', 'replac', 'crack', 'screen', 'iphon', 'interest', 'innov', 'rais', 'ban', 'eccentr', 'contract', 'episod', 'suck', 'news', 'media', 'platform', 'beer', 'opportun', 'construct', 'acquir', 'moral', 'deposit', 'hollywood', 'repeat', 'kolkata', 'fill', 'motor', 'solut', 'incid', 'self', 'jio', 'gap', 'excus', 'explain', 'sweet', 'predict', 'birth', 'function', 'creator', 'nice', 'financ', 'design', 'system', 'someth', 'avail', 'request', 'meet', 'appoint', 'goe', 'implic', 'currenc', 'economi', 'consid', 'cheat', 'relationship', 'id', 'concentr', 'invit', 'sent', 'chanc', 'german', 'pet', 'habit', 'rock', 'ok', 'husband', 'stay', 'partner', 'consent', 'cse', 'iit', 'consult', 'firm', 'biolog', 'fact', 'hard', 'hang', 'fake', 'locat', 'quit', 'smoke', 'resist', 'actress', 'popular', 'date', 'tabl', 'frame', 'below', 'noth', 'j', 'tamil', 'messag', 'contact', 'written', 'uk', 'sector', 'medic', 'industri', 'across', 'icon', 'racist', 'root', 'zero', 'edit', 'add', 'speaker', 'elect', 'donald', 'snow', 'jon', 'theori', 'hous', 'nadu', 'propos', 'feed', 'anxieti', 'stomach', 'degre', 'mechan', 'maintain', 'peac', 'mind', 'build', 'shake', 'cure', 'ignor', 'motiv', 'assess', 'global', 'discharg', 'pregnanc', 'scientist', 'style', 'wed', 'western', 'condit', 'bed', 'offshor', 'khan', 'titl', 'gift', 'lyric', 'mom', 'hit', 'freedom', 'middl', 'ive', 'found', 'drug', 'project', 'kind', 'harm', 'cricket', 'tea', 'entri', 'mit', 'select', 'remain', 'scene', 'crush', 'never', 'accord', 'robot', 'kg', 'summer', 'million', 'regret', 'divorc', 'dna', 'myself', 'cool', 'suggest', 'secret', 'organis', 'map', 'financi', 'teach', 'might', 'comedi', 'convinc', 'fight', 'again', 'introduc', 'il', 'l', 'sodium', 'spous', 'pop', 'rid', 'affili', 'non', 'ac', 'island', 'describ', 'tourist', 'queen', 'healthi', 'meal', 'sore', 'ugc', 'net', 'literatur', 'latest', 'protein', 'exchang', 'breakfast', 'coffe', 'graphic', 'israel', 'figur', 'fall', 'includ', 'lee', 'favourit', 'decis', 'euro', 'saltwat', 'candi', 'iran', 'went', 'piano', 'natur', 'plu', 'm', 'adopt', 'origin', 'certif', 'light', 'factor', 'main', 'section', 'big', 'discuss', 'point', 'anymor', 'crimin', 'amazon', 'daniel', 'ek', 'phrase', 'touch', 'weed', 'polic', 'trend', 'recip', 'chicken', 'pizza', 'mountain', 'tenni', 'damag', 'unlock', 'particular', 'atm', 'wrong', 'graviti', 'dimens', 'interstellar', 'fighter', 'pilot', 'less', 'wake', 'sleep', 'techniqu', 'climat', 'revers', 'believ', 'planet', 'congress', 'purchas', 'thank', 'safe', 'pound', 'pokémon', 'blue', 'valu', 'gandhi', 'gym', 'morn', 'swim', 'milk', 'trick', 'educ', 'imag', 'highway', 'california', 'texa', 'space', 'chemic', 'programm', 'iii', 'risk', 'foreign', 'crop', 'circl', 'viru', 'certain', 'subject', 'prospect', 'display', 'appear', 'commun', 'flower', 'sex', 'younger', 'macbook', 'brain', 'server', 'exercis', 'camera', 'difficulti', 'foot', 'symptom', 'recommend', 'nepal', 'tire', 'quickli', 'solid', 'placement', 'known', 'writer', 'pen', 'provid', 'variou', 'ssc', 'delhi', 'toward', 'demonet', 'problem', 'fusion', 'capabl', 'face', 'version', 'cheap', 'translat', 'neg', 'four', 'desir', 'clear', 'phd', 'favorit', 'transact', 'per', 'equiti', 'three', 'destroy', 'particl', 'mutual', 'putin', 'bang', 'seriou', 'abroad', 'solar', 'gb', 'ram', 'intel', 'mhz', 'onc', 'effici', 'playstat', 'european', 'basi', 'portabl', 'british', 'manufactur', 'research', 'earth', 'photograph', 'steel', 'held', 'ratio', 'conflict', 'side', 'huge', 'straight', 'feet', 'perfect', 'constant', 'sit', 'slowli', 'qualifi', 'bollywood', 'celebr', 'everyon', 'anyth', 'statement', 'told', 'modern', 'advanc', 'lot', 'fli', 'refus', 'barack', 'classic', 'unusu', 'aspect', 'singapor', 'respond', 'boss', 'via', 'okay', 'somebodi', 'presenc', 'mathemat', 'proof', 'credit', 'due', 'babi', 'suprem', 'court', 'anthem', 'cinema', 'professor', 'may', 'left', 'corpor', 'embed', 'spoken', 'gay', 'w', 'percent', 'rate', 'burn', 'ye', 'multipl', 'stress', 'decent', 'scientif', 'behind', 'profession', 'sydney', 'instagram', 'anger', 'raw', 'electron', 'homosexu', 'role', 'cup', 'stamp', 'reflect', 'yet', 'insur', 'armi', 'remov', 'parent', 'group', 'individu', 'front', 'close', 'repres', 'maximum', 'protect', 'interfac', 'beauti', 'photo', 'sort', 'truth', 'sinc', 'clean', 'spot', 'turkey', 'brother', 'sister', 'record', 'resid', 'street', 'fbi', 'agent', 'local', 'flow', 'typic', 'today', 'invent', 'detail', 'within', 'horror', 'betray', 'inch', 'mexico', 'danger', 'fail', 'amaz', 'concept', 'realiti', 'column', 'size', 'teacher', 'west', 'pick', 'opera', 'paid', 'critic', 'tattoo', 'america', 'return', 'finish', 'fantasi', 'properti', 'hear', 'ultim', 'calcul', 'percentag', 'wale', 'dubai', 'cast', 'tree', 'john', 'spin', 'x', 'distanc', 'target', 'guitar', 'extrem', 'weak', 'jee', 'drop', 'entranc', 'often', 'case', 'neural', 'practic', 'updat', 'track', 'ride', 'bike', 'item', 'fashion', 'evid', 'histor', 'jesu', 'action', 'cash', 'shift', 'kapoor', 'plant', 'evolut', 'cat', 'necessari', 'anal', 'winter', 'weather', 'oh', 'safeti', 'regul', 'handl', 'nra', 'arizona', 'max', 'martial', 'later', 'bring', 'truli', 'area', 'involv', 'count', 'vote', 'blind', 'vice', 'airlin', 'perman', 'gre', 'pollut', 'land', 'upgrad', 'window', 'bit', 'famou', 'agricultur', 'philosophi', 'sing', 'coach', 'center', 'receiv', 'troop', 'principl', 'filter', 'sd', 'visual', 'puppi', 'wear', 'situat', 'arrang', 'themselv', 'franc', 'treatment', 'sperm', 'peni', 'denomin', 'corrupt', 'let', 'shoe', 'third', 'complex', 'o', 'adob', 'script', 'hardest', 'econom', 'footbal', 'rather', 'taller', 'hot', 'simpl', 'mumbai', 'magic', 'g', 'voltag', 'shadow', 'blog', 'son', 'tata', 'broker', 'angri', 'advis', 'consequ', 'moment', 'domest', 'vacuum', 'virtual', 'bihar', 'scope', 'suppli', 'na', 'hong', 'drama', 'brexit', 'lock', 'format', 'independ', 'polici', 'olymp', 'medal', 'afford', 'silicon', 'valley', 'australia', 'mother', 'comfort', 'itself', 'secretari', 'evolv', 'band', 'export', 'venezuela', 'microsoft', 'thought', 'awkward', 'rare', 'defin', 'commit', 'suicid', 'accident', 'pressur', 'twice', 'paranorm', 'heart', 'cope', 'convert', 'mentor', 'messeng', 'identifi', 'satellit', 'race', 'art', 'topic', 'specif', 'capac', 'mention', 'element', 'decid', 'act', 'processor', 'licens', 'chat', 'politician', 'probabl', 'ms', 'hotel', 'unmarri', 'harass', 'staff', 'proven', 'worst', 'hindi', 'everyday', 'reaction', 'constitut', 'pan', 'dynam', 'trait', 'admit', 'definit', 'achiev', 'goal', 'empir', 'retriev', 'logic', 'link', 'articl', 'depart', 'bruce', 'abov', 'hip', 'round', 'inner', 'flat', 'temperatur', 'promot', 'super', 'pregnant', 'angel', 'brazil', 'islam', 'territori', 'uniqu', 'transgend', 'total', 'browser', 'keyboard', 'alien', 'seven', 'wisdom', 'teeth', 'background', 'vocabulari', 'poor', 'mr', 'nativ', 'fan', 'offici', 'metro', 'pattern', 'audio', 'fat', 'acid', 'analyst', 'mi', 'choic', 'lead', 'loss', 'muscl', 'stand', 'alcohol', 'therapi', 'counti', 'scare', 'theme', 'flight', 'mt', 'denmark', 'deni', 'copi', 'wine', 'bigger', 'genet', 'trip', 'batteri', 'samsung', 'sc', 'decreas', 'climb', 'bag', 'academi', 'sharma', 'mistak', 'impress', 'vision', 'panel', 'near', 'south', 'though', 'pronunci', 'suppos', 'floor', 'wall', 'criteria', 'democraci', 'examin', 'full', 'overcom', 'experienc', 'shield', 'canadian', 'spanish', 'conductor', 'memori', 'cooki', 'omegl', 'pull', 'marri', 'wonder', 'teenag', 'captain', 'premium', 'skin', 'sweat', 'doubl', 'boil', 'addict', 'didnot', 'campu', 'mba', 'destin', 'nail', 'pari', 'hors', 'shower', 'tech', 'readi', 'xbox', 'e', 'king', 'reput', 'ion', 'cousin', 'rape', 'scar', 'php', 'notic', 'visitor', 'theyr', 'contain', 'measur', 'cuisin', 'voter', 'greater', 'soni', 'grammar', 'load', 'equat', 'embarrass', 'russia', 'sa', 'anybodi', 'june', 'passport', 'ethic', 'nato', 'militari', 'repair', 'honda', 'sometim', 'quantum', 'gear', 'box', 'york', 'pitch', 'engag', 'ivi', 'path', 'russian', 'fit', 'masturb', 'childhood', 'twitter', 'investor', 'backup', 'icloud', 'vehicl', 'steal', 'valid', 'malaysia', 'disappear', 'plastic', 'berni', 'sander', 'won', 'error', 'arm', 'introduct', 'assist', 'pair', 'librari', 'comic', 'cold', 'popul', 'statist', 'san', 'fever', 'tc', 'marin', 'bush', 'carri', 'ii', 'plane', 'lower', 'discov', 'tough', 'warm', 'arriv', 'permit', 'listen', 'mayb', 'incred', 'whose', 'himself', 'navi', 'lesser', 'essenti', 'toler', 'stronger', 'iron', 'metal', 'duti', 'hope', 'stupid', 'direct', 'expert', 'automat', 'rout', 'yahoo', 'formula', 'capit', 'pool', 'weird', 'deaf', 'driver', 'meter', 'gp', 'chip', 'pokemon', 'wash', 'succeed', 'rental', 'interior', 'angl', 'speci', 'team', 'argument', 'centr', 'overal', 'mode', 'cm', 'penalti', 'alabama', 'park', 'restaur', 'pin', 'chennai', 'internship', 'balanc', 'soap', 'd', 'galaxi', 'relev', 'guatemala', 'approach', 'dell', 'factori', 'documentari', 'arab', 'centuri', 'uniform', 'n', 'appropri', 'disadvantag', 'interact', 'remot', 'zone', 'korean', 'matrix', 'nearest', 'wide', 'host', 'england', 'terrorist', 'transit', 'resourc', 'wordpress', 'soon', 'half', 'wage', 'poem', 'cpu', 'marriag', 'caught', 'equival', 'curb', 'ghost', 'rang', 'michael', 'trace', 'jayalalitha', 'phase', 'k', 'demand', 'boost', 'struggl', 'strike', 'philippin', 'directli', 'extra', 'instanc', 'debat', 'singer', 'underwear', 'belli', 'button', 'ia', 'studio', 'born', 'rice', 'highland', 'advertis', 'cycl', 'shouldnot', 'led', 'explan', 'jersey', 'wheel', 'r', 'judg', 'enfield', 'obsess', 'pure', 'ipod', 'desert', 'crew', 'variabl', 'sampl', 'bird', 'motion', 'gurgaon', 'diploma', 'tower', 'coloni', 'former', 'johnson', 'compet', 'dealer', 'dual', 'inflat', 'bar', 'launch', 'commerc', 'architect', 'ecosystem', 'cluster', 'formal', 'francisco', 'sql', 'throat', 'sahara', 'extern', 'procrastin', 'sikh', 'jew', 'bottl', 'automobil', 'naruto', 'tortur', 'premier', 'isi', 'wave', 'dose', 'saudi', 'arabia', 'tank', 'met', 'scheme', 'vol']\n",
            "2032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the documents and queries, and find cosine similarities for each query with documents"
      ],
      "metadata": {
        "id": "r650FjYsPrqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_stemmed = TfidfVectorizer(vocabulary=stemmed_vocabulary)\n",
        "\n",
        "# Create tf idf vectors for the documents and queries\n",
        "stemmed_doc_vectors = vectorizer_stemmed.fit_transform(stemmed_docs)\n",
        "stemmed_query_vectors = vectorizer_stemmed.transform(stemmed_queries)"
      ],
      "metadata": {
        "id": "1GQ4bILwPqF7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_similarities = cosine_similarity(stemmed_query_vectors, stemmed_doc_vectors)\n",
        "\n",
        "# find top 5 and top 10 similar document indices for each query\n",
        "stemmed_top_5_indices = np.argsort(stemmed_similarities, axis=1)[:, -5:]\n",
        "stemmed_top_10_indices = np.argsort(stemmed_similarities, axis=1)[:, -10:]\n",
        "\n",
        "# for each query, store the top 5 and top 10 similar document ids\n",
        "stemmed_top_5_ids = [[doc_ids[i] for i in indices] for indices in stemmed_top_5_indices]\n",
        "stemmed_top_10_ids = [[doc_ids[i] for i in indices] for indices in stemmed_top_10_indices]\n",
        "stemmed_top_ids = [[ids[-1]] for ids in stemmed_top_5_ids]"
      ],
      "metadata": {
        "id": "TLbal6o7RCkg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision Scores"
      ],
      "metadata": {
        "id": "j20Lh7gXRF-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec_1_stemmed = precision_k_score(query_id_idx_map, relevant_docs, stemmed_top_ids, 1)\n",
        "prec_5_stemmed = precision_k_score(query_id_idx_map, relevant_docs, stemmed_top_5_ids, 5)\n",
        "prec_10_stemmed = precision_k_score(query_id_idx_map, relevant_docs, stemmed_top_10_ids, 10)\n",
        "\n",
        "print(f\"Average precision at 1 for stemmed: {round(prec_1_stemmed * 100, 2)} %\")\n",
        "print(f\"Average precision at 5 for stemmed: {round(prec_5_stemmed * 100, 2)} %\")\n",
        "print(f\"Average precision at 10 for stemmed: {round(prec_10_stemmed * 100, 2)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MRJ7DFAREdt",
        "outputId": "ea452eb9-bfef-4373-9414-f7804b373841"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average precision at 1 for stemmed: 61.0 %\n",
            "Average precision at 5 for stemmed: 19.0 %\n",
            "Average precision at 10 for stemmed: 10.4 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "Add lemmatizer back to the nlp pipe"
      ],
      "metadata": {
        "id": "J-R7czJ5RPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe('lemmatizer')\n",
        "nlp.initialize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdIsBYvlRT9i",
        "outputId": "f8418615-7ff2-4d50-f74a-02f8930128e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<thinc.optimizers.Optimizer at 0x7aa8237723e0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_vocabulary = get_vocabulary(nlp, spell_checked_docs_file, \"doc_text\")\n",
        "lemmatized_docs = get_processed_text(nlp, spell_checked_docs_file, \"doc_text\")\n",
        "lemmatized_queries = get_processed_text(nlp, spell_checked_queries_file, \"query_text\")\n",
        "\n",
        "print(len(lemmatized_vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJNY6i8hfrvO",
        "outputId": "0c896094-0f2b-432a-8233-fd5a40f6565b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/attributeruler.py:149: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
            "  matches = self.matcher(doc, allow_missing=True, as_spans=False)\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the documents and queries, and find cosine similarities for each query with documents"
      ],
      "metadata": {
        "id": "A7aNAx1Q0ey2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_lemmatized = TfidfVectorizer(vocabulary=lemmatized_vocabulary)\n",
        "\n",
        "# Create tf idf vectors for the documents and queries\n",
        "lemmatized_doc_vectors = vectorizer_lemmatized.fit_transform(lemmatized_docs)\n",
        "lemmatized_query_vectors = vectorizer_lemmatized.transform(lemmatized_queries)"
      ],
      "metadata": {
        "id": "UloKwDAM0cnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCCqjB2U5oob"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}