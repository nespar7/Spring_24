{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2yziUb3EgVl9hFy8lNuYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nespar7/Spring_24/blob/main/NLP/Assignments/Assignment_1/NLP_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "0r-rTccDuFr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please reload session after the installations are done and DO NOT run the install block again."
      ],
      "metadata": {
        "id": "igDs85sED7uh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64yWLxtXoOrG"
      },
      "outputs": [],
      "source": [
        "!pip install spacy --force-reinstall\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install contextualSpellCheck\n",
        "!pip install scikit-learn\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure spacy version is 3.7.2\n",
        "!python -m spacy info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "URBUXJa0DvcM",
        "outputId": "63c34c81-8f44-4a63-a242-0679e712613b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.7.2                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-6.1.58+-x86_64-with-glibc2.35\n",
            "Python version   3.10.12                       \n",
            "Pipelines        en_core_web_sm (3.7.1)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "YO4VRNdHoxSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import contextualSpellCheck\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from nltk import download as nltk_dw\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk_dw('punkt')"
      ],
      "metadata": {
        "id": "Nt8Cj2NMpmTA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b620be30-df78-41a4-fc62-da0e0b5b44db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "opEEAC1UqVO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "contextualSpellCheck.add_to_pipe(nlp)\n",
        "\n",
        "# Removing since initially only spell check is to be done\n",
        "nlp.remove_pipe('lemmatizer')\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('ner')"
      ],
      "metadata": {
        "id": "h34NMkubqXbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa246b1-8104-43e9-e53d-a5237fcaf4f5"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7bb7c8fdcf20>)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Definitions"
      ],
      "metadata": {
        "id": "Dk6ujq08qMEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_check(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    return doc._.performed_spellCheck, doc._.outcome_spellCheck"
      ],
      "metadata": {
        "id": "K9L5_aGpprWg"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spellings(csv_file, out_csv_file, column_name, should_log=False):\n",
        "    # read a csv file, in each line, see if spellCheck was performed(if text was correctly spelled) and print original and corrected texts\n",
        "    lines = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        # Index of the column to be spell checked\n",
        "        column_index = lines[0].index(column_name)\n",
        "\n",
        "        # For each line, perform spell check\n",
        "        for i in range(1, len(lines)):\n",
        "            line = lines[i][column_index]\n",
        "            performed, outcome = spell_check(line)\n",
        "\n",
        "\n",
        "            # If spell check was performed, update the line and print the original and corrected texts\n",
        "            if performed:\n",
        "                # Update the line\n",
        "                lines[i][column_index] = outcome\n",
        "\n",
        "                if should_log:\n",
        "                    print(line)\n",
        "                    print(outcome)\n",
        "                    print()\n",
        "\n",
        "            if i%100 == 0:\n",
        "                print(i)\n",
        "\n",
        "    with open(out_csv_file, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(lines)\n",
        "\n",
        "    print(\"Spell check completed\")"
      ],
      "metadata": {
        "id": "hJlxZytPw3oK"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process(csv_file, out_csv_file, column_name):\n",
        "    # read a csv file, in each line, remove characters apart from alphanumerics and whitespaces in the given column\n",
        "    lines = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        # Index of the column to be pre processed\n",
        "        column_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            lines[i][column_index] = ''.join([c for c in lines[i][column_index] if c.isalnum() or c.isspace()])\n",
        "\n",
        "    # write the pre-processed data to the resulting csv file\n",
        "    with open(out_csv_file, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(lines)"
      ],
      "metadata": {
        "id": "iXrFFBtPsQIV"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary(nlp, csv_file, column_name):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    docs = []\n",
        "\n",
        "    print(nlp.pipeline)\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        doc_col_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            docs.append(lines[i][doc_col_index])\n",
        "\n",
        "        num_docs = len(docs)\n",
        "        lower_lim = 5\n",
        "        upper_lim = int(num_docs * 0.85)\n",
        "\n",
        "        # tokenize the documents using spacy and store the tokens in a vocabulary,\n",
        "        # remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "\n",
        "        doc_tokenized = []\n",
        "\n",
        "        # use the model to tokenize the documents\n",
        "        for doc in docs:\n",
        "            # process doc using the model passed\n",
        "            doc = nlp(doc)\n",
        "\n",
        "            # Select alphabetical tokens and remove stop words\n",
        "            tokens = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "            doc_tokenized.append(tokens)\n",
        "\n",
        "        # Now that we have the tokenized documents, we can create a vocabulary\n",
        "        vocabulary = {}\n",
        "\n",
        "        for doc in doc_tokenized:\n",
        "            for token in doc:\n",
        "                if token in vocabulary:\n",
        "                    vocabulary[token] += 1\n",
        "                else:\n",
        "                    vocabulary[token] = 1\n",
        "\n",
        "        # Remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "        vocabulary = [k for k, v in vocabulary.items() if v >= lower_lim and v <= upper_lim]\n",
        "\n",
        "        return vocabulary"
      ],
      "metadata": {
        "id": "AMSjXyLexJ9l"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_processed_text(nlp, csv_file, col_name):\n",
        "    # Read the csv file and return the processed text using the passed model\n",
        "    lines = []\n",
        "    docs = []\n",
        "\n",
        "    print(nlp.pipeline)\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        col_index = lines[0].index(col_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            doc = nlp(lines[i][col_index])\n",
        "            docs.append(' '.join([token.text.lower() for token in doc]))\n",
        "\n",
        "    return docs"
      ],
      "metadata": {
        "id": "nNLDLx_7f5tT"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stemmed_vocabulary(csv_file, col_name):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    docs = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        doc_col_index = lines[0].index(col_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            docs.append(lines[i][doc_col_index])\n",
        "\n",
        "    # tokenize the documents using nltk and store the tokens in a vocabulary,\n",
        "    # remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "\n",
        "    lower_lim = 5\n",
        "    upper_lim = int(len(docs) * 0.85)\n",
        "\n",
        "    doc_tokenized = []\n",
        "\n",
        "    # use nltk to tokenize the documents\n",
        "    for doc in docs:\n",
        "        doc = word_tokenize(doc)\n",
        "\n",
        "        tokens = [token.lower() for token in doc if token.isalpha()]\n",
        "\n",
        "        # stem the tokens\n",
        "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
        "\n",
        "        doc_tokenized.append(stemmed_tokens)\n",
        "\n",
        "    vocabulary = {}\n",
        "\n",
        "    for doc in doc_tokenized:\n",
        "        for token in doc:\n",
        "            if token in vocabulary:\n",
        "                vocabulary[token] += 1\n",
        "            else:\n",
        "                vocabulary[token] = 1\n",
        "\n",
        "    # Remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "    stemmed_vocabulary = [k for k, v in vocabulary.items() if v >= lower_lim and v <= upper_lim]\n",
        "\n",
        "    return stemmed_vocabulary"
      ],
      "metadata": {
        "id": "pqW6THucFcwZ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stemmed_texts(csv_file, column_name, porter_stemmer):\n",
        "    # Read the spell checked docs csv file and store the queries in a list\n",
        "    texts = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        col_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            texts.append(lines[i][col_index])\n",
        "\n",
        "    stemmed_texts = []\n",
        "\n",
        "    # use nltk to tokenize the documents\n",
        "    for text in texts:\n",
        "        text = word_tokenize(text)\n",
        "\n",
        "        tokens = [token.lower() for token in text]\n",
        "\n",
        "        # stem the tokens\n",
        "        stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        stemmed_texts.append(' '.join(stemmed_tokens))\n",
        "\n",
        "    return stemmed_texts"
      ],
      "metadata": {
        "id": "IK51o0M6OvSc"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemmatized_vocabulary(nlp, csv_file, column_name):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    docs = []\n",
        "\n",
        "    print(nlp.pipeline)\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        doc_col_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            docs.append(lines[i][doc_col_index])\n",
        "\n",
        "        num_docs = len(docs)\n",
        "        lower_lim = 5\n",
        "        upper_lim = int(num_docs * 0.85)\n",
        "\n",
        "        # tokenize the documents using spacy and store the tokens in a vocabulary,\n",
        "        # remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "\n",
        "        doc_tokenized = []\n",
        "\n",
        "        # use the model to tokenize the documents\n",
        "        for i, doc in enumerate(docs):\n",
        "            # process doc using the model passed\n",
        "            doc = nlp(doc)\n",
        "\n",
        "            # Select alphabetical tokens and remove stop words\n",
        "            tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "            doc_tokenized.append(tokens)\n",
        "\n",
        "        # Now that we have the tokenized documents, we can create a vocabulary\n",
        "        vocabulary = {}\n",
        "\n",
        "        for doc in doc_tokenized:\n",
        "            for token in doc:\n",
        "                if token in vocabulary:\n",
        "                    vocabulary[token] += 1\n",
        "                else:\n",
        "                    vocabulary[token] = 1\n",
        "\n",
        "        # Remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "        vocabulary = [k for k, v in vocabulary.items() if v >= lower_lim and v <= upper_lim]\n",
        "\n",
        "        return vocabulary"
      ],
      "metadata": {
        "id": "DkV3nqRS74Cw"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemmatized_text(nlp, csv_file, col_name):\n",
        "    # Read the csv file and return the processed text using the passed model\n",
        "    lines = []\n",
        "    docs = []\n",
        "\n",
        "    print(nlp.pipeline)\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        col_index = lines[0].index(col_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            doc = nlp(lines[i][col_index])\n",
        "            docs.append(' '.join([token.lemma_ for token in doc]))\n",
        "\n",
        "    return docs"
      ],
      "metadata": {
        "id": "8iQTswhc8NFG"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_texts(nlp, csv_file, id_col, text_col):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    ids = []\n",
        "    texts = []\n",
        "\n",
        "    print(nlp.pipeline)\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        id_col_index = lines[0].index(id_col)\n",
        "        text_col_index = lines[0].index(text_col)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            ids.append(lines[i][id_col_index])\n",
        "\n",
        "            doc = nlp(lines[i][text_col_index])\n",
        "            texts.append(' '.join([token.text.lower() for token in doc]))\n",
        "\n",
        "    return ids, texts"
      ],
      "metadata": {
        "id": "J9UTSpYTrXO2"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_docs(qdrels):\n",
        "    # Get relevant docs list for each query from qdrels.csv file\n",
        "    relevant_docs = {}\n",
        "\n",
        "    with open(qdrels, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            line = lines[i]\n",
        "\n",
        "            query_id = line[1]\n",
        "            doc_id = line[2]\n",
        "\n",
        "            if query_id in relevant_docs:\n",
        "                relevant_docs[query_id].append(doc_id)\n",
        "            else:\n",
        "                relevant_docs[query_id] = [doc_id]\n",
        "\n",
        "    return relevant_docs"
      ],
      "metadata": {
        "id": "ciOjy6BSv6S1"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_k_score(query_id_idx_map, relevant_docs, top_docs, k):\n",
        "    # For each query in relevant_docs, calculate true positives in top_docs and return average precision_k score\n",
        "    precision_k = 0\n",
        "\n",
        "    for query_id, relevant_docs_list in relevant_docs.items():\n",
        "        true_positives = 0\n",
        "\n",
        "        query_idx = query_id_idx_map[query_id]\n",
        "\n",
        "        for doc in top_docs[query_idx]:\n",
        "            if doc in relevant_docs_list:\n",
        "                true_positives += 1\n",
        "\n",
        "        precision_k += true_positives / k\n",
        "\n",
        "    return precision_k / len(relevant_docs)"
      ],
      "metadata": {
        "id": "bb6tV5nK1R8h"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weigh_vectors(nlp, vocabulary, docs_vector, queries_vector):\n",
        "    print(nlp.pipeline)\n",
        "\n",
        "    for i, token in enumerate(vocabulary):\n",
        "        doc = nlp(token)\n",
        "\n",
        "        # There is only one word in doc but taking a list to make it easier to understand\n",
        "        pos = [token.pos_ for token in doc]\n",
        "        nes = doc.ents\n",
        "\n",
        "        # If part of speech is tagged as noun multiply that token's value by 2 in all the vectors\n",
        "        if pos and pos[0] == \"NOUN\":\n",
        "            docs_vector[:][i] *= 2\n",
        "            queries_vector[:][i] *= 2\n",
        "\n",
        "        # If entities are found multiply by 4\n",
        "        if nes:\n",
        "            docs_vector[:][i] *= 4\n",
        "            queries_vector[:][i] *= 4\n",
        "\n",
        "    return docs_vector, queries_vector"
      ],
      "metadata": {
        "id": "WA-30imRL9K9"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "jDvR6WfCtVT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CSV file locations need to be changed as applicable if being run locally."
      ],
      "metadata": {
        "id": "K9vjusFbu--4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_file = \"/content/Query_Doc/docs.csv\"\n",
        "queries_file = \"/content/Query_Doc/queries.csv\"\n",
        "pre_processed_docs_file = \"/content/Query_Doc/pre_processed_docs.csv\"\n",
        "pre_processed_queries_file = \"/content/Query_Doc/pre_processed_queries.csv\"\n",
        "spell_checked_docs_file = \"/content/Query_Doc/spell_checked_docs.csv\"\n",
        "spell_checked_queries_file = \"/content/Query_Doc/spell_checked_queries.csv\"\n",
        "qdrels_file = \"/content/Query_Doc/qdrel.csv\""
      ],
      "metadata": {
        "id": "JLx7wJi-uCAn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre processing"
      ],
      "metadata": {
        "id": "K9r6lSZ36o1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_process(docs_file, pre_processed_docs_file, \"doc_text\")\n",
        "pre_process(queries_file, pre_processed_queries_file, \"query_text\")"
      ],
      "metadata": {
        "id": "Skt7Avvj6sr0"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spell correction on the pre processed docs and queries"
      ],
      "metadata": {
        "id": "U9UG2gEAzrTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print logs(original and corrected for query doc only)\n",
        "correct_spellings(pre_processed_queries_file, spell_checked_queries_file, \"query_text\", True)\n",
        "correct_spellings(pre_processed_docs_file, spell_checked_docs_file, \"doc_text\")"
      ],
      "metadata": {
        "id": "VVCp3evivOyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5cb3c07-5209-47c5-fc47-d31244507569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Atal Pension Yojana What are its benefits\n",
            "What is the Pension? What are its benefits\n",
            "\n",
            "Where is starch digested How is it digested\n",
            "Where is it eaten How is it here\n",
            "\n",
            "What can India do to support the people suffering from civilian war in Syria\n",
            "What can I do to support the people suffering from civilian war in India\n",
            "\n",
            "How do introverts enjoy life\n",
            "How do they enjoy life\n",
            "\n",
            "Kindly tell me whole process of admission at vits Vellore for biotechi m a bio student in 12I dont have math there\n",
            "Kindly tell me whole process of admission at the college for i m a bio student in I donot have math there\n",
            "\n",
            "What did Theodore Roosevelt mean when he said Black care never sits behind a rider whose pace is fast enough\n",
            "What did Theodore Roosevelt mean when he said Black care never sits behind a rider whose pace is fast enough\n",
            "\n",
            "How does Quora look to a moderator\n",
            "How does one look to a man\n",
            "\n",
            "Did Tywin sleep with Shae out of pure spite\n",
            "Did you sleep with her out of pure spite\n",
            "\n",
            "Why does phase shift take place in the output of the common emitter amplifier when compared to the input signal\n",
            "Why does phase shift take place in the output of the common linear amplifier when compared to the input signal\n",
            "\n",
            "Why do people say Dhanush South Indian actor is ugly I dont think so\n",
            "Why do people say this South Indian actor is ugly I donot think so\n",
            "\n",
            "How do I reset my Gmail password when I dont remember my recovery information\n",
            "How do I get my email password when I donot remember my recovery information\n",
            "\n",
            "How can I get rid of cellulite on my stomach\n",
            "How can I get rid of it on my stomach\n",
            "\n",
            "How do I rid myself of my paranoia\n",
            "How do I rid myself of my past\n",
            "\n",
            "Who is better for India Donald Trump or Hillary Clinton Why\n",
            "Who is better for if Donald Trump or Hillary Clinton Why\n",
            "\n",
            "Will the value of Indian rupee increase after the ban of 500 and 1000 rupee notes\n",
            "Will the value of one coins increase after the ban of 500 and 1000 paper notes\n",
            "\n",
            "How can I become a billionaire in mindset\n",
            "How can I become a billionaire in that\n",
            "\n",
            "How does the Laravel authentication work\n",
            "How does the actual authentication work\n",
            "\n",
            "What if humans were nocturnal\n",
            "What if humans were what\n",
            "\n",
            "How is it to work with Satya Nadella\n",
            "How is it to work with Tony?\n",
            "\n",
            "How and where did feudalism develop\n",
            "How and where did this develop\n",
            "\n",
            "How can I observe like Sherlock Holmes\n",
            "How can I observe like Sherlock?\n",
            "\n",
            "What is the best way to memorize notes\n",
            "What is the best way to write notes\n",
            "\n",
            "Is Syrio Forel a faceless man following Arya around being different people\n",
            "Isis merely a wise man following you around being different people\n",
            "\n",
            "Will President Obama declare martial law and remain in power if Trump is elected\n",
            "Will President Obama declare martial law and remain in power if Trump is elected\n",
            "\n",
            "What are the effects of demonitization of 500 and 1000 rupees notes on real estate sector\n",
            "What are the effects of inflation of 500 and 1000 paper notes on real estate sector\n",
            "\n",
            "Is Donald Trump the 666 Antichrist\n",
            "Is Donald Trump the 666 Antichrist\n",
            "\n",
            "What is the Gülen movement\n",
            "What is the new movement\n",
            "\n",
            "What is the Best way to learn Oracle HRMS\n",
            "What is the Best way to learn from?\n",
            "\n",
            "How can I get free gems in Clash of Clans\n",
            "How can I get free time in Clash of Clans\n",
            "\n",
            "How do I get job in Google or Microsoft\n",
            "How do I get job in Google or what\n",
            "\n",
            "In the world we are second largest country in terms of population but we win only very few medals in the Olympics Why\n",
            "In the world we are second largest country in terms of population but we win only very few medals in the Why Why\n",
            "\n",
            "Who was the most powerful king in the history of India\n",
            "Who was the most powerful king in the history of?\n",
            "\n",
            "How do I use Facebook in China\n",
            "How do I use them in this\n",
            "\n",
            "How does the LinkedIn acquisition help Microsoft achieve its mission\n",
            "How does the new acquisition help Microsoft achieve its mission\n",
            "\n",
            "What is the benefit to Quora\n",
            "What is the benefit to?\n",
            "\n",
            "How do you feel when someone upvotes your answer on Quora\n",
            "How do you feel when someone gives your answer on?\n",
            "\n",
            "How do I impress the bosss boss\n",
            "How do I impress the big boss\n",
            "\n",
            "What are the best thriller movie in Hollywood\n",
            "What are the best thriller movie in?\n",
            "\n",
            "What are some interesting facts about Bengaluru\n",
            "What are some interesting facts about here\n",
            "\n",
            "Are there any other republicans here who voted for Hillary Clinton\n",
            "Are there any other Americans here who voted for Hillary and\n",
            "\n",
            "How can I find a job in Mumbai\n",
            "How can I find a job in that\n",
            "\n",
            "How do I get rid of mosquitoes bites quickly\n",
            "How do I get rid of the bites quickly\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the documents and get the tokens forming the vocabulary"
      ],
      "metadata": {
        "id": "07oTyhn7wLoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove contextualSpellCheck so that it does not run while tokenizing\n",
        "nlp.remove_pipe('contextual spellchecker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvyVyY85wOiG",
        "outputId": "00cd41e0-a0de-49c0-fe24-fc32e445e9ca"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('contextual spellchecker',\n",
              " <contextualSpellCheck.contextualSpellCheck.ContextualSpellCheck at 0x7bb7c911aef0>)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = get_vocabulary(nlp, spell_checked_docs_file, \"doc_text\")"
      ],
      "metadata": {
        "id": "ewjngCJy3Aha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "f2abd90a-1a9f-47b0-c282-2b60ba0d4fb0"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "get_vocabulary() missing 1 required positional argument: 'column_name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-818b771cc681>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspell_checked_docs_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"doc_text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: get_vocabulary() missing 1 required positional argument: 'column_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNv8dpHa4wK9",
        "outputId": "432485db-a0b3-4c86-ac60-355da153b33c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ids, docs = get_texts(nlp, spell_checked_docs_file, 'doc_id', 'doc_text')\n",
        "query_ids, queries = get_texts(nlp, spell_checked_queries_file, 'query_id', 'query_text')\n",
        "\n",
        "query_id_idx_map = {query_ids[i]: i for i in range(len(query_ids))}\n",
        "doc_id_idx_map = {doc_ids[i]: i for i in range(len(doc_ids))}"
      ],
      "metadata": {
        "id": "22C0oTTPpFwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a818ebfb-2164-44a5-f8c8-ca00a30ba91f"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7bb7c8f8f220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7bb7c8fdd070>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7bb7db97d340>)]\n",
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7bb7c8f8f220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7bb7c8fdd070>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7bb7db97d340>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs = get_relevant_docs(qdrels_file)"
      ],
      "metadata": {
        "id": "sHni6hNqwuUI"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize documents and queries, and find cosine similarities for each query with the documents"
      ],
      "metadata": {
        "id": "3F6cfubesRxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
        "\n",
        "# Create tf idf vectors for the documents and queries\n",
        "doc_vectors = vectorizer.fit_transform(docs)\n",
        "query_vectors = vectorizer.transform(queries)"
      ],
      "metadata": {
        "id": "CMXPyt65sbnc"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = cosine_similarity(query_vectors, doc_vectors)"
      ],
      "metadata": {
        "id": "zZVlHYxRsfBS"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find top 5 and top 10 similar document indices for each query\n",
        "top_5_indices = np.argsort(similarities, axis=1)[:, -5:]\n",
        "top_10_indices = np.argsort(similarities, axis=1)[:, -10:]\n",
        "\n",
        "# for each query, store the top 5 and top 10 similar document ids\n",
        "top_5_ids = [[doc_ids[i] for i in indices] for indices in top_5_indices]\n",
        "top_10_ids = [[doc_ids[i] for i in indices] for indices in top_10_indices]\n",
        "top_ids = [[i[-1]] for i in top_5_ids]"
      ],
      "metadata": {
        "id": "hqaIYiMxsn0x"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prec_1 = precision_k_score(query_id_idx_map, relevant_docs, top_ids, 1)\n",
        "prec_5 = precision_k_score(query_id_idx_map, relevant_docs, top_5_ids, 5)\n",
        "prec_10 = precision_k_score(query_id_idx_map, relevant_docs, top_10_ids, 10)\n",
        "\n",
        "print(f\"Average precision at 1: {round(prec_1 * 100, 2)} %\")\n",
        "print(f\"Average precision at 5: {round(prec_5 * 100, 2)} %\")\n",
        "print(f\"Average precision at 10: {round(prec_10 * 100, 2)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-AMHjh4y_Ad",
        "outputId": "0d114a57-b2ab-4c2d-e991-5b3f45b23acb"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average precision at 1: 46.0 %\n",
            "Average precision at 5: 15.0 %\n",
            "Average precision at 10: 8.4 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "V-fkgWra68EA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "Using nltk's porter stemmer here since spacy does not provide a stemmer"
      ],
      "metadata": {
        "id": "EuREmYXY7kHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "mvlGOo2869b6"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_vocabulary = get_stemmed_vocabulary(spell_checked_docs_file, \"doc_text\")\n",
        "stemmed_docs = get_stemmed_texts(spell_checked_docs_file, \"doc_text\", ps)\n",
        "stemmed_queries = get_stemmed_texts(spell_checked_queries_file, \"query_text\", ps)"
      ],
      "metadata": {
        "id": "t5lTj9GoF1L1"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stemmed_vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipWJcwTd929s",
        "outputId": "ed7788f1-7fa6-4cf8-c7ae-42a9420a130b"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the documents and queries, and find cosine similarities for each query with documents"
      ],
      "metadata": {
        "id": "r650FjYsPrqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_stemmed = TfidfVectorizer(vocabulary=stemmed_vocabulary)\n",
        "\n",
        "# Create tf idf vectors for the documents and queries\n",
        "stemmed_doc_vectors = vectorizer_stemmed.fit_transform(stemmed_docs)\n",
        "stemmed_query_vectors = vectorizer_stemmed.transform(stemmed_queries)"
      ],
      "metadata": {
        "id": "1GQ4bILwPqF7"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_similarities = cosine_similarity(stemmed_query_vectors, stemmed_doc_vectors)\n",
        "\n",
        "# find top 5 and top 10 similar document indices for each query\n",
        "stemmed_top_5_indices = np.argsort(stemmed_similarities, axis=1)[:, -5:]\n",
        "stemmed_top_10_indices = np.argsort(stemmed_similarities, axis=1)[:, -10:]\n",
        "\n",
        "# for each query, store the top 5 and top 10 similar document ids\n",
        "stemmed_top_5_ids = [[doc_ids[i] for i in indices] for indices in stemmed_top_5_indices]\n",
        "stemmed_top_10_ids = [[doc_ids[i] for i in indices] for indices in stemmed_top_10_indices]\n",
        "stemmed_top_ids = [[ids[-1]] for ids in stemmed_top_5_ids]"
      ],
      "metadata": {
        "id": "TLbal6o7RCkg"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision Scores"
      ],
      "metadata": {
        "id": "j20Lh7gXRF-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec_1_stemmed = precision_k_score(query_id_idx_map, relevant_docs, stemmed_top_ids, 1)\n",
        "prec_5_stemmed = precision_k_score(query_id_idx_map, relevant_docs, stemmed_top_5_ids, 5)\n",
        "prec_10_stemmed = precision_k_score(query_id_idx_map, relevant_docs, stemmed_top_10_ids, 10)\n",
        "\n",
        "print(f\"Average precision at 1 for stemmed: {round(prec_1_stemmed * 100, 2)} %\")\n",
        "print(f\"Average precision at 5 for stemmed: {round(prec_5_stemmed * 100, 2)} %\")\n",
        "print(f\"Average precision at 10 for stemmed: {round(prec_10_stemmed * 100, 2)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MRJ7DFAREdt",
        "outputId": "44918c51-d257-46c5-ef27-491d7447d96a"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average precision at 1 for stemmed: 57.0 %\n",
            "Average precision at 5 for stemmed: 18.2 %\n",
            "Average precision at 10 for stemmed: 10.1 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "Add lemmatizer back to the nlp pipe"
      ],
      "metadata": {
        "id": "J-R7czJ5RPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe('lemmatizer')\n",
        "nlp.initialize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdIsBYvlRT9i",
        "outputId": "2db39d68-aea1-48b2-b4a6-d4c64b348bab"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<thinc.optimizers.Optimizer at 0x7bb7db751940>"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_vocabulary = get_lemmatized_vocabulary(nlp, spell_checked_docs_file, \"doc_text\")\n",
        "lemmatized_docs = get_lemmatized_text(nlp, spell_checked_docs_file, \"doc_text\")\n",
        "lemmatized_queries = get_lemmatized_text(nlp, spell_checked_queries_file, \"query_text\")\n",
        "\n",
        "print(len(lemmatized_vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJNY6i8hfrvO",
        "outputId": "bbd49f38-4372-4556-9cac-b29b820a0088"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7bb7c8f8f220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7bb7c8fdd070>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7bb7db97d340>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7bb7e2b5c0c0>)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/attributeruler.py:149: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
            "  matches = self.matcher(doc, allow_missing=True, as_spans=False)\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7bb7c8f8f220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7bb7c8fdd070>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7bb7db97d340>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7bb7e2b5c0c0>)]\n",
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7bb7c8f8f220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7bb7c8fdd070>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7bb7db97d340>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7bb7e2b5c0c0>)]\n",
            "2030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(lemmatized_vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s06RrcU9JSUE",
        "outputId": "fa3edd9d-a2b9-49a7-ee2a-83ef643b53ea"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the documents and queries, and find cosine similarities for each query with documents"
      ],
      "metadata": {
        "id": "A7aNAx1Q0ey2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_lemmatized = TfidfVectorizer(vocabulary=lemmatized_vocabulary)\n",
        "\n",
        "# Create tf idf vectors for the documents and queries\n",
        "lemmatized_doc_vectors = vectorizer_lemmatized.fit_transform(lemmatized_docs)\n",
        "lemmatized_query_vectors = vectorizer_lemmatized.transform(lemmatized_queries)"
      ],
      "metadata": {
        "id": "UloKwDAM0cnZ"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_similarities = cosine_similarity(lemmatized_query_vectors, lemmatized_doc_vectors)\n",
        "\n",
        "# find top 5 and top 10 similar document indices for each query\n",
        "lemmatized_top_5_indices = np.argsort(lemmatized_similarities, axis=1)[:, -5:]\n",
        "lemmatized_top_10_indices = np.argsort(lemmatized_similarities, axis=1)[:, -10:]\n",
        "\n",
        "# for each query, store the top 5 and top 10 similar document ids\n",
        "lemmatized_top_5_ids = [[doc_ids[i] for i in indices] for indices in lemmatized_top_5_indices]\n",
        "lemmatized_top_10_ids = [[doc_ids[i] for i in indices] for indices in lemmatized_top_10_indices]\n",
        "lemmatized_top_ids = [[ids[-1]] for ids in lemmatized_top_5_ids]"
      ],
      "metadata": {
        "id": "lCCqjB2U5oob"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision Scores"
      ],
      "metadata": {
        "id": "dQW0K_246uoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec_1_lemmatized = precision_k_score(query_id_idx_map, relevant_docs, lemmatized_top_ids, 1)\n",
        "prec_5_lemmatized = precision_k_score(query_id_idx_map, relevant_docs, lemmatized_top_5_ids, 5)\n",
        "prec_10_lemmatized = precision_k_score(query_id_idx_map, relevant_docs, lemmatized_top_10_ids, 10)\n",
        "\n",
        "print(f\"Average precision at 1 for lemmatized: {round(prec_1_lemmatized * 100, 2)} %\")\n",
        "print(f\"Average precision at 5 for lemmatized: {round(prec_5_lemmatized * 100, 2)} %\")\n",
        "print(f\"Average precision at 10 for lemmatized: {round(prec_10_lemmatized * 100, 2)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phtSphOz6vF1",
        "outputId": "0e94828a-6e15-4e32-c3b7-99c8ce299376"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average precision at 1 for lemmatized: 46.0 %\n",
            "Average precision at 5 for lemmatized: 15.0 %\n",
            "Average precision at 10 for lemmatized: 8.4 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER and POS tagging"
      ],
      "metadata": {
        "id": "ds60X-63JQFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe('ner')\n",
        "nlp.add_pipe('tagger', source=spacy.load(\"en_core_web_sm\"))\n",
        "nlp.initialize()"
      ],
      "metadata": {
        "id": "MuOfKxF75skx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457da933-0163-4c92-f47f-864e74a2d433"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<thinc.optimizers.Optimizer at 0x7bb7e29de7a0>"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pos_vocabulary = get_vocabulary(nlp, spell_checked_docs_file, \"doc_text\")\n",
        "ner_pos_docs = get_processed_text(nlp, spell_checked_docs_file, \"doc_text\")\n",
        "ner_pos_queries = get_processed_text(nlp, spell_checked_queries_file, \"query_text\")"
      ],
      "metadata": {
        "id": "1Pchh2ME5yrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41400ede-ef1d-438e-ead6-3188ff4fdfbb"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7bb7c8f8f220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7bb7c8fdd070>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7bb7db97d340>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7bb7e2b5c0c0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7bb7e2044040>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7bb7da75d600>)]\n",
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7bb7c8f8f220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7bb7c8fdd070>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7bb7db97d340>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7bb7e2b5c0c0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7bb7e2044040>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7bb7da75d600>)]\n",
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7bb7c8f8f220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7bb7c8fdd070>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7bb7db97d340>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7bb7e2b5c0c0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7bb7e2044040>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7bb7da75d600>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ner_pos_vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GAvI7k4JQos",
        "outputId": "e2ff12d2-25a8-47c1-e02f-463d5afcf564"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_ner_pos = TfidfVectorizer(vocabulary=ner_pos_vocabulary)\n",
        "\n",
        "# Create tf idf vectors for the documents and queries\n",
        "ner_pos_doc_vectors = vectorizer_ner_pos.fit_transform(ner_pos_docs)\n",
        "ner_pos_query_vectors = vectorizer_ner_pos.transform(ner_pos_queries)"
      ],
      "metadata": {
        "id": "JDKstDCwLNt2"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pos_doc_vectors, ner_pos_query_vectors = weigh_vectors(nlp, ner_pos_vocabulary, ner_pos_doc_vectors, ner_pos_query_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FlZwsDCLw5_",
        "outputId": "03399b11-1b51-4493-8d31-704d1b2b3649"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7bb7c8f8f220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7bb7c8fdd070>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7bb7db97d340>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7bb7e2b5c0c0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7bb7e2044040>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7bb7da75d600>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pos_similarities = cosine_similarity(ner_pos_query_vectors, ner_pos_doc_vectors)\n",
        "\n",
        "# find top 5 and top 10 similar document indices for each query\n",
        "ner_pos_top_5_indices = np.argsort(ner_pos_similarities, axis=1)[:, -5:]\n",
        "ner_pos_top_10_indices = np.argsort(ner_pos_similarities, axis=1)[:, -10:]\n",
        "\n",
        "# for each query, store the top 5 and top 10 similar document ids\n",
        "ner_pos_top_5_ids = [[doc_ids[i] for i in indices] for indices in ner_pos_top_5_indices]\n",
        "ner_pos_top_10_ids = [[doc_ids[i] for i in indices] for indices in ner_pos_top_10_indices]\n",
        "ner_pos_top_ids = [[ids[-1]] for ids in ner_pos_top_5_ids]"
      ],
      "metadata": {
        "id": "skMWJFL0Pa87"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prec_1_ner_pos = precision_k_score(query_id_idx_map, relevant_docs, ner_pos_top_ids, 1)\n",
        "prec_5_ner_pos = precision_k_score(query_id_idx_map, relevant_docs, ner_pos_top_5_ids, 5)\n",
        "prec_10_ner_pos = precision_k_score(query_id_idx_map, relevant_docs, ner_pos_top_10_ids, 10)\n",
        "\n",
        "print(f\"Average precision at 1 for ner and pos: {round(prec_1_ner_pos * 100, 2)} %\")\n",
        "print(f\"Average precision at 5 for ner and pos: {round(prec_5_ner_pos * 100, 2)} %\")\n",
        "print(f\"Average precision at 10 for ner and pos: {round(prec_10_ner_pos * 100, 2)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZlYCy0oP40N",
        "outputId": "8d2843df-15f8-440c-d0e2-6d1501dd1eca"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average precision at 1 for ner and pos: 46.0 %\n",
            "Average precision at 5 for ner and pos: 15.0 %\n",
            "Average precision at 10 for ner and pos: 8.4 %\n"
          ]
        }
      ]
    }
  ]
}