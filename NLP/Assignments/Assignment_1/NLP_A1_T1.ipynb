{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r-rTccDuFr8"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64yWLxtXoOrG"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install contextualSpellCheck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO4VRNdHoxSD"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt8Cj2NMpmTA"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import contextualSpellCheck\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opEEAC1UqVO-"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h34NMkubqXbo"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "contextualSpellCheck.add_to_pipe(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk6ujq08qMEZ"
      },
      "source": [
        "## Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9L5_aGpprWg"
      },
      "outputs": [],
      "source": [
        "def spell_check(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    return doc._.performed_spellCheck, doc._.outcome_spellCheck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJlxZytPw3oK"
      },
      "outputs": [],
      "source": [
        "def correct_spellings(csv_file, out_csv_file, column_name, should_log=False):\n",
        "    # read a csv file, in each line, see if spellCheck was performed(if text was correctly spelled) and print original and corrected texts\n",
        "    lines = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        # Index of the column to be spell checked\n",
        "        column_index = lines[0].index(column_name)\n",
        "\n",
        "        # For each line, perform spell check\n",
        "        for i in range(1, len(lines)):\n",
        "            line = lines[i][column_index]\n",
        "            performed, outcome = spell_check(line)\n",
        "\n",
        "            # If spell check was performed, update the line and print the original and corrected texts\n",
        "            if should_log:\n",
        "                print(line)\n",
        "                print(outcome)\n",
        "                print()\n",
        "\n",
        "            if performed:\n",
        "                # Update the line\n",
        "                lines[i][column_index] = outcome\n",
        "\n",
        "    with open(out_csv_file, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(lines)\n",
        "\n",
        "    print(\"Spell check completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXrFFBtPsQIV"
      },
      "outputs": [],
      "source": [
        "def pre_process(csv_file, out_csv_file, column_name):\n",
        "    # read a csv file, in each line, remove characters apart from alphanumerics and whitespaces in the given column\n",
        "    lines = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = list(reader)\n",
        "\n",
        "        # Index of the column to be pre processed\n",
        "        column_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            lines[i][column_index] = ''.join([c for c in lines[i][column_index] if c.isalnum() or c.isspace()])\n",
        "\n",
        "    # write the pre-processed data to the resulting csv file\n",
        "    with open(out_csv_file, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(lines)\n",
        "\n",
        "    print(\"Pre procesing completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMSjXyLexJ9l"
      },
      "outputs": [],
      "source": [
        "def get_vocabulary(csv_file, column_name):\n",
        "    # Read the spell checked docs csv file and store the documents in a list\n",
        "    docs = []\n",
        "\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lines = [line for line in reader]\n",
        "\n",
        "        doc_col_index = lines[0].index(column_name)\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            docs.append(lines[i][doc_col_index])\n",
        "\n",
        "        num_docs = len(docs)\n",
        "        lower_lim = 5\n",
        "        upper_lim = int(num_docs * 0.85)\n",
        "\n",
        "        # tokenize the documents using spacy and store the tokens in a vocabulary,\n",
        "        # remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "\n",
        "        doc_tokenized = []\n",
        "\n",
        "        # use nlp pipe to tokenize the documents\n",
        "        # docs = nlp.pipe(docs, batch_size=10000, n_process=-1)\n",
        "        for doc in docs:\n",
        "            # process doc using nlp\n",
        "            doc = nlp(doc)\n",
        "\n",
        "            # Select alphabetical tokens and remove stop words\n",
        "            tokens = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "            doc_tokenized.append(tokens)\n",
        "\n",
        "        # Now that we have the tokenized documents, we can create a vocabulary\n",
        "        vocabulary = {}\n",
        "\n",
        "        for doc in doc_tokenized:\n",
        "            for token in doc:\n",
        "                if token in vocabulary:\n",
        "                    vocabulary[token] += 1\n",
        "                else:\n",
        "                    vocabulary[token] = 1\n",
        "\n",
        "        # Remove the tokens that occur less than lower_lim times and more than upper_lim times\n",
        "        vocabulary = [k for k, v in vocabulary.items() if v >= lower_lim and v <= upper_lim]\n",
        "\n",
        "        return vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDvR6WfCtVT8"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9vjusFbu--4"
      },
      "source": [
        "The CSV file locations need to be changed as applicable if being run locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLx7wJi-uCAn"
      },
      "outputs": [],
      "source": [
        "docs_file = \"/content/Query_Doc/docs.csv\"\n",
        "queries_file = \"/content/Query_Doc/queries.csv\"\n",
        "pre_processed_docs_file = \"/content/Query_Doc/pre_processed_docs.csv\"\n",
        "pre_processed_queries_file = \"/content/Query_Doc/pre_processed_queries.csv\"\n",
        "spell_checked_docs_file = \"/content/Query_Doc/spell_checked_docs.csv\"\n",
        "spell_checked_queries_file = \"/content/Query_Doc/spell_checked_queries.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9r6lSZ36o1v"
      },
      "source": [
        "Pre processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skt7Avvj6sr0"
      },
      "outputs": [],
      "source": [
        "pre_process(docs_file, pre_processed_docs_file, \"doc_text\")\n",
        "pre_process(queries_file, pre_processed_queries_file, \"query_text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9UG2gEAzrTb"
      },
      "source": [
        "Spell correction on the pre processed docs and queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVCp3evivOyQ"
      },
      "outputs": [],
      "source": [
        "# Print logs(original and corrected for query doc only)\n",
        "correct_spellings(pre_processed_queries_file, spell_checked_queries_file, \"query_text\", True)\n",
        "correct_spellings(pre_processed_docs_file, spell_checked_docs_file, \"doc_text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07oTyhn7wLoR"
      },
      "source": [
        "Tokenize the documents and get the tokens forming the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvyVyY85wOiG",
        "outputId": "8fb356ba-086f-43cf-ef52-4fefd3e328cb"
      },
      "outputs": [],
      "source": [
        "# Remove contextualSpellCheck, lemmatizer, ner and tagger from the nlp pipe since we are checking performance after only spell check\n",
        "nlp.remove_pipe('contextual spellchecker')\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('lemmatizer')\n",
        "nlp.remove_pipe('ner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewjngCJy3Aha"
      },
      "outputs": [],
      "source": [
        "vocabulary = get_vocabulary(spell_checked_docs_file, \"doc_text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNv8dpHa4wK9",
        "outputId": "84efe6b6-a2a6-4c04-bf39-b6792a21069e"
      },
      "outputs": [],
      "source": [
        "print(vocabulary)\n",
        "print(len(vocabulary))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
