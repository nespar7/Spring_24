{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tagger():\n",
    "    def __init__(self):\n",
    "        self.emissions = defaultdict(lambda: defaultdict(int))\n",
    "        self.transitions = defaultdict(lambda: defaultdict(int))\n",
    "        self.tag_counts = defaultdict(int)\n",
    "        self.tags = []\n",
    "        self.vocab = []\n",
    "        self.V = 0\n",
    "\n",
    "    def calculate_emission_probs(self, words):\n",
    "        # Use add one smoothing for the emission probabilities\n",
    "        emission_probs = defaultdict(lambda: defaultdict(float))\n",
    "        smoothed_words = []\n",
    "\n",
    "        for tag in self.tags:\n",
    "            for word in words:\n",
    "                emission_probs[tag][word] = (self.emissions[tag][word] + 1) / (self.tag_counts[tag] + self.V)\n",
    "                if not word in self.vocab:\n",
    "                    smoothed_words.append(word)\n",
    "\n",
    "        return emission_probs, smoothed_words\n",
    "\n",
    "    def calculate_transitions(self):\n",
    "        # For each tag in tags, calculate the transition probabilities\n",
    "        for prev_tag, tags in self.transitions.items():\n",
    "            total = sum(tags.values())\n",
    "            for tag, count in tags.items():\n",
    "                self.transitions[prev_tag][tag] = count / total\n",
    "        \n",
    "\n",
    "    def viterbi(self, words):\n",
    "        T = len(self.tags)\n",
    "        W = len(words)\n",
    "\n",
    "        score = np.zeros((T, W))\n",
    "        backPtr = np.zeros((T, W), dtype=int)\n",
    "\n",
    "        emission_probs, smoothed_words = self.calculate_emission_probs(words)\n",
    "\n",
    "        # Initialize the first column\n",
    "        for i, tag in enumerate(self.tags):\n",
    "            score[i, 0] = self.transitions['LINESTART'][tag] * emission_probs[tag][words[0]]\n",
    "            backPtr[i, 0] = 0\n",
    "\n",
    "        # Iterate over the rest of the columns\n",
    "        for i in range(1, W):\n",
    "            for j, tag in enumerate(self.tags):\n",
    "                max_score = 0\n",
    "                max_index = 0\n",
    "                for k, prev_tag in enumerate(self.tags):\n",
    "                    s = score[k, i-1] * self.transitions[prev_tag][tag] * emission_probs[tag][words[i]]\n",
    "                    if s > max_score:\n",
    "                        max_score = s\n",
    "                        max_index = k\n",
    "                score[j, i] = max_score\n",
    "                backPtr[j, i] = max_index\n",
    "\n",
    "        # Find the best path\n",
    "        best_path = [np.argmax(score[:, -1])]\n",
    "\n",
    "        for i in range(W-1, 0, -1):\n",
    "            best_path.insert(0, backPtr[best_path[0], i])\n",
    "\n",
    "        return [self.tags[i] for i in best_path], smoothed_words\n",
    "\n",
    "    def train(self, filename):\n",
    "        prev_tag = 'LINESTART'\n",
    "\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                elif line.strip().startswith('# sent_id'):\n",
    "                    prev_tag = 'LINESTART'\n",
    "                    continue\n",
    "                elif line.strip().startswith('#'):\n",
    "                    continue\n",
    "\n",
    "                parts = line.split()\n",
    "                word, tag = parts[1], parts[3]\n",
    "\n",
    "                self.emissions[tag][word] += 1\n",
    "                self.transitions[prev_tag][tag] += 1\n",
    "\n",
    "                if tag not in self.tags:\n",
    "                    self.tags.append(tag)\n",
    "\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.append(word)\n",
    "\n",
    "                self.tag_counts[tag] += 1\n",
    "\n",
    "                prev_tag = tag\n",
    "\n",
    "        self.V = len(self.vocab)\n",
    "\n",
    "        # calculate transition probabilities\n",
    "        self.calculate_transitions()\n",
    "\n",
    "    def predict(self, filename, output_filename):\n",
    "        print('', file=open(output_filename, 'w'), end='')\n",
    "\n",
    "        data_tags = []\n",
    "        predicted_tags = []\n",
    "        smoothed = []\n",
    "\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            sent_words = []\n",
    "            sent_tags = []\n",
    "            sent_id = None\n",
    "\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                elif line.strip().startswith('# sent_id'):\n",
    "                    # if sent_words is empty, skip\n",
    "                    if not sent_words:\n",
    "                        sent_id = line.split()[3]\n",
    "                        continue\n",
    "\n",
    "                    sent_preds, sent_smoothed = self.viterbi(sent_words)\n",
    "\n",
    "                    for i, pred in enumerate(sent_preds):\n",
    "                        print(f'{sent_id}\\t{i+1}\\t{sent_words[i]}\\t{pred}\\n', file=open(output_filename, 'a', encoding='utf-8'))\n",
    "\n",
    "                    # add the sent tags to the data tags list\n",
    "                    data_tags.extend(sent_tags)\n",
    "\n",
    "                    # add the sentence predictions to the predicted tags list\n",
    "                    predicted_tags.extend(sent_preds)\n",
    "\n",
    "                    # clear the sentence words and tags\n",
    "                    sent_words = []\n",
    "                    sent_tags = []\n",
    "\n",
    "                    # add the smoothed words to the total smoothed words list\n",
    "                    smoothed.extend(sent_smoothed)\n",
    "\n",
    "                    sent_id = line.split()[3]\n",
    "                    continue\n",
    "                elif line.strip().startswith('#'):\n",
    "                    continue\n",
    "\n",
    "                parts = line.split()\n",
    "\n",
    "                word, tag = parts[1], parts[3]\n",
    "\n",
    "                sent_words.append(word)\n",
    "                sent_tags.append(tag)\n",
    "\n",
    "            # if sent_words is not empty, predict the tags\n",
    "            if sent_words:\n",
    "                sent_preds, sent_smoothed = self.viterbi(sent_words)\n",
    "\n",
    "                for i, pred in enumerate(sent_preds):\n",
    "                    print(f'{sent_id}\\t{i+1}\\t{sent_words[i]}\\t{pred}', file=open(output_filename, 'a', encoding='utf-8'))\n",
    "\n",
    "                # add the sent tags to the data tags list\n",
    "                data_tags.extend(sent_tags)\n",
    "\n",
    "                # add the sentence predictions to the predicted tags list\n",
    "                predicted_tags.extend(sent_preds)\n",
    "\n",
    "                # add the smoothed words to the total smoothed words list\n",
    "                smoothed.extend(sent_smoothed)\n",
    "        \n",
    "        smoothed = set(smoothed)\n",
    "        smoothed = len(smoothed)\n",
    "\n",
    "        return data_tags, predicted_tags, smoothed\n",
    "    \n",
    "    def get_scores(self, data_tags, predicted_tags):\n",
    "        accuracy = accuracy_score(data_tags, predicted_tags)\n",
    "        recall = recall_score(data_tags, predicted_tags, average='weighted', zero_division=0)\n",
    "        precision = precision_score(data_tags, predicted_tags, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(data_tags, predicted_tags, average='weighted', zero_division=0)\n",
    "\n",
    "        return accuracy, recall, precision, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./train.txt\"\n",
    "test_file = \"./test.txt\"\n",
    "train_output_file = \"./viterbi_train_predictions.tsv\"\n",
    "test_output_file = \"./viterbi_test_predictions.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "customTagger = tagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "customTagger.train(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tags, predicted_tags, smoothed = customTagger.predict(train_file, train_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train file:  ./train.txt\n",
      "Accuracy: 0.8310089344083679\n",
      "Recall: 0.8310089344083679\n",
      "Precision: 0.8359764794990692\n",
      "F1: 0.821836977061188\n",
      "Smoothed words: 0\n"
     ]
    }
   ],
   "source": [
    "accuracy, recall, precision, f1 = customTagger.get_scores(data_tags, predicted_tags)\n",
    "\n",
    "print(\"For train file: \", train_file)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"Smoothed words: {smoothed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tags, predicted_tags, smoothed = customTagger.predict(test_file, test_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test file:  ./test.txt\n",
      "Accuracy: 0.701007326007326\n",
      "Recall: 0.701007326007326\n",
      "Precision: 0.731895558358753\n",
      "F1: 0.676887398811972\n",
      "Smoothed words: 284\n"
     ]
    }
   ],
   "source": [
    "accuracy, recall, precision, f1 = customTagger.get_scores(data_tags, predicted_tags)\n",
    "\n",
    "print(\"For test file: \", test_file)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"Smoothed words: {smoothed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
