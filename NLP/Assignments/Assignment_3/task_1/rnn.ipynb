{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm9NiIz_ysce"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0dop66Ayscg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZr8t17Gy3dV",
        "outputId": "3f950e53-39b7-4e0d-8059-b24988e73449"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_iGYPaOysci",
        "outputId": "43ae70e5-8134-49c0-e234-d4badb8fe62e"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3IZqE9nysci"
      },
      "outputs": [],
      "source": [
        "stopwords = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWj8HNKgyscj"
      },
      "source": [
        "# Classes and Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxnIlvduyscj"
      },
      "outputs": [],
      "source": [
        "hyperlink_regex = re.compile(r'&lt;A.*?&gt;.*?&lt;/A&gt;')\n",
        "font_regex = re.compile(r'&lt;font.*?/font&gt;')\n",
        "img_regex = re.compile(r'&lt;img.*?&gt;')\n",
        "nobr_regex = re.compile(r'&lt;nobr&gt;.*?&lt;/nobr&gt;')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N7f-gseysck"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stopwords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wi0y94zysck"
      },
      "outputs": [],
      "source": [
        "def read_dataset(filename, rmst=False):\n",
        "    with open(filename, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        data = list(reader)\n",
        "\n",
        "        # remove header\n",
        "        data = data[1:]\n",
        "\n",
        "        # remove the first column\n",
        "        data = [row[1:] for row in data]\n",
        "\n",
        "    data = preprocess_data(data, rmst)\n",
        "\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data, rmst=False):\n",
        "    for i, row in enumerate(data):\n",
        "        text = row[0]\n",
        "\n",
        "        # Change text to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Replace occurences of '#36;', \"\\$\" or '#151;' or '#160;' with a space\n",
        "        text = text.replace(\"#36;\", \" \")\n",
        "        text = text.replace(\"\\$ \", \" \")\n",
        "        text = text.replace(\"#151;\", \" \")\n",
        "        text = text.replace(\"#160;\", \" \")\n",
        "        text = text.replace(\"\\\\\",\" \")\n",
        "\n",
        "        # Replace '#39;' or '#8217;' with an apostrophe\n",
        "        text = text.replace(\"#39;\", \"\")\n",
        "        text = text.replace(\"#8217;\", \"\")\n",
        "\n",
        "        # Replace '#147;' or '#148;' with a double quote\n",
        "        text = text.replace(\"#147;\", '')\n",
        "        text = text.replace(\"#148;\", '')\n",
        "        text = text.replace(\"quot;\", '')\n",
        "\n",
        "        # Remove occurences of some tags like bold, strong, etc.\n",
        "        text = text.replace('&lt;br&gt;', ' ')\n",
        "        text = text.replace('&lt;br/&gt;', ' ')\n",
        "        text = text.replace('&lt;b&gt;...&lt;/b&gt;', ' ')\n",
        "        text = text.replace('&lt;strong&gt;', ' ')\n",
        "        text = text.replace('&lt;/strong&gt;', ' ')\n",
        "        text = text.replace('&lt;cite&gt;is&lt;/cite&gt;', ' ')\n",
        "        text = text.replace('&lt;p&gt;', ' ')\n",
        "        text = text.replace('&lt;/p&gt;', ' ')\n",
        "        text = text.replace('\\&lt;p&gt;', ' ')\n",
        "        text = text.replace('\\&lt;hpq.n&gt;', ' ')\n",
        "        text = text.replace('&lt;i&gt;', ' ')\n",
        "        text = text.replace('&lt;/i&gt;', ' ')\n",
        "\n",
        "        # Match and remove the regexes defined above\n",
        "        text = hyperlink_regex.sub('', text)\n",
        "        text = font_regex.sub('', text)\n",
        "        text = img_regex.sub('', text)\n",
        "        text = nobr_regex.sub('', text)\n",
        "\n",
        "        # Remove \\ in the text\n",
        "        # First replace any possible escape characters like \\', \\n, \\r, \\t, \\b, \\f, \\\" with appropriate characters\n",
        "        text = text.replace(\"\\'\", \"\")\n",
        "        text = text.replace(\"\\\"\", \"\")\n",
        "        text = text.replace(\"\\n\", \" n\")\n",
        "        text = text.replace(\"\\r\", \" r\")\n",
        "        text = text.replace(\"\\t\", \" t\")\n",
        "        text = text.replace(\"\\b\", \" b\")\n",
        "        text = text.replace(\"\\f\", \" f\")\n",
        "\n",
        "        # Remove any remaining \\ in the text\n",
        "        text = text.replace(\"\\ \".replace(\" \", \"\"), \" \")\n",
        "\n",
        "        text = ''.join([c if c.isalnum() or c.isspace() else ' ' for c in text])\n",
        "\n",
        "        if rmst:\n",
        "            text = remove_stopwords(text)\n",
        "\n",
        "        row[0] = text\n",
        "\n",
        "        # # Converting the label to one-hot encoding\n",
        "        # label = np.zeros(4)\n",
        "        # label[int(row[1])] = 1\n",
        "        # row[1] = label\n",
        "\n",
        "        data[i] = row\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3hC_HWJyscl"
      },
      "outputs": [],
      "source": [
        "def create_vocab(dataset):\n",
        "    sentences = [row[0] for row in dataset]\n",
        "\n",
        "    # If sentences is not a 2D list, convert it to one\n",
        "    if not isinstance(sentences[0], list):\n",
        "        tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    else:\n",
        "        tokens = sentences\n",
        "\n",
        "    vocab = {}\n",
        "\n",
        "    for token_list in tokens:\n",
        "        for token in token_list:\n",
        "            if token in vocab:\n",
        "                vocab[token] += 1\n",
        "            else:\n",
        "                vocab[token] = 1\n",
        "\n",
        "    vocab = [k for k, v in vocab.items()]\n",
        "\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for token_list in tokens:\n",
        "        tokenized_sentences.append([token for token in token_list if token in vocab])\n",
        "\n",
        "    return vocab, tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r5Y00xUyscm"
      },
      "outputs": [],
      "source": [
        "def average_embeddings(sentence, embeddings, vector_size):\n",
        "    avg = np.zeros(vector_size)\n",
        "    count = 0\n",
        "\n",
        "    for word in sentence:\n",
        "        if word in embeddings:\n",
        "            avg += embeddings[word]\n",
        "            count += 1\n",
        "\n",
        "    if count > 0:\n",
        "        avg /= count\n",
        "    else:\n",
        "        avg = np.zeros(vector_size)\n",
        "\n",
        "    return np.array(avg, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kyZVEl35KqO"
      },
      "outputs": [],
      "source": [
        "# Data class\n",
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y).type(torch.LongTensor)\n",
        "\n",
        "        self.len = self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PecVGxhyscm"
      },
      "outputs": [],
      "source": [
        "# Neural Network class\n",
        "class classifier_nn(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
        "        super(classifier_nn, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        prev_layer_size = input_size\n",
        "        for hidden_size in hidden_sizes:\n",
        "            self.layers.append(nn.Linear(prev_layer_size, hidden_size))\n",
        "            # self.layers.append(nn.ReLU())\n",
        "            prev_layer_size = hidden_size\n",
        "\n",
        "        self.output_layer = nn.Linear(prev_layer_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            x = torch.sigmoid(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "def train_nn(model, train_loader, epochs, lr):\n",
        "    model.train()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        if epoch%10 == 0:\n",
        "            print(f'Epoch {epoch}: Loss: {epoch_loss}')\n",
        "\n",
        "def get_predictions(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # _, labels = torch.max(labels, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return all_preds, all_labels\n",
        "\n",
        "def get_perf(preds, labels):\n",
        "    accuracy = np.mean(np.array(preds) == np.array(labels))\n",
        "\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "\n",
        "    return accuracy, f1\n",
        "\n",
        "def plot_conf_matrix(preds, labels):\n",
        "    conf_matrix = confusion_matrix(labels, preds)\n",
        "    print(conf_matrix)\n",
        "\n",
        "def save_predictions(preds, og_filename, filename):\n",
        "    with open(og_filename, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        data = list(reader)\n",
        "        header = data[0]\n",
        "\n",
        "        lines = [row[1] for row in data[1:]]\n",
        "    print(\"index,text,label\", file=open(filename, \"w\"))\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        print(f\"{i},{line},{preds[i]}\", file=open(filename, \"a\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codes for RNN part\n",
        "def get_embedding_vectors(sentences, vector_size=100, max_seq_len=50):\n",
        "    sentences = [sentence.split() for sentence in sentences]\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if len(sentence) > max_seq_len:\n",
        "            sentence = sentence[:max_seq_len]\n",
        "        sentences[i] = ' '.join(sentence)\n",
        "\n",
        "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    model = Word2Vec(tokenized_sentences, vector_size=vector_size, window=5, min_count=1, workers=4)\n",
        "\n",
        "    embedded_vectors = []\n",
        "    \n",
        "    # Create vocabulary from the model\n",
        "    vocab = model.wv\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        embedded_vectors.append(average_embeddings(sentence, model.wv, vector_size))\n",
        "\n",
        "    return np.array(embedded_vectors), vocab\n",
        "\n",
        "def collate_fn(batch, max_sequence):\n",
        "    sentences, labels = zip(*batch)\n",
        "\n",
        "    padded_sentences = pad_sequence(sentences, batch_first=True)\n",
        "\n",
        "    return padded_sentences, torch.tensor(labels)\n",
        "\n",
        "def prepare_data(embedded_vectors, labels, batch_size):\n",
        "    dataset = Data(embedded_vectors, labels)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    return data_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class Classifier_RNN(nn.Module):\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes, bidirectional=True):\n",
        "#         super(Classifier_RNN, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "#         self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "#         self.fc = nn.Linear(hidden_size*2 if bidirectional else hidden_size, num_classes)\n",
        "#         self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x, _ = self.rnn(x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = torch.mean(x, 1)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# class Classifier_LSTM(nn.Module):\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes, bidirectional=True):\n",
        "#         super(Classifier_LSTM, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "#         self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "#         self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, num_classes)\n",
        "#         self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x, _ = self.lstm(x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = torch.mean(x, 1)\n",
        "#         x = self.fc(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQUzqjjLyscm"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-zWqXgoyscn"
      },
      "source": [
        "## Data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozkltUvPyscn"
      },
      "outputs": [],
      "source": [
        "train_file = './train.csv'\n",
        "test_file = './test.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVrBY8p4yscn"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tKxeJ8kyscn"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36X8Hz7uyscn"
      },
      "outputs": [],
      "source": [
        "train_data_task_1 = read_dataset(train_file, rmst=True)\n",
        "test_data_task_1 = read_dataset(test_file, rmst=True)\n",
        "\n",
        "# Now we set a 10% of the training data aside for validation\n",
        "train_data_task_1, val_data_task_1 = train_test_split(train_data_task_1, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37oUzUfSysco"
      },
      "outputs": [],
      "source": [
        "vocabulary_1, tokenized_sentences = create_vocab(train_data_task_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s38gq_Wysco"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZsW5cF2ysco"
      },
      "outputs": [],
      "source": [
        "vec_size = 100\n",
        "output_dim = 4\n",
        "epoch_sizes = [10, 20, 40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odCRagLbysco"
      },
      "outputs": [],
      "source": [
        "# Create the word2vec model using the train_data as the corpus\n",
        "word2vec_model = Word2Vec(tokenized_sentences, vector_size=vec_size, window=5, min_count=1, workers=4)\n",
        "word2vec_model.save('word2vec_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gosko8SUyscp"
      },
      "outputs": [],
      "source": [
        "# Get embeddings for the vocabulary\n",
        "embeddings = {}\n",
        "\n",
        "for word in vocabulary_1:\n",
        "    embeddings[word] = word2vec_model.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nubDvgcxyscp"
      },
      "outputs": [],
      "source": [
        "# Split train data into X, y\n",
        "X_train = [row[0] for row in train_data_task_1]\n",
        "y_train = [row[1] for row in train_data_task_1]\n",
        "\n",
        "X_train = [average_embeddings(word_tokenize(sentence), embeddings, vec_size) for sentence in X_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzl6eGF9yscp"
      },
      "outputs": [],
      "source": [
        "# Split validation data into X, y\n",
        "X_val = [row[0] for row in val_data_task_1]\n",
        "y_val = [row[1] for row in val_data_task_1]\n",
        "\n",
        "X_val = [average_embeddings(word_tokenize(sentence), embeddings, vec_size) for sentence in X_val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKwxlSq0yscp"
      },
      "outputs": [],
      "source": [
        "# Split test data into X, y\n",
        "X_test = [row[0] for row in test_data_task_1]\n",
        "y_test = [row[1] for row in test_data_task_1]\n",
        "\n",
        "X_test = [average_embeddings(word_tokenize(sentence), embeddings, vec_size) for sentence in X_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPhbGTqEyscp"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVWFtDon07uQ"
      },
      "outputs": [],
      "source": [
        "# load train_data into DataLoader\n",
        "train_data = Data(np.array(X_train), np.array(y_train))\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eNLE0Hz8ELi"
      },
      "outputs": [],
      "source": [
        "# load val_data into DataLoader\n",
        "val_data = Data(np.array(X_val), np.array(y_val))\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIC_La5Y8FQc"
      },
      "outputs": [],
      "source": [
        "# load test_data into DataLoader\n",
        "test_data = Data(np.array(X_test), np.array(y_test))\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpancjtQyscq"
      },
      "source": [
        "Model 1:\n",
        "* 1 hidden layer - 64 size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD-uANWfyscq",
        "outputId": "2b35e648-eac7-430f-d7b1-c09265475703"
      },
      "outputs": [],
      "source": [
        "# Create the neural network\n",
        "input_size = vec_size\n",
        "hidden_sizes = [64]\n",
        "\n",
        "model_1 = classifier_nn(input_size, hidden_sizes, output_dim).to(device)\n",
        "\n",
        "train_nn(model_1, train_loader, 100, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yRdnOUh2qbV",
        "outputId": "cb47717e-935e-4137-a33d-459455ea825d"
      },
      "outputs": [],
      "source": [
        "# Get accuracy on validation set\n",
        "preds_1, labels_1 = get_predictions(model_1, val_loader)\n",
        "\n",
        "accuracy_1, f1_1 = get_perf(preds_1, labels_1)\n",
        "\n",
        "print(f'Accuracy: {accuracy_1}, F1: {f1_1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a2xWmE4A22L",
        "outputId": "a295e4ca-0090-483b-c7d3-69a8d08923ad"
      },
      "outputs": [],
      "source": [
        "accuracy_1, f1_1 = get_perf(preds_1, labels_1)\n",
        "\n",
        "print(f'Accuracy: {accuracy_1}, F1: {f1_1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgFwwXA0yscq"
      },
      "source": [
        "Model 2:\n",
        "* 1 hidden layer - 128 size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RflyDkl13cxS",
        "outputId": "1aa18602-bea9-4422-ece8-6ec8a61d8f9f"
      },
      "outputs": [],
      "source": [
        "# Create Neural network 2\n",
        "input_size = vec_size\n",
        "hidden_sizes = [128]\n",
        "\n",
        "model_2 = classifier_nn(input_size, hidden_sizes, output_dim).to(device)\n",
        "\n",
        "train_nn(model_2, train_loader, 100, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGHgbw6N3rdu",
        "outputId": "1c8902c7-08d3-45f3-e89d-1ffacb4e14bd"
      },
      "outputs": [],
      "source": [
        "# Get accuracy on validation set\n",
        "preds_2, labels_2 = get_predictions(model_2, val_loader)\n",
        "\n",
        "accuracy_2, f1_2 = get_perf(preds_2, labels_2)\n",
        "\n",
        "print(f'Accuracy: {accuracy_2}, F1: {f1_2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgnOtn5eyscq"
      },
      "source": [
        "Model 3:\n",
        "\n",
        "* 2 hidden layers - sizes 64, 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZkchkFW3wRi",
        "outputId": "c4b8a6bb-0861-477f-a974-2404b78d7fcd"
      },
      "outputs": [],
      "source": [
        "# Create Neural network 3\n",
        "input_size = vec_size\n",
        "hidden_sizes = [64, 32]\n",
        "\n",
        "model_3 = classifier_nn(input_size, hidden_sizes, output_dim).to(device)\n",
        "\n",
        "train_nn(model_3, train_loader, 100, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTqolXol35wi",
        "outputId": "609d98df-b8b8-4c37-9d91-7554310c26e1"
      },
      "outputs": [],
      "source": [
        "# Get accuracy on validation set\n",
        "preds_3, labels_3 = get_predictions(model_3, val_loader)\n",
        "\n",
        "accuracy_3, f1_3 = get_perf(preds_3, labels_3)\n",
        "\n",
        "print(f'Accuracy: {accuracy_3}, F1: {f1_3}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7hGNC3G5dOw"
      },
      "outputs": [],
      "source": [
        "best_model = model_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4IGP4tc5jM6",
        "outputId": "0fcd256e-620f-446e-bee4-2fb21cdaacae"
      },
      "outputs": [],
      "source": [
        "# Get predictions, labels\n",
        "preds, labels = get_predictions(best_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTzJeKqf7PnA",
        "outputId": "da5c78f8-d367-426d-b37e-0b4f7f560390"
      },
      "outputs": [],
      "source": [
        "print(preds[0:5])\n",
        "print(labels[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WTG3wE98rJ_",
        "outputId": "d55a2c18-7436-4e00-b6f9-ce2be2d82273"
      },
      "outputs": [],
      "source": [
        "# Get accuracy, f1\n",
        "accuracy, f1 = get_perf(preds, labels)\n",
        "\n",
        "print(f'Accuracy on test set: {accuracy}')\n",
        "print(f'F1 score on test set: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cXLcBa_5s2f",
        "outputId": "f65c62a6-c0e6-4a1f-b082-eec068e9558d"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plot_conf_matrix(preds, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xsO4gPG8tYn"
      },
      "outputs": [],
      "source": [
        "# Save the predictions to a csv file named \"w2v_test.csv\"\n",
        "save_predictions(preds, \"test.csv\",\"w2v_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO_kbnlIyscq"
      },
      "source": [
        "### Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_sequence_length = 50\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_rnn = read_dataset(train_file, rmst=True)\n",
        "train_data_rnn, val_data_rnn = train_test_split(train_data_rnn, test_size=0.1)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = [row[0] for row in train_data_rnn]\n",
        "labels = np.array([row[1] for row in train_data_rnn])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_sentences = [row[0] for row in val_data_rnn]\n",
        "val_labels = np.array([row[1] for row in val_data_rnn])\n",
        "\n",
        "val_embeddings, _ = get_embedding_vectors(val_sentences, vector_size=100, max_seq_len=max_sequence_length)\n",
        "\n",
        "val_data_loader = prepare_data(val_embeddings, val_labels, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data_rnn = read_dataset(test_file, rmst=True)\n",
        "\n",
        "sentences = [row[0] for row in test_data_rnn]\n",
        "labels = np.array([row[1] for row in test_data_rnn])\n",
        "\n",
        "embeddings_rnn, _ = get_embedding_vectors(sentences, vector_size=100, max_seq_len=max_sequence_length)\n",
        "\n",
        "test_data_loader = prepare_data(embeddings_rnn, labels, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training RNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1\n",
        "hidden_sizes = 64\n",
        "num_layers = 1\n",
        "num_classes = 4\n",
        "\n",
        "model_1 = Classifier_RNN(vocab_size, embedding_dim, hidden_sizes, num_layers, num_classes).to(device)\n",
        "\n",
        "train_nn(model_1, train_data_loader, 100, 0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get accuracy on validation set\n",
        "preds_1, labels_1 = get_predictions(model_1, val_data_loader)\n",
        "\n",
        "accuracy_1, f1_1 = get_perf(preds_1, labels_1)\n",
        "\n",
        "print(f'Accuracy: {accuracy_1}, F1: {f1_1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
